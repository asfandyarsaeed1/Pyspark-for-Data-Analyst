{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x14e28d84b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = [(1, \"Scott\", \"Tiger\", 1000.0, 10,\n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, None,\n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, '',\n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, 10,\n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = spark. \\\n",
    "    createDataFrame(sm,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\"\n",
    "                   )\n",
    "print (f'created dataframe for employees us sm as name of spark dataframe')\n",
    "sm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 34,\"class\":\"mid\",\"marks\":222},\n",
    "    {\"ID\": 2, \"Name\": None, \"Age\": 30,\"class\":\"mid\",\"marks\":201},\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 25,\"class\":\"high\",\"marks\":100},  # Duplicate row\n",
    "    {\"ID\": 2, \"Name\": None, \"Age\": None,\"class\":\"Low\",\"marks\":400},\n",
    "    {\"ID\": 2, \"Name\": \"Bob\", \"Age\": 30,\"class\":\"Low\",\"marks\":200},  # Duplicate row and ID column\n",
    "    {\"ID\":3, \"Name\": None, \"Age\": None,\"class\":None,\"marks\":222},\n",
    "    {\"ID\": 5, \"Name\": \"Eve\", \"Age\": 28,\"class\":\"mid\",\"marks\":150},\n",
    "    {\"ID\": 1, \"Name\": \"\", \"Age\": 25,\"class\":\"Low\",\"marks\":80}, \n",
    "    {\"ID\": 6, \"Name\": None, \"Age\": 34,\"class\":\"Low\",\"marks\":180}# Duplicate row\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spark.createDataFrame(pd.DataFrame(datas))\n",
    "# data.show()\n",
    "print(f'data count:: {data.count()}')\n",
    "data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data of University courses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+---------+--------+------+\n",
      "|CourseId|  Complexity|     Name|Enrolled| Price|\n",
      "+--------+------------+---------+--------+------+\n",
      "|       1|    Beginner| Course 1|     150| 99.99|\n",
      "|       2|Intermediate| Course 2|     120|129.99|\n",
      "|       3|    Advanced| Course 3|     180|149.99|\n",
      "|       4|    Beginner| Course 4|      90| 79.99|\n",
      "|       5|Intermediate| Course 5|     160|119.99|\n",
      "|       6|    Advanced| Course 6|     190|159.99|\n",
      "|       7|    Beginner| Course 7|     110| 89.99|\n",
      "|       8|Intermediate| Course 8|     140|109.99|\n",
      "|       9|    Advanced| Course 9|     170|139.99|\n",
      "|      10|    Beginner|Course 10|      70| 69.99|\n",
      "|      11|Intermediate|Course 11|     130| 99.99|\n",
      "|      12|    Advanced|Course 12|     200|169.99|\n",
      "|      13|    Beginner|Course 13|      95| 79.99|\n",
      "|      14|Intermediate|Course 14|     175|129.99|\n",
      "|      15|    Advanced|Course 15|     185|149.99|\n",
      "|      16|    Beginner|Course 16|     105| 89.99|\n",
      "|      17|Intermediate|Course 17|     145|119.99|\n",
      "|      18|    Advanced|Course 18|     195|159.99|\n",
      "|      19|    Beginner|Course 19|      75| 69.99|\n",
      "|      20|Intermediate|Course 20|     125| 99.99|\n",
      "+--------+------------+---------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Course Id\", IntegerType(), False),\n",
    "    StructField(\"Level of Complexity\", StringType(), False),\n",
    "    StructField(\"Name\", StringType(), False),\n",
    "    StructField(\"Students Enrolled\", IntegerType(), False),\n",
    "    StructField(\"Price\", FloatType(), False)\n",
    "])\n",
    "abcs=[\"Course Id\",\"Level of Complexity\",\"Name\",\"Students Enrolled\",\"Price\"]\n",
    "\n",
    "# Define the data\n",
    "data = [\n",
    "    (1, \"Beginner\", \"Course 1\", 150, 99.99),\n",
    "    (2, \"Intermediate\", \"Course 2\", 120, 129.99),\n",
    "    (3, \"Advanced\", \"Course 3\", 180, 149.99),\n",
    "    (4, \"Beginner\", \"Course 4\", 90, 79.99),\n",
    "    (5, \"Intermediate\", \"Course 5\", 160, 119.99),\n",
    "    (6, \"Advanced\", \"Course 6\", 190, 159.99),\n",
    "    (7, \"Beginner\", \"Course 7\", 110, 89.99),\n",
    "    (8, \"Intermediate\", \"Course 8\", 140, 109.99),\n",
    "    (9, \"Advanced\", \"Course 9\", 170, 139.99),\n",
    "    (10, \"Beginner\", \"Course 10\", 70, 69.99),\n",
    "    (11, \"Intermediate\", \"Course 11\", 130, 99.99),\n",
    "    (12, \"Advanced\", \"Course 12\", 200, 169.99),\n",
    "    (13, \"Beginner\", \"Course 13\", 95, 79.99),\n",
    "    (14, \"Intermediate\", \"Course 14\", 175, 129.99),\n",
    "    (15, \"Advanced\", \"Course 15\", 185, 149.99),\n",
    "    (16, \"Beginner\", \"Course 16\", 105, 89.99),\n",
    "    (17, \"Intermediate\", \"Course 17\", 145, 119.99),\n",
    "    (18, \"Advanced\", \"Course 18\", 195, 159.99),\n",
    "    (19, \"Beginner\", \"Course 19\", 75, 69.99),\n",
    "    (20, \"Intermediate\", \"Course 20\", 125, 99.99),\n",
    "    (21, \"Advanced\", \"Course 21\", 165, 139.99),\n",
    "    (22, \"Beginner\", \"Course 22\", 100, 79.99),\n",
    "    (23, \"Intermediate\", \"Course 23\", 155, 129.99),\n",
    "    (24, \"Advanced\", \"Course 24\", 210, 169.99),\n",
    "    (25, \"Beginner\", \"Course 25\", 85, 69.99),\n",
    "    (26, \"Intermediate\", \"Course 26\", 115, 99.99),\n",
    "    (27, \"Advanced\", \"Course 27\", 160, 139.99),\n",
    "    (28, \"Beginner\", \"Course 28\", 105, 79.99),\n",
    "    (29, \"Intermediate\", \"Course 29\", 140, 119.99),\n",
    "    (30, \"Advanced\", \"Course 30\", 175, 149.99)\n",
    "]\n",
    "\n",
    "# Create a Spark DataFrame\n",
    "new_column_names = [\"CourseId\", \"Complexity\", \"Name\", \"Enrolled\", \"Price\"]\n",
    "spark_df = spark.createDataFrame(data, schema=schema)\n",
    "spark_dfs=spark_df.select(abcs).toDF(*new_column_names)\n",
    "\n",
    "# Show the Spark DataFrame\n",
    "spark_dfs.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
