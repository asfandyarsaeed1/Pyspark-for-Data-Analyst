{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x156a679aa40>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---+--------+------+-----+-----+\n",
      "|gender|  ID|     depart|age|    name|Salary|bonus|State|\n",
      "+------+----+-----------+---+--------+------+-----+-----+\n",
      "|female|1110|    biology| 41|    John|  7200| 6650|   CA|\n",
      "|female|1111|    biology| 41|    Jane|  6900| 6350|   AK|\n",
      "|female|1112|       Chem| 44|   Sarah|  9000| 8450|   LA|\n",
      "|  male|1113|       Chem| 42|   Frank|  4700| 4150|   AK|\n",
      "|  male|1114|      Maths| 40|    Mike|  7600| 7050|   CA|\n",
      "|female|1115|      Maths| 42|Jennifer|  7100| 6550|   CA|\n",
      "|female|1116|      Maths| 46| Jessica|  8800| 8250|   CA|\n",
      "|  male|1117|Pakstudeies| 40|    Fred|  4000| 3450|   LA|\n",
      "|  male|1118|Pakstudeies| 33|     Bob|  6400| 5850|   LA|\n",
      "|female|1119|Pakstudeies| 46|     Doe|  3800| 3250|   NY|\n",
      "|  male|1120|Pakstudeies| 40|   Smith|  5800| 5250|   WA|\n",
      "|  male|1121|Pakstudeies| 38|  Thomas|  4000| 3450|   NY|\n",
      "|female|1122|    physics| 44|   Brown|  6500| 5950|   NY|\n",
      "|  male|1123|    physics| 37|   Davis|  7800| 7250|   WA|\n",
      "|female|1124|    physics| 40|  Wilson|  5000| 4450|   CA|\n",
      "|female|1125|    physics| 39|  Garcia|  6900| 6350|   CA|\n",
      "|  male|1126|    physics| 43|   Clark|  8800| 8250|   NY|\n",
      "|female|1127|       Urdu| 38|   Lopez|  1800| 1250|   NY|\n",
      "|  male|1128|       Urdu| 45|  Samuel|  4600| 4050|   AK|\n",
      "|female|1129|       Urdu| 39|  Morgan|  5400| 4850|   AK|\n",
      "+------+----+-----------+---+--------+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.read.csv(\"employee_data.csv\",header=True,inferSchema=True)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- depart: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- bonus: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender', 'ID']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df1.columns[:2])\n",
    "df1.columns[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|     depart|age|\n",
      "+-----------+---+\n",
      "|    biology| 41|\n",
      "|    biology| 41|\n",
      "|       Chem| 44|\n",
      "|       Chem| 42|\n",
      "|      Maths| 40|\n",
      "|      Maths| 42|\n",
      "|      Maths| 46|\n",
      "|Pakstudeies| 40|\n",
      "|Pakstudeies| 33|\n",
      "|Pakstudeies| 46|\n",
      "|Pakstudeies| 40|\n",
      "|Pakstudeies| 38|\n",
      "|    physics| 44|\n",
      "|    physics| 37|\n",
      "|    physics| 40|\n",
      "|    physics| 39|\n",
      "|    physics| 43|\n",
      "|       Urdu| 38|\n",
      "|       Urdu| 45|\n",
      "|       Urdu| 39|\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(col(\"depart\"),\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---+--------+------+-----+-----+--------------+\n",
      "|gender|  ID|     depart|age|    name|Salary|bonus|State|Updated_Salary|\n",
      "+------+----+-----------+---+--------+------+-----+-----+--------------+\n",
      "|female|1110|    biology| 41|    John|  7200| 6650|   CA|         12200|\n",
      "|female|1111|    biology| 41|    Jane|  6900| 6350|   AK|         11900|\n",
      "|female|1112|       Chem| 44|   Sarah|  9000| 8450|   LA|         14000|\n",
      "|  male|1113|       Chem| 42|   Frank|  4700| 4150|   AK|          9700|\n",
      "|  male|1114|      Maths| 40|    Mike|  7600| 7050|   CA|         12600|\n",
      "|female|1115|      Maths| 42|Jennifer|  7100| 6550|   CA|         12100|\n",
      "|female|1116|      Maths| 46| Jessica|  8800| 8250|   CA|         13800|\n",
      "|  male|1117|Pakstudeies| 40|    Fred|  4000| 3450|   LA|          9000|\n",
      "|  male|1118|Pakstudeies| 33|     Bob|  6400| 5850|   LA|         11400|\n",
      "|female|1119|Pakstudeies| 46|     Doe|  3800| 3250|   NY|          8800|\n",
      "|  male|1120|Pakstudeies| 40|   Smith|  5800| 5250|   WA|         10800|\n",
      "|  male|1121|Pakstudeies| 38|  Thomas|  4000| 3450|   NY|          9000|\n",
      "|female|1122|    physics| 44|   Brown|  6500| 5950|   NY|         11500|\n",
      "|  male|1123|    physics| 37|   Davis|  7800| 7250|   WA|         12800|\n",
      "|female|1124|    physics| 40|  Wilson|  5000| 4450|   CA|         10000|\n",
      "|female|1125|    physics| 39|  Garcia|  6900| 6350|   CA|         11900|\n",
      "|  male|1126|    physics| 43|   Clark|  8800| 8250|   NY|         13800|\n",
      "|female|1127|       Urdu| 38|   Lopez|  1800| 1250|   NY|          6800|\n",
      "|  male|1128|       Urdu| 45|  Samuel|  4600| 4050|   AK|          9600|\n",
      "|female|1129|       Urdu| 39|  Morgan|  5400| 4850|   AK|         10400|\n",
      "+------+----+-----------+---+--------+------+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"Updated_Salary\",col(\"Salary\")+5000).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Casting a column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- depart: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- bonus: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- age_in_string: string (nullable = true)\n",
      "\n",
      "+------+----+-----------+---+--------+------+-----+-----+-------------+\n",
      "|gender|  ID|     depart|age|    name|Salary|bonus|State|age_in_string|\n",
      "+------+----+-----------+---+--------+------+-----+-----+-------------+\n",
      "|female|1110|    biology| 41|    John|  7200| 6650|   CA|           41|\n",
      "|female|1111|    biology| 41|    Jane|  6900| 6350|   AK|           41|\n",
      "|female|1112|       Chem| 44|   Sarah|  9000| 8450|   LA|           44|\n",
      "|  male|1113|       Chem| 42|   Frank|  4700| 4150|   AK|           42|\n",
      "|  male|1114|      Maths| 40|    Mike|  7600| 7050|   CA|           40|\n",
      "|female|1115|      Maths| 42|Jennifer|  7100| 6550|   CA|           42|\n",
      "|female|1116|      Maths| 46| Jessica|  8800| 8250|   CA|           46|\n",
      "|  male|1117|Pakstudeies| 40|    Fred|  4000| 3450|   LA|           40|\n",
      "|  male|1118|Pakstudeies| 33|     Bob|  6400| 5850|   LA|           33|\n",
      "|female|1119|Pakstudeies| 46|     Doe|  3800| 3250|   NY|           46|\n",
      "|  male|1120|Pakstudeies| 40|   Smith|  5800| 5250|   WA|           40|\n",
      "|  male|1121|Pakstudeies| 38|  Thomas|  4000| 3450|   NY|           38|\n",
      "|female|1122|    physics| 44|   Brown|  6500| 5950|   NY|           44|\n",
      "|  male|1123|    physics| 37|   Davis|  7800| 7250|   WA|           37|\n",
      "|female|1124|    physics| 40|  Wilson|  5000| 4450|   CA|           40|\n",
      "|female|1125|    physics| 39|  Garcia|  6900| 6350|   CA|           39|\n",
      "|  male|1126|    physics| 43|   Clark|  8800| 8250|   NY|           43|\n",
      "|female|1127|       Urdu| 38|   Lopez|  1800| 1250|   NY|           38|\n",
      "|  male|1128|       Urdu| 45|  Samuel|  4600| 4050|   AK|           45|\n",
      "|female|1129|       Urdu| 39|  Morgan|  5400| 4850|   AK|           39|\n",
      "+------+----+-----------+---+--------+------+-----+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df11=df1.withColumn('age_in_string',col(\"age\").cast(\"string\"))\n",
    "df11.printSchema()\n",
    "df11.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- depart: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- bonus: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- age_in_string: integer (nullable = true)\n",
      "\n",
      "+------+----+-----------+---+--------+------+-----+-----+-------------+\n",
      "|gender|  ID|     depart|age|    name|Salary|bonus|State|age_in_string|\n",
      "+------+----+-----------+---+--------+------+-----+-----+-------------+\n",
      "|female|1110|    biology| 41|    John|  7200| 6650|   CA|           41|\n",
      "|female|1111|    biology| 41|    Jane|  6900| 6350|   AK|           41|\n",
      "|female|1112|       Chem| 44|   Sarah|  9000| 8450|   LA|           44|\n",
      "|  male|1113|       Chem| 42|   Frank|  4700| 4150|   AK|           42|\n",
      "|  male|1114|      Maths| 40|    Mike|  7600| 7050|   CA|           40|\n",
      "|female|1115|      Maths| 42|Jennifer|  7100| 6550|   CA|           42|\n",
      "|female|1116|      Maths| 46| Jessica|  8800| 8250|   CA|           46|\n",
      "|  male|1117|Pakstudeies| 40|    Fred|  4000| 3450|   LA|           40|\n",
      "|  male|1118|Pakstudeies| 33|     Bob|  6400| 5850|   LA|           33|\n",
      "|female|1119|Pakstudeies| 46|     Doe|  3800| 3250|   NY|           46|\n",
      "|  male|1120|Pakstudeies| 40|   Smith|  5800| 5250|   WA|           40|\n",
      "|  male|1121|Pakstudeies| 38|  Thomas|  4000| 3450|   NY|           38|\n",
      "|female|1122|    physics| 44|   Brown|  6500| 5950|   NY|           44|\n",
      "|  male|1123|    physics| 37|   Davis|  7800| 7250|   WA|           37|\n",
      "|female|1124|    physics| 40|  Wilson|  5000| 4450|   CA|           40|\n",
      "|female|1125|    physics| 39|  Garcia|  6900| 6350|   CA|           39|\n",
      "|  male|1126|    physics| 43|   Clark|  8800| 8250|   NY|           43|\n",
      "|female|1127|       Urdu| 38|   Lopez|  1800| 1250|   NY|           38|\n",
      "|  male|1128|       Urdu| 45|  Samuel|  4600| 4050|   AK|           45|\n",
      "|female|1129|       Urdu| 39|  Morgan|  5400| 4850|   AK|           39|\n",
      "+------+----+-----------+---+--------+------+-----+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df11=df1.withColumn('age_in_string',col(\"age\").cast(\"int\"))\n",
    "df11.printSchema()\n",
    "df11.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- depart: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- bonus: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- age_in_integer: integer (nullable = true)\n",
      "\n",
      "+------+----+-----------+---+--------+------+-----+-----+--------------+\n",
      "|gender|  ID|     depart|age|    name|Salary|bonus|State|age_in_integer|\n",
      "+------+----+-----------+---+--------+------+-----+-----+--------------+\n",
      "|female|1110|    biology| 41|    John|  7200| 6650|   CA|            41|\n",
      "|female|1111|    biology| 41|    Jane|  6900| 6350|   AK|            41|\n",
      "|female|1112|       Chem| 44|   Sarah|  9000| 8450|   LA|            44|\n",
      "|  male|1113|       Chem| 42|   Frank|  4700| 4150|   AK|            42|\n",
      "|  male|1114|      Maths| 40|    Mike|  7600| 7050|   CA|            40|\n",
      "|female|1115|      Maths| 42|Jennifer|  7100| 6550|   CA|            42|\n",
      "|female|1116|      Maths| 46| Jessica|  8800| 8250|   CA|            46|\n",
      "|  male|1117|Pakstudeies| 40|    Fred|  4000| 3450|   LA|            40|\n",
      "|  male|1118|Pakstudeies| 33|     Bob|  6400| 5850|   LA|            33|\n",
      "|female|1119|Pakstudeies| 46|     Doe|  3800| 3250|   NY|            46|\n",
      "|  male|1120|Pakstudeies| 40|   Smith|  5800| 5250|   WA|            40|\n",
      "|  male|1121|Pakstudeies| 38|  Thomas|  4000| 3450|   NY|            38|\n",
      "|female|1122|    physics| 44|   Brown|  6500| 5950|   NY|            44|\n",
      "|  male|1123|    physics| 37|   Davis|  7800| 7250|   WA|            37|\n",
      "|female|1124|    physics| 40|  Wilson|  5000| 4450|   CA|            40|\n",
      "|female|1125|    physics| 39|  Garcia|  6900| 6350|   CA|            39|\n",
      "|  male|1126|    physics| 43|   Clark|  8800| 8250|   NY|            43|\n",
      "|female|1127|       Urdu| 38|   Lopez|  1800| 1250|   NY|            38|\n",
      "|  male|1128|       Urdu| 45|  Samuel|  4600| 4050|   AK|            45|\n",
      "|female|1129|       Urdu| 39|  Morgan|  5400| 4850|   AK|            39|\n",
      "+------+----+-----------+---+--------+------+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df12=df11.withColumnRenamed('age_in_string',\"age_in_integer\")\n",
    "df12.printSchema()\n",
    "df12.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **lit()**\n",
    "add a fix String or value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---+--------+------+-----+-----+-------+\n",
      "|gender|  ID|     depart|age|    name|Salary|bonus|State|age_add|\n",
      "+------+----+-----------+---+--------+------+-----+-----+-------+\n",
      "|female|1110|    biology| 41|    John|  7200| 6650|   CA|    500|\n",
      "|female|1111|    biology| 41|    Jane|  6900| 6350|   AK|    500|\n",
      "|female|1112|       Chem| 44|   Sarah|  9000| 8450|   LA|    500|\n",
      "|  male|1113|       Chem| 42|   Frank|  4700| 4150|   AK|    500|\n",
      "|  male|1114|      Maths| 40|    Mike|  7600| 7050|   CA|    500|\n",
      "|female|1115|      Maths| 42|Jennifer|  7100| 6550|   CA|    500|\n",
      "|female|1116|      Maths| 46| Jessica|  8800| 8250|   CA|    500|\n",
      "|  male|1117|Pakstudeies| 40|    Fred|  4000| 3450|   LA|    500|\n",
      "|  male|1118|Pakstudeies| 33|     Bob|  6400| 5850|   LA|    500|\n",
      "|female|1119|Pakstudeies| 46|     Doe|  3800| 3250|   NY|    500|\n",
      "|  male|1120|Pakstudeies| 40|   Smith|  5800| 5250|   WA|    500|\n",
      "|  male|1121|Pakstudeies| 38|  Thomas|  4000| 3450|   NY|    500|\n",
      "|female|1122|    physics| 44|   Brown|  6500| 5950|   NY|    500|\n",
      "|  male|1123|    physics| 37|   Davis|  7800| 7250|   WA|    500|\n",
      "|female|1124|    physics| 40|  Wilson|  5000| 4450|   CA|    500|\n",
      "|female|1125|    physics| 39|  Garcia|  6900| 6350|   CA|    500|\n",
      "|  male|1126|    physics| 43|   Clark|  8800| 8250|   NY|    500|\n",
      "|female|1127|       Urdu| 38|   Lopez|  1800| 1250|   NY|    500|\n",
      "|  male|1128|       Urdu| 45|  Samuel|  4600| 4050|   AK|    500|\n",
      "|female|1129|       Urdu| 39|  Morgan|  5400| 4850|   AK|    500|\n",
      "+------+----+-----------+---+--------+------+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df1.withColumn(\"age_add\",lit(500)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---+--------+------+-----+-----+-------+--------+\n",
      "|gender|  ID|     depart|age|    name|Salary|bonus|State|age_add|NEW_AGES|\n",
      "+------+----+-----------+---+--------+------+-----+-----+-------+--------+\n",
      "|female|1110|    biology| 41|    John|  7200| 6650|   CA|    500|     541|\n",
      "|female|1111|    biology| 41|    Jane|  6900| 6350|   AK|    500|     541|\n",
      "|female|1112|       Chem| 44|   Sarah|  9000| 8450|   LA|    500|     544|\n",
      "|  male|1113|       Chem| 42|   Frank|  4700| 4150|   AK|    500|     542|\n",
      "|  male|1114|      Maths| 40|    Mike|  7600| 7050|   CA|    500|     540|\n",
      "|female|1115|      Maths| 42|Jennifer|  7100| 6550|   CA|    500|     542|\n",
      "|female|1116|      Maths| 46| Jessica|  8800| 8250|   CA|    500|     546|\n",
      "|  male|1117|Pakstudeies| 40|    Fred|  4000| 3450|   LA|    500|     540|\n",
      "|  male|1118|Pakstudeies| 33|     Bob|  6400| 5850|   LA|    500|     533|\n",
      "|female|1119|Pakstudeies| 46|     Doe|  3800| 3250|   NY|    500|     546|\n",
      "|  male|1120|Pakstudeies| 40|   Smith|  5800| 5250|   WA|    500|     540|\n",
      "|  male|1121|Pakstudeies| 38|  Thomas|  4000| 3450|   NY|    500|     538|\n",
      "|female|1122|    physics| 44|   Brown|  6500| 5950|   NY|    500|     544|\n",
      "|  male|1123|    physics| 37|   Davis|  7800| 7250|   WA|    500|     537|\n",
      "|female|1124|    physics| 40|  Wilson|  5000| 4450|   CA|    500|     540|\n",
      "|female|1125|    physics| 39|  Garcia|  6900| 6350|   CA|    500|     539|\n",
      "|  male|1126|    physics| 43|   Clark|  8800| 8250|   NY|    500|     543|\n",
      "|female|1127|       Urdu| 38|   Lopez|  1800| 1250|   NY|    500|     538|\n",
      "|  male|1128|       Urdu| 45|  Samuel|  4600| 4050|   AK|    500|     545|\n",
      "|female|1129|       Urdu| 39|  Morgan|  5400| 4850|   AK|    500|     539|\n",
      "+------+----+-----------+---+--------+------+-----+-----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"age_add\",lit(500)).withColumn(\"Age_Updt\",col(\"age_add\")+col(\"age\")).withColumnRenamed(\"Age_Updt\",\"NEW_AGES\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Filter/Where**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+---+--------+------+-----+-----+--------------+\n",
      "|gender|  ID| depart|age|    name|Salary|bonus|State|age_in_integer|\n",
      "+------+----+-------+---+--------+------+-----+-----+--------------+\n",
      "|female|1110|biology| 41|    John|  7200| 6650|   CA|            41|\n",
      "|  male|1114|  Maths| 40|    Mike|  7600| 7050|   CA|            40|\n",
      "|female|1115|  Maths| 42|Jennifer|  7100| 6550|   CA|            42|\n",
      "|female|1116|  Maths| 46| Jessica|  8800| 8250|   CA|            46|\n",
      "|female|1124|physics| 40|  Wilson|  5000| 4450|   CA|            40|\n",
      "|female|1125|physics| 39|  Garcia|  6900| 6350|   CA|            39|\n",
      "+------+----+-------+---+--------+------+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df12.filter(col(\"State\")==\"CA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---+--------+------+-----+-----+--------------+\n",
      "|gender|  ID|     depart|age|    name|Salary|bonus|State|age_in_integer|\n",
      "+------+----+-----------+---+--------+------+-----+-----+--------------+\n",
      "|female|1110|    biology| 41|    John|  7200| 6650|   CA|            41|\n",
      "|female|1111|    biology| 41|    Jane|  6900| 6350|   AK|            41|\n",
      "|female|1112|       Chem| 44|   Sarah|  9000| 8450|   LA|            44|\n",
      "|  male|1113|       Chem| 42|   Frank|  4700| 4150|   AK|            42|\n",
      "|  male|1114|      Maths| 40|    Mike|  7600| 7050|   CA|            40|\n",
      "|female|1115|      Maths| 42|Jennifer|  7100| 6550|   CA|            42|\n",
      "|female|1116|      Maths| 46| Jessica|  8800| 8250|   CA|            46|\n",
      "|  male|1117|Pakstudeies| 40|    Fred|  4000| 3450|   LA|            40|\n",
      "|  male|1118|Pakstudeies| 33|     Bob|  6400| 5850|   LA|            33|\n",
      "|female|1119|Pakstudeies| 46|     Doe|  3800| 3250|   NY|            46|\n",
      "+------+----+-----------+---+--------+------+-----+-----+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df12.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+---+--------+------+-----+-----+--------------+\n",
      "|gender|  ID| depart|age|    name|Salary|bonus|State|age_in_integer|\n",
      "+------+----+-------+---+--------+------+-----+-----+--------------+\n",
      "|female|1110|biology| 41|    John|  7200| 6650|   CA|            41|\n",
      "|female|1111|biology| 41|    Jane|  6900| 6350|   AK|            41|\n",
      "|  male|1113|   Chem| 42|   Frank|  4700| 4150|   AK|            42|\n",
      "|  male|1114|  Maths| 40|    Mike|  7600| 7050|   CA|            40|\n",
      "|female|1115|  Maths| 42|Jennifer|  7100| 6550|   CA|            42|\n",
      "|female|1116|  Maths| 46| Jessica|  8800| 8250|   CA|            46|\n",
      "|female|1124|physics| 40|  Wilson|  5000| 4450|   CA|            40|\n",
      "|female|1125|physics| 39|  Garcia|  6900| 6350|   CA|            39|\n",
      "|  male|1128|   Urdu| 45|  Samuel|  4600| 4050|   AK|            45|\n",
      "|female|1129|   Urdu| 39|  Morgan|  5400| 4850|   AK|            39|\n",
      "+------+----+-------+---+--------+------+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colum=[\"CA\",\"AK\"]\n",
    "df12.filter(col(\"State\").isin(colum)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+---+--------+------+-----+-----+--------------+\n",
      "|gender|  ID| depart|age|    name|Salary|bonus|State|age_in_integer|\n",
      "+------+----+-------+---+--------+------+-----+-----+--------------+\n",
      "|female|1110|biology| 41|    John|  7200| 6650|   CA|            41|\n",
      "|female|1111|biology| 41|    Jane|  6900| 6350|   AK|            41|\n",
      "|  male|1113|   Chem| 42|   Frank|  4700| 4150|   AK|            42|\n",
      "|  male|1114|  Maths| 40|    Mike|  7600| 7050|   CA|            40|\n",
      "|female|1115|  Maths| 42|Jennifer|  7100| 6550|   CA|            42|\n",
      "|female|1116|  Maths| 46| Jessica|  8800| 8250|   CA|            46|\n",
      "|female|1124|physics| 40|  Wilson|  5000| 4450|   CA|            40|\n",
      "|female|1125|physics| 39|  Garcia|  6900| 6350|   CA|            39|\n",
      "|  male|1128|   Urdu| 45|  Samuel|  4600| 4050|   AK|            45|\n",
      "|female|1129|   Urdu| 39|  Morgan|  5400| 4850|   AK|            39|\n",
      "+------+----+-------+---+--------+------+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df12.filter((col(\"State\")== \"CA\")| (col(\"State\")==\"AK\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+---+--------+------+-----+-----+--------------+\n",
      "|gender|  ID| depart|age|    name|Salary|bonus|State|age_in_integer|\n",
      "+------+----+-------+---+--------+------+-----+-----+--------------+\n",
      "|female|1110|biology| 41|    John|  7200| 6650|   CA|            41|\n",
      "|female|1111|biology| 41|    Jane|  6900| 6350|   AK|            41|\n",
      "|female|1115|  Maths| 42|Jennifer|  7100| 6550|   CA|            42|\n",
      "|female|1116|  Maths| 46| Jessica|  8800| 8250|   CA|            46|\n",
      "+------+----+-------+---+--------+------+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df12.filter((col(\"name\").startswith(\"J\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---+----+------+-----+-----+--------------+\n",
      "|gender|  ID|     depart|age|name|Salary|bonus|State|age_in_integer|\n",
      "+------+----+-----------+---+----+------+-----+-----+--------------+\n",
      "|female|1111|    biology| 41|Jane|  6900| 6350|   AK|            41|\n",
      "|  male|1114|      Maths| 40|Mike|  7600| 7050|   CA|            40|\n",
      "|female|1119|Pakstudeies| 46| Doe|  3800| 3250|   NY|            46|\n",
      "+------+----+-----------+---+----+------+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df12.filter((col(\"name\").endswith(\"e\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+---+----+------+-----+-----+--------------+\n",
      "|gender|  ID|depart|age|name|Salary|bonus|State|age_in_integer|\n",
      "+------+----+------+---+----+------+-----+-----+--------------+\n",
      "|  male|1114| Maths| 40|Mike|  7600| 7050|   CA|            40|\n",
      "+------+----+------+---+----+------+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df12.filter(df12.name.like(\"%ik%\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+---+----+------+-----+-----+--------------+\n",
      "|gender|  ID|depart|age|name|Salary|bonus|State|age_in_integer|\n",
      "+------+----+------+---+----+------+-----+-----+--------------+\n",
      "|  male|1114| Maths| 40|Mike|  7600| 7050|   CA|            40|\n",
      "+------+----+------+---+----+------+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df12.filter(col(\"name\").like(\"%ik%\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+---+----+------+-----+-----+--------------+\n",
      "|gender|  ID|depart|age|name|Salary|bonus|State|age_in_integer|\n",
      "+------+----+------+---+----+------+-----+-----+--------------+\n",
      "|  male|1114| Maths| 40|Mike|  7600| 7050|   CA|            40|\n",
      "+------+----+------+---+----+------+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df12.filter('name like \"%ke%\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12.filter('name contains \"ke\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+---+----+------+-----+-----+--------------+\n",
      "|gender|  ID|depart|age|name|Salary|bonus|State|age_in_integer|\n",
      "+------+----+------+---+----+------+-----+-----+--------------+\n",
      "|  male|1114| Maths| 40|Mike|  7600| 7050|   CA|            40|\n",
      "+------+----+------+---+----+------+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df12.filter(col(\"name\").contains(\"ke\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quiz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-----+---+--------+\n",
      "|gender|Rollno|subject|marks|age|    name|\n",
      "+------+------+-------+-----+---+--------+\n",
      "|female|  1110|biology|   72| 41|    John|\n",
      "|female|  1111|biology|   69| 41|    Jane|\n",
      "|female|  1112|biology|   90| 44|   Sarah|\n",
      "|  male|  1113|biology|   47| 42|   Frank|\n",
      "|  male|  1114|biology|   76| 40|    Mike|\n",
      "|female|  1115|biology|   71| 42|Jennifer|\n",
      "|female|  1116|biology|   88| 46| Jessica|\n",
      "|  male|  1117|biology|   40| 40|    Fred|\n",
      "|  male|  1118|biology|   64| 33|     Bob|\n",
      "|female|  1119|biology|   38| 46|     Doe|\n",
      "+------+------+-------+-----+---+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student=spark.read.csv(\"StudentsPerformance.csv\",inferSchema=True,header=True)\n",
    "student.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1=student.withColumn(\"Total_Marks\",lit(120)).withColumn(\"avg_marks\",(col(\"marks\")/col(\"Total_Marks\"))*100).filter((col(\"avg_marks\")>=80) & (col(\"subject\")==\"Maths\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-----+---+------+-----------+-----------------+\n",
      "|gender|Rollno|subject|marks|age|  name|Total_Marks|        avg_marks|\n",
      "+------+------+-------+-----+---+------+-----------+-----------------+\n",
      "|  male|  1144|  Maths|   97| 40|Wilson|        120|80.83333333333333|\n",
      "+------+------+-------+-----+---+------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-----+---+--------+\n",
      "|gender|Rollno|subject|marks|age|    name|\n",
      "+------+------+-------+-----+---+--------+\n",
      "|female|  1110|biology|   72| 41|    John|\n",
      "|female|  1111|biology|   69| 41|    Jane|\n",
      "|female|  1112|biology|   90| 44|   Sarah|\n",
      "|  male|  1113|biology|   47| 42|   Frank|\n",
      "|  male|  1114|biology|   76| 40|    Mike|\n",
      "|female|  1115|biology|   71| 42|Jennifer|\n",
      "|female|  1116|biology|   88| 46| Jessica|\n",
      "|  male|  1117|biology|   40| 40|    Fred|\n",
      "|  male|  1118|biology|   64| 33|     Bob|\n",
      "|female|  1119|biology|   38| 46|     Doe|\n",
      "|  male|  1120|biology|   58| 40|   Smith|\n",
      "|  male|  1121|biology|   40| 38|  Thomas|\n",
      "|female|  1122|biology|   65| 44|   Brown|\n",
      "|  male|  1123|biology|   78| 37|   Davis|\n",
      "|female|  1124|biology|   50| 40|  Wilson|\n",
      "|female|  1125|biology|   69| 39|  Garcia|\n",
      "|  male|  1126|biology|   88| 43|   Clark|\n",
      "|female|  1127|biology|   18| 38|   Lopez|\n",
      "|  male|  1128|biology|   46| 45|  Samuel|\n",
      "|female|  1129|biology|   54| 39|  Morgan|\n",
      "+------+------+-------+-----+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student=student_back\n",
    "student_back.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  126\n",
      "After:  12\n"
     ]
    }
   ],
   "source": [
    "print(\"before: \",student.count())\n",
    "student=student.dropDuplicates([\"gender\",\"subject\"])\n",
    "print(\"After: \",student.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|gender|    subject|\n",
      "+------+-----------+\n",
      "|female|      Maths|\n",
      "|  male|       Chem|\n",
      "|female|    biology|\n",
      "|  male|       Urdu|\n",
      "|female|       Chem|\n",
      "|  male|      Maths|\n",
      "|  male|    physics|\n",
      "|  male|Pakstudeies|\n",
      "|female|Pakstudeies|\n",
      "|female|    physics|\n",
      "|  male|    biology|\n",
      "|female|       Urdu|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student.select([\"gender\",\"subject\"]).distinct().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quiz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-----+---+--------+\n",
      "|gender|Rollno|subject|marks|age|    name|\n",
      "+------+------+-------+-----+---+--------+\n",
      "|female|  1110|biology|   72| 41|    John|\n",
      "|female|  1111|biology|   69| 41|    Jane|\n",
      "|female|  1112|biology|   90| 44|   Sarah|\n",
      "|  male|  1113|biology|   47| 42|   Frank|\n",
      "|  male|  1114|biology|   76| 40|    Mike|\n",
      "|female|  1115|biology|   71| 42|Jennifer|\n",
      "|female|  1116|biology|   88| 46| Jessica|\n",
      "|  male|  1117|biology|   40| 40|    Fred|\n",
      "|  male|  1118|biology|   64| 33|     Bob|\n",
      "|female|  1119|biology|   38| 46|     Doe|\n",
      "+------+------+-------+-----+---+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+\n",
      "|age|gender|    subject|\n",
      "+---+------+-----------+\n",
      "| 37|  male|    biology|\n",
      "| 40|female|    biology|\n",
      "| 40|female|       Chem|\n",
      "| 51|  male|    physics|\n",
      "| 45|female|      Maths|\n",
      "| 59|  male|       Urdu|\n",
      "| 57|female|       Urdu|\n",
      "| 44|  male|       Chem|\n",
      "| 42|  male|    physics|\n",
      "| 60|female|       Urdu|\n",
      "| 45|  male|Pakstudeies|\n",
      "| 43|  male|    physics|\n",
      "| 45|female|    physics|\n",
      "| 39|  male|      Maths|\n",
      "| 41|  male|    physics|\n",
      "| 41|  male|       Chem|\n",
      "| 44|female|    physics|\n",
      "| 39|female|Pakstudeies|\n",
      "| 55|  male|       Urdu|\n",
      "| 40|  male|Pakstudeies|\n",
      "+---+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student.select ([\"age\",\"gender\",\"subject\"]).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B:  126\n",
      "A:  87\n"
     ]
    }
   ],
   "source": [
    "print(\"B: \",student.count())\n",
    "student11=student.dropDuplicates([\"age\",\"gender\",\"subject\"])\n",
    "print(\"A: \",student11.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sort/OrderBy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----------+-----+---+------+\n",
      "|gender|Rollno|    subject|marks|age|  name|\n",
      "+------+------+-----------+-----+---+------+\n",
      "|female|  1158|Pakstudeies|   57| 33|   Bob|\n",
      "|female|  1198|    physics|   58| 33|Philip|\n",
      "|  male|  1178|    physics|   61| 33|   Bob|\n",
      "|  male|  1118|    biology|   64| 33|   Bob|\n",
      "+------+------+-----------+-----+---+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student.sort([\"age\",\"marks\"]).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----------+-----+---+------+\n",
      "|gender|Rollno|    subject|marks|age|  name|\n",
      "+------+------+-----------+-----+---+------+\n",
      "|female|  1158|Pakstudeies|   57| 33|   Bob|\n",
      "|female|  1198|    physics|   58| 33|Philip|\n",
      "|  male|  1178|    physics|   61| 33|   Bob|\n",
      "|  male|  1118|    biology|   64| 33|   Bob|\n",
      "|  male|  1138|       Chem|   70| 33|   Bob|\n",
      "+------+------+-----------+-----+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student.sort(\"age\",\"marks\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----------+-----+---+------+\n",
      "|gender|Rollno|    subject|marks|age|  name|\n",
      "+------+------+-----------+-----+---+------+\n",
      "|  male|  1138|       Chem|   70| 33|   Bob|\n",
      "|  male|  1118|    biology|   64| 33|   Bob|\n",
      "|  male|  1178|    physics|   61| 33|   Bob|\n",
      "|female|  1198|    physics|   58| 33|Philip|\n",
      "|female|  1158|Pakstudeies|   57| 33|   Bob|\n",
      "|  male|  1214|       Urdu|   98| 35|  Jane|\n",
      "|  male|  1163|Pakstudeies|   88| 37| Davis|\n",
      "|  male|  1123|    biology|   78| 37| Davis|\n",
      "|  male|  1183|    physics|   61| 37| Davis|\n",
      "|  male|  1203|    physics|   43| 37| Hanks|\n",
      "+------+------+-----------+-----+---+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student.sort(student.age.asc(),student.marks.desc()).show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quiz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---+--------+------+-----+-----+\n",
      "|gender|  ID|     depart|age|    name|Salary|bonus|State|\n",
      "+------+----+-----------+---+--------+------+-----+-----+\n",
      "|female|1110|    biology| 41|    John|  7200| 6650|   CA|\n",
      "|female|1111|    biology| 41|    Jane|  6900| 6350|   AK|\n",
      "|female|1112|       Chem| 44|   Sarah|  9000| 8450|   LA|\n",
      "|  male|1113|       Chem| 42|   Frank|  4700| 4150|   AK|\n",
      "|  male|1114|      Maths| 40|    Mike|  7600| 7050|   CA|\n",
      "|female|1115|      Maths| 42|Jennifer|  7100| 6550|   CA|\n",
      "|female|1116|      Maths| 46| Jessica|  8800| 8250|   CA|\n",
      "|  male|1117|Pakstudeies| 40|    Fred|  4000| 3450|   LA|\n",
      "|  male|1118|Pakstudeies| 33|     Bob|  6400| 5850|   LA|\n",
      "|female|1119|Pakstudeies| 46|     Doe|  3800| 3250|   NY|\n",
      "+------+----+-----------+---+--------+------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order by ascending on bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---+------+------+-----+-----+\n",
      "|gender|  ID|     depart|age|  name|Salary|bonus|State|\n",
      "+------+----+-----------+---+------+------+-----+-----+\n",
      "|female|1127|       Urdu| 38| Lopez|  1800| 1250|   NY|\n",
      "|female|1119|Pakstudeies| 46|   Doe|  3800| 3250|   NY|\n",
      "|  male|1117|Pakstudeies| 40|  Fred|  4000| 3450|   LA|\n",
      "|  male|1121|Pakstudeies| 38|Thomas|  4000| 3450|   NY|\n",
      "|  male|1128|       Urdu| 45|Samuel|  4600| 4050|   AK|\n",
      "|  male|1113|       Chem| 42| Frank|  4700| 4150|   AK|\n",
      "|female|1124|    physics| 40|Wilson|  5000| 4450|   CA|\n",
      "|female|1129|       Urdu| 39|Morgan|  5400| 4850|   AK|\n",
      "|  male|1120|Pakstudeies| 40| Smith|  5800| 5250|   WA|\n",
      "|  male|1118|Pakstudeies| 33|   Bob|  6400| 5850|   LA|\n",
      "+------+----+-----------+---+------+------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.sort(\"bonus\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---+--------+------+-----+-----+\n",
      "|gender|  ID|     depart|age|    name|Salary|bonus|State|\n",
      "+------+----+-----------+---+--------+------+-----+-----+\n",
      "|female|1119|Pakstudeies| 46|     Doe|  3800| 3250|   NY|\n",
      "|female|1116|      Maths| 46| Jessica|  8800| 8250|   CA|\n",
      "|  male|1128|       Urdu| 45|  Samuel|  4600| 4050|   AK|\n",
      "|female|1122|    physics| 44|   Brown|  6500| 5950|   NY|\n",
      "|female|1112|       Chem| 44|   Sarah|  9000| 8450|   LA|\n",
      "|  male|1126|    physics| 43|   Clark|  8800| 8250|   NY|\n",
      "|  male|1113|       Chem| 42|   Frank|  4700| 4150|   AK|\n",
      "|female|1115|      Maths| 42|Jennifer|  7100| 6550|   CA|\n",
      "|female|1111|    biology| 41|    Jane|  6900| 6350|   AK|\n",
      "|female|1110|    biology| 41|    John|  7200| 6650|   CA|\n",
      "+------+----+-----------+---+--------+------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.sort(df1.age.desc(),df1.Salary.asc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---+--------+------+-----+-----+\n",
      "|gender|  ID|     depart|age|    name|Salary|bonus|State|\n",
      "+------+----+-----------+---+--------+------+-----+-----+\n",
      "|female|1116|      Maths| 46| Jessica|  8800| 8250|   CA|\n",
      "|female|1119|Pakstudeies| 46|     Doe|  3800| 3250|   NY|\n",
      "|  male|1128|       Urdu| 45|  Samuel|  4600| 4050|   AK|\n",
      "|female|1112|       Chem| 44|   Sarah|  9000| 8450|   LA|\n",
      "|female|1122|    physics| 44|   Brown|  6500| 5950|   NY|\n",
      "|  male|1126|    physics| 43|   Clark|  8800| 8250|   NY|\n",
      "|female|1115|      Maths| 42|Jennifer|  7100| 6550|   CA|\n",
      "|  male|1113|       Chem| 42|   Frank|  4700| 4150|   AK|\n",
      "|female|1110|    biology| 41|    John|  7200| 6650|   CA|\n",
      "|female|1111|    biology| 41|    Jane|  6900| 6350|   AK|\n",
      "+------+----+-----------+---+--------+------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.sort(df1.age.desc(),df1.bonus.desc(),df1.Salary.asc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-----+---+--------+\n",
      "|gender|Rollno|subject|marks|age|    name|\n",
      "+------+------+-------+-----+---+--------+\n",
      "|female|  1110|biology|   72| 41|    John|\n",
      "|female|  1111|biology|   69| 41|    Jane|\n",
      "|female|  1112|biology|   90| 44|   Sarah|\n",
      "|  male|  1113|biology|   47| 42|   Frank|\n",
      "|  male|  1114|biology|   76| 40|    Mike|\n",
      "|female|  1115|biology|   71| 42|Jennifer|\n",
      "|female|  1116|biology|   88| 46| Jessica|\n",
      "|  male|  1117|biology|   40| 40|    Fred|\n",
      "|  male|  1118|biology|   64| 33|     Bob|\n",
      "|female|  1119|biology|   38| 46|     Doe|\n",
      "+------+------+-------+-----+---+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+---------+\n",
      "|    subject|max_marks|mini_marks|sum_marks|\n",
      "+-----------+---------+----------+---------+\n",
      "|       Urdu|       99|        51|     1668|\n",
      "|Pakstudeies|       88|         0|     1267|\n",
      "|       Chem|       74|        44|      659|\n",
      "|    biology|       90|        18|     1221|\n",
      "|      Maths|       97|        40|      937|\n",
      "|    physics|       85|        27|     2211|\n",
      "+-----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student.groupBy(\"subject\").agg(max(\"marks\").alias('max_marks'),min(\"marks\").alias('mini_marks'),sum(\"marks\").alias(\"sum_marks\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quiz**\n",
    "1. Total number of student enrolled in each course?\n",
    "2. Total number of male and female students enrolled in each course.\n",
    "3. total marks achieved my each gender in each course.\n",
    "4. Display the min and max and avg marks in each course by each group age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|    subject|total_students|\n",
      "+-----------+--------------+\n",
      "|       Urdu|            23|\n",
      "|Pakstudeies|            21|\n",
      "|       Chem|            10|\n",
      "|    biology|            20|\n",
      "|      Maths|            15|\n",
      "|    physics|            37|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student.groupBy(\"subject\").agg(count(\"*\").alias(\"total_students\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------------------+\n",
      "|gender|    subject|student_per_subject|\n",
      "+------+-----------+-------------------+\n",
      "|female|      Maths|                  9|\n",
      "|  male|       Chem|                  6|\n",
      "|female|    biology|                 11|\n",
      "|  male|       Urdu|                  9|\n",
      "|female|       Chem|                  4|\n",
      "|  male|      Maths|                  6|\n",
      "|  male|    physics|                 19|\n",
      "|  male|Pakstudeies|                 12|\n",
      "|female|Pakstudeies|                  9|\n",
      "|female|    physics|                 18|\n",
      "|  male|    biology|                  9|\n",
      "|female|       Urdu|                 14|\n",
      "+------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student.groupBy(\"gender\",\"subject\").agg(count(\"*\").alias (\"student_per_subject\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+\n",
      "|gender|    subject|sum(marks)|\n",
      "+------+-----------+----------+\n",
      "|female|      Maths|       548|\n",
      "|  male|       Chem|       396|\n",
      "|female|    biology|       684|\n",
      "|  male|       Urdu|       661|\n",
      "|female|       Chem|       263|\n",
      "|  male|      Maths|       389|\n",
      "|  male|    physics|      1077|\n",
      "|  male|Pakstudeies|       775|\n",
      "|female|Pakstudeies|       492|\n",
      "|female|    physics|      1134|\n",
      "|  male|    biology|       537|\n",
      "|female|       Urdu|      1007|\n",
      "+------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student.groupBy(\"gender\",\"subject\").agg(sum(\"marks\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------------+-------------+\n",
      "|    subject|age|maxmum_marks|minimum_marks|\n",
      "+-----------+---+------------+-------------+\n",
      "|      Maths| 41|          58|           55|\n",
      "|       Urdu| 52|          70|           70|\n",
      "|       Urdu| 55|          66|           66|\n",
      "|      Maths| 39|          81|           57|\n",
      "|Pakstudeies| 37|          88|           88|\n",
      "|    physics| 40|          79|           42|\n",
      "|    physics| 45|          61|           58|\n",
      "|       Chem| 46|          69|           62|\n",
      "|       Chem| 40|          74|           67|\n",
      "|    biology| 42|          71|           47|\n",
      "|    physics| 41|          49|           47|\n",
      "|    biology| 40|          76|           40|\n",
      "|      Maths| 44|          56|           53|\n",
      "|      Maths| 42|          59|           59|\n",
      "|      Maths| 40|          97|           50|\n",
      "|    biology| 46|          88|           38|\n",
      "|      Maths| 38|          63|           50|\n",
      "|    biology| 45|          46|           46|\n",
      "|       Chem| 44|          44|           44|\n",
      "|Pakstudeies| 46|          82|           55|\n",
      "+-----------+---+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student.groupBy(\"subject\",\"age\").agg(max(\"marks\").alias(\"maxmum_marks\"),min(\"marks\").alias(\"minimum_marks\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quiz_Word_Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  value|\n",
      "+-------+\n",
      "|payeein|\n",
      "|    One|\n",
      "| Eleven|\n",
      "|   ball|\n",
      "|  mango|\n",
      "|   ball|\n",
      "|   paji|\n",
      "|payeein|\n",
      "|  apple|\n",
      "|   ball|\n",
      "|payeein|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs=spark.read.text(\"text1.txt\")\n",
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|  value|number_of_things|\n",
      "+-------+----------------+\n",
      "|    One|               1|\n",
      "|  apple|               1|\n",
      "|  mango|               1|\n",
      "| Eleven|               1|\n",
      "|   ball|               3|\n",
      "|   paji|               1|\n",
      "|payeein|               3|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.groupBy(\"value\").agg(count(\"*\").alias(\"number_of_things\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **UFD**\n",
    "find the total salary = salary + bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(salary,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+---+-----+------+-----+-----+\n",
      "|gender|  ID| depart|age| name|Salary|bonus|State|\n",
      "+------+----+-------+---+-----+------+-----+-----+\n",
      "|female|1110|biology| 41| John|  7200| 6650|   CA|\n",
      "|female|1111|biology| 41| Jane|  6900| 6350|   AK|\n",
      "|female|1112|   Chem| 44|Sarah|  9000| 8450|   LA|\n",
      "|  male|1113|   Chem| 42|Frank|  4700| 4150|   AK|\n",
      "|  male|1114|  Maths| 40| Mike|  7600| 7050|   CA|\n",
      "+------+----+-------+---+-----+------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType,FloatType,DoubleType\n",
    "def get_bonus(salary,bonus):\n",
    "    return salary+bonus\n",
    "transbon=udf(lambda x,y: get_bonus(x,y),IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+---+-----+------+-----+-----+------------+\n",
      "|gender|  ID| depart|age| name|Salary|bonus|State|total_salary|\n",
      "+------+----+-------+---+-----+------+-----+-----+------------+\n",
      "|female|1110|biology| 41| John|  7200| 6650|   CA|       13850|\n",
      "|female|1111|biology| 41| Jane|  6900| 6350|   AK|       13250|\n",
      "|female|1112|   Chem| 44|Sarah|  9000| 8450|   LA|       17450|\n",
      "|  male|1113|   Chem| 42|Frank|  4700| 4150|   AK|        8850|\n",
      "|  male|1114|  Maths| 40| Mike|  7600| 7050|   CA|       14650|\n",
      "+------+----+-------+---+-----+------+-----+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"total_salary\",transbon(df1.Salary,df1.bonus)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+---+-----+------+-----+-----+------------+\n",
      "|gender|  ID| depart|age| name|Salary|bonus|State|total_salary|\n",
      "+------+----+-------+---+-----+------+-----+-----+------------+\n",
      "|female|1110|biology| 41| John|  7200| 6650|   CA|       13850|\n",
      "|female|1111|biology| 41| Jane|  6900| 6350|   AK|       13250|\n",
      "|female|1112|   Chem| 44|Sarah|  9000| 8450|   LA|       17450|\n",
      "|  male|1113|   Chem| 42|Frank|  4700| 4150|   AK|        8850|\n",
      "|  male|1114|  Maths| 40| Mike|  7600| 7050|   CA|       14650|\n",
      "+------+----+-------+---+-----+------+-----+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"total_salary\",col(\"Salary\")+col(\"bonus\")).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quiz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_increment(State,Salary,bonus):\n",
    "    salaries=0\n",
    "    if State ==\"NY\":\n",
    "        print(\"NY\")\n",
    "        salaries=(Salary*0.1)+(bonus*0.05)\n",
    "    elif State == \"CA\":\n",
    "        salaries=Salary*0.12+(bonus*0.03)\n",
    "    return salaries\n",
    "udfs=udf(lambda x,y,z: get_increment(x,y,z),DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "106.1"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_increment(\"NY\",1000,122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+---+-----+------+-----+-----+\n",
      "|gender|  ID| depart|age| name|Salary|bonus|State|\n",
      "+------+----+-------+---+-----+------+-----+-----+\n",
      "|female|1110|biology| 41| John|  7200| 6650|   CA|\n",
      "|female|1111|biology| 41| Jane|  6900| 6350|   AK|\n",
      "|female|1112|   Chem| 44|Sarah|  9000| 8450|   LA|\n",
      "|  male|1113|   Chem| 42|Frank|  4700| 4150|   AK|\n",
      "|  male|1114|  Maths| 40| Mike|  7600| 7050|   CA|\n",
      "+------+----+-------+---+-----+------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---+--------+------+-----+-----+----------------+\n",
      "|gender|  ID|     depart|age|    name|Salary|bonus|State|increment_salary|\n",
      "+------+----+-----------+---+--------+------+-----+-----+----------------+\n",
      "|female|1110|    biology| 41|    John|  7200| 6650|   CA|          1063.5|\n",
      "|female|1111|    biology| 41|    Jane|  6900| 6350|   AK|            null|\n",
      "|female|1112|       Chem| 44|   Sarah|  9000| 8450|   LA|            null|\n",
      "|  male|1113|       Chem| 42|   Frank|  4700| 4150|   AK|            null|\n",
      "|  male|1114|      Maths| 40|    Mike|  7600| 7050|   CA|          1123.5|\n",
      "|female|1115|      Maths| 42|Jennifer|  7100| 6550|   CA|          1048.5|\n",
      "|female|1116|      Maths| 46| Jessica|  8800| 8250|   CA|          1303.5|\n",
      "|  male|1117|Pakstudeies| 40|    Fred|  4000| 3450|   LA|            null|\n",
      "|  male|1118|Pakstudeies| 33|     Bob|  6400| 5850|   LA|            null|\n",
      "|female|1119|Pakstudeies| 46|     Doe|  3800| 3250|   NY|           542.5|\n",
      "|  male|1120|Pakstudeies| 40|   Smith|  5800| 5250|   WA|            null|\n",
      "|  male|1121|Pakstudeies| 38|  Thomas|  4000| 3450|   NY|           572.5|\n",
      "|female|1122|    physics| 44|   Brown|  6500| 5950|   NY|           947.5|\n",
      "|  male|1123|    physics| 37|   Davis|  7800| 7250|   WA|            null|\n",
      "|female|1124|    physics| 40|  Wilson|  5000| 4450|   CA|           733.5|\n",
      "|female|1125|    physics| 39|  Garcia|  6900| 6350|   CA|          1018.5|\n",
      "|  male|1126|    physics| 43|   Clark|  8800| 8250|   NY|          1292.5|\n",
      "|female|1127|       Urdu| 38|   Lopez|  1800| 1250|   NY|           242.5|\n",
      "|  male|1128|       Urdu| 45|  Samuel|  4600| 4050|   AK|            null|\n",
      "|female|1129|       Urdu| 39|  Morgan|  5400| 4850|   AK|            null|\n",
      "+------+----+-----------+---+--------+------+-----+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"increment_salary\",udfs(df1.State,df1.Salary,df1.bonus)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+---+-----+------+-----+-----+\n",
      "|gender|  ID| depart|age| name|Salary|bonus|State|\n",
      "+------+----+-------+---+-----+------+-----+-----+\n",
      "|female|1110|biology| 41| John|  7200| 6650|   CA|\n",
      "|female|1111|biology| 41| Jane|  6900| 6350|   AK|\n",
      "|female|1112|   Chem| 44|Sarah|  9000| 8450|   LA|\n",
      "+------+----+-------+---+-----+------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select (\"gender\",\"ID\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------+\n",
      "|     depart|approx_count_distinct(gender)|\n",
      "+-----------+-----------------------------+\n",
      "|       Urdu|                            2|\n",
      "|Pakstudeies|                            2|\n",
      "|       Chem|                            2|\n",
      "|    biology|                            1|\n",
      "|      Maths|                            2|\n",
      "|    physics|                            2|\n",
      "+-----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"depart\").agg(approx_count_distinct(\"gender\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|     depart|count(gender)|\n",
      "+-----------+-------------+\n",
      "|       Urdu|            2|\n",
      "|Pakstudeies|            2|\n",
      "|       Chem|            2|\n",
      "|    biology|            1|\n",
      "|      Maths|            2|\n",
      "|    physics|            2|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"depart\").agg(countDistinct(\"gender\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|     depart|   collect_list(age)|\n",
      "+-----------+--------------------+\n",
      "|       Urdu|        [38, 45, 39]|\n",
      "|Pakstudeies|[40, 33, 46, 40, 38]|\n",
      "|       Chem|            [44, 42]|\n",
      "|    biology|            [41, 41]|\n",
      "|      Maths|        [40, 42, 46]|\n",
      "|    physics|[44, 37, 40, 39, 43]|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"depart\").agg(collect_list(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\functions.py:214: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|     depart|sum(DISTINCT Salary)|\n",
      "+-----------+--------------------+\n",
      "|       Urdu|               11800|\n",
      "|Pakstudeies|               20000|\n",
      "|       Chem|               13700|\n",
      "|    biology|               14100|\n",
      "|      Maths|               23500|\n",
      "|    physics|               35000|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"depart\").agg(sumDistinct(\"Salary\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spark by ITversity**\n",
    "### **Create the Spark DataFrame from List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[value: string]>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "name=[\"Asfand\",\"Saeed\",\"Sidra\",\"Mamoon\"]\n",
    "df=spark.createDataFrame(name,StringType())\n",
    "df.printSchema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multi column Spark DataFrame using Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "+---+\n",
      "\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "### No. of tuple elements is showing the rows\n",
    "## Elements inside the single tuple show the number of columns\n",
    "#\n",
    "agelist=[(12,),(13,),(14,)]\n",
    "df=spark.createDataFrame(agelist,\"age int\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|age|user_name|\n",
      "+---+---------+\n",
      "| 12|   Asfand|\n",
      "| 13|    saeed|\n",
      "| 14|    sidra|\n",
      "| 15|   Mamoon|\n",
      "+---+---------+\n",
      "\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Lets increase the number of elements inside the tuples\n",
    "##\n",
    "#\n",
    "age_list=[(12,\"Asfand\"),(13,\"saeed\"),(14,\"sidra\"),(15,\"Mamoon\")]\n",
    "df=spark.createDataFrame(age_list,'age int, user_name string')\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=12, user_name='Asfand'),\n",
       " Row(age=13, user_name='saeed'),\n",
       " Row(age=14, user_name='sidra'),\n",
       " Row(age=15, user_name='Mamoon')]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of ROW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'names']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=df.columns\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|age|user_name|\n",
      "+---+---------+\n",
      "| 12|   Asfand|\n",
      "| 13|    saeed|\n",
      "| 14|    sidra|\n",
      "| 15|   Mamoon|\n",
      "+---+---------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfff=spark.createDataFrame(age_list,a)\n",
    "dfff.show()\n",
    "dfff.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(age,IntegerType,true),StructField(user_name,StringType,true)))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=12, user_name='Asfand'),\n",
       " Row(age=13, user_name='saeed'),\n",
       " Row(age=14, user_name='sidra'),\n",
       " Row(age=15, user_name='Mamoon')]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "dfff.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfff.collect()[1].age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asfandss'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ro=Row(age=22,names=\"asfandss\")\n",
    "ro.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|names|\n",
      "+---+-----+\n",
      "|  1|  sah|\n",
      "|  2|   gg|\n",
      "|  3|   HH|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=1, names='sah'), Row(age=2, names='gg'), Row(age=3, names='HH')]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nams=[[1,\"sah\"],[2,\"gg\"],[3,\"HH\"]]\n",
    "df=spark.createDataFrame(nams,'age int,names string')\n",
    "df.show()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Row(1, 'sah')>, <Row(2, 'gg')>, <Row(3, 'HH')>]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nams=[[1,\"sah\"],[2,\"gg\"],[3,\"HH\"]]\n",
    "ros=[Row(*usr) for usr in nams]\n",
    "ros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|names|\n",
      "+---+-----+\n",
      "|  1|  sah|\n",
      "|  2|   gg|\n",
      "|  3|   HH|\n",
      "+---+-----+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- names: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs=spark.createDataFrame(ros,['age', 'names'])\n",
    "dfs.show()\n",
    "dfs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=1, names='sah'), Row(age=2, names='gg'), Row(age=3, names='HH')]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convert List of Dicts into SparkDataFrame using Row**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|  names|\n",
      "+---+-------+\n",
      "| 12| Asfand|\n",
      "| 13|  Saeed|\n",
      "| 15|Peoples|\n",
      "+---+-------+\n",
      "\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- names: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=12, names='Asfand'),\n",
       " Row(age=13, names='Saeed'),\n",
       " Row(age=15, names='Peoples')]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "user_dic=[{'age':12,'names':\"Asfand\"},{'age':13,'names':\"Saeed\"},{'age':15,'names':\"Peoples\"}]\n",
    "# user_dic.value()\n",
    "df=spark.createDataFrame(user_dic,'age int,names string')\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convert the Dic to Row and then to SparkDataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(age=12, names='Asfand'), Row(age=13, names='Saeed'), Row(age=15, names='Peoples')]\n",
      "+---+-------+\n",
      "|age|  names|\n",
      "+---+-------+\n",
      "| 12| Asfand|\n",
      "| 13|  Saeed|\n",
      "| 15|Peoples|\n",
      "+---+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- names: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_dic=[{'age':12,'names':\"Asfand\"},{'age':13,'names':\"Saeed\"},{'age':15,'names':\"Peoples\"}]\n",
    "ros_user=[Row(**usr )for usr in user_dic]\n",
    "print(ros_user)\n",
    "df=spark.createDataFrame(ros_user)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Row(<built-in method values of dict object at 0x00000156BA9FB780>)>, <Row(<built-in method values of dict object at 0x00000156BA818600>)>, <Row(<built-in method values of dict object at 0x00000156BA819380>)>]\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- names: string (nullable = true)\n",
      "\n",
      "+---+-------+\n",
      "|age|  names|\n",
      "+---+-------+\n",
      "| 12| Asfand|\n",
      "| 13|  Saeed|\n",
      "| 15|Peoples|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=12, names='Asfand'),\n",
       " Row(age=13, names='Saeed'),\n",
       " Row(age=15, names='Peoples')]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_dic=[{'age':12,'names':\"Asfand\"},{'age':13,'names':\"Saeed\"},{'age':15,'names':\"Peoples\"}]\n",
    "ros_user=[Row(usr.values) for usr in user_dic]\n",
    "print(ros_user)\n",
    "df=spark.createDataFrame(user_dic)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "df.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of Basic Data Types in Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "user_list=[\n",
    "    {\n",
    "        'first':'asfand',\n",
    "        'last':'saeed',\n",
    "        'email':'asfand@gmail.com',\n",
    "        'amount_paid':1000.02,\n",
    "        'customer_time':datetime.date(2022,1,22)\n",
    "        \n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'first':'faizan',\n",
    "        'last':'ali',\n",
    "        'email':'faizan@gmail.com',\n",
    "        'amount_paid':1020.02,\n",
    "        'customer_time':datetime.date(2022,2,22)\n",
    "    }    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'first': 'asfand',\n",
       "  'last': 'saeed',\n",
       "  'email': 'asfand@gmail.com',\n",
       "  'amount_paid': 1000.02,\n",
       "  'customer_time': datetime.date(2022, 1, 22)},\n",
       " {'first': 'faizan',\n",
       "  'last': 'ali',\n",
       "  'email': 'faizan@gmail.com',\n",
       "  'amount_paid': 1020.02,\n",
       "  'customer_time': datetime.date(2022, 2, 22)}]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+-----------+-------------+\n",
      "| first| last|           email|amount_paid|customer_time|\n",
      "+------+-----+----------------+-----------+-------------+\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|   2022-01-22|\n",
      "|faizan|  ali|faizan@gmail.com|    1020.02|   2022-02-22|\n",
      "+------+-----+----------------+-----------+-------------+\n",
      "\n",
      "root\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- customer_time: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(first='asfand', last='saeed', email='asfand@gmail.com', amount_paid=1000.02, customer_time=datetime.date(2022, 1, 22)),\n",
       " Row(first='faizan', last='ali', email='faizan@gmail.com', amount_paid=1020.02, customer_time=datetime.date(2022, 2, 22))]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.createDataFrame(Row(**uses) for uses in user_list)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Specifying Schema for Spark Dataframe using String**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list=[\n",
    "    (1,\n",
    "     \"Asfand\",\n",
    "     \"Saeed\",\n",
    "     \"asfand@gmail.com\",\n",
    "     datetime.date(2022,12,1),\n",
    "     datetime.datetime(2022,12,1,11,22)),\n",
    "    (2,\n",
    "     \"Ali\",\n",
    "     \"zafar\",\n",
    "     \"ali@gmail.com\",\n",
    "     datetime.date(2021,2,1),\n",
    "     datetime.datetime(2021,12,1,11,22))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "schem='''\n",
    "id INT,\n",
    "name STRING,\n",
    "last STRING,\n",
    "email STRING,\n",
    "purchase DATE,\n",
    "times TIMESTAMP\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----------------+----------+-------------------+\n",
      "| id|  name| last|           email|  purchase|              times|\n",
      "+---+------+-----+----------------+----------+-------------------+\n",
      "|  1|Asfand|Saeed|asfand@gmail.com|2022-12-01|2022-12-01 11:22:00|\n",
      "|  2|   Ali|zafar|   ali@gmail.com|2021-02-01|2021-12-01 11:22:00|\n",
      "+---+------+-----+----------------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- purchase: date (nullable = true)\n",
      " |-- times: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(names_list,schem)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Specifying Schema for Spark Dataframe using List**\n",
    "*we will just pass the columns names the datatype will be infer from the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list=[\n",
    "    (1,\n",
    "     \"Asfand\",\n",
    "     \"Saeed\",\n",
    "     \"asfand@gmail.com\",\n",
    "     datetime.date(2022,12,1),\n",
    "     datetime.datetime(2022,12,1,11,22)),\n",
    "    (2,\n",
    "     \"Ali\",\n",
    "     \"zafar\",\n",
    "     \"ali@gmail.com\",\n",
    "     datetime.date(2021,2,1),\n",
    "     datetime.datetime(2021,12,1,11,22))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+----------------+-------------+-------------------+\n",
      "| id|first_name|second_name|           email|purchase_date|      time_purchase|\n",
      "+---+----------+-----------+----------------+-------------+-------------------+\n",
      "|  1|    Asfand|      Saeed|asfand@gmail.com|   2022-12-01|2022-12-01 11:22:00|\n",
      "|  2|       Ali|      zafar|   ali@gmail.com|   2021-02-01|2021-12-01 11:22:00|\n",
      "+---+----------+-----------+----------------+-------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- second_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- purchase_date: date (nullable = true)\n",
      " |-- time_purchase: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col=[\n",
    "    'id',\n",
    "    'first_name',\n",
    "    'second_name',\n",
    "    'email',\n",
    "    'purchase_date',\n",
    "    'time_purchase'\n",
    "]\n",
    "df=spark.createDataFrame(names_list,col)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Specifying Schema using Spark Types**\n",
    "1. *Import the spark Type class*\n",
    "2. *Find out the Spark Type API*\n",
    "3. *Make the schema*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list=[\n",
    "    (1,\n",
    "     \"Asfand\",\n",
    "     \"Saeed\",\n",
    "     \"asfand@gmail.com\",\n",
    "     datetime.date(2022,12,1),\n",
    "     datetime.datetime(2022,12,1,11,22)),\n",
    "    (2,\n",
    "     \"Ali\",\n",
    "     \"zafar\",\n",
    "     \"ali@gmail.com\",\n",
    "     datetime.date(2021,2,1),\n",
    "     datetime.datetime(2021,12,1,11,22))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----------------+-------------+-------------------+\n",
      "| id| first| last|           email|purchase_date|      time_purchase|\n",
      "+---+------+-----+----------------+-------------+-------------------+\n",
      "|  1|Asfand|Saeed|asfand@gmail.com|   2022-12-01|2022-12-01 11:22:00|\n",
      "|  2|   Ali|zafar|   ali@gmail.com|   2021-02-01|2021-12-01 11:22:00|\n",
      "+---+------+-----+----------------+-------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- purchase_date: date (nullable = true)\n",
      " |-- time_purchase: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "scha=StructType([\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('first', StringType()),\n",
    "    StructField('last', StringType()),\n",
    "    StructField('email', StringType()),\n",
    "    StructField('purchase_date', DateType()),\n",
    "    StructField('time_purchase', TimestampType())\n",
    "])\n",
    "df=spark.createDataFrame(names_list,scha)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Spark Dataframe using Pandas Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "user_list=[\n",
    "    {\n",
    "        'first':'asfand',\n",
    "        'last':'saeed',\n",
    "        'email':'asfand@gmail.com',\n",
    "        'amount_paid':1000.02,\n",
    "        'customer_time':datetime.date(2022,1,22)\n",
    "        \n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'first':'faizan',\n",
    "        'last':'ali',\n",
    "        'email':'faizan@gmail.com',\n",
    "        'amount_paid':1020.02,\n",
    "        'customer_time':datetime.date(2022,2,22)\n",
    "    }    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>email</th>\n",
       "      <th>amount_paid</th>\n",
       "      <th>customer_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asfand</td>\n",
       "      <td>saeed</td>\n",
       "      <td>asfand@gmail.com</td>\n",
       "      <td>1000.02</td>\n",
       "      <td>2022-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faizan</td>\n",
       "      <td>ali</td>\n",
       "      <td>faizan@gmail.com</td>\n",
       "      <td>1020.02</td>\n",
       "      <td>2022-02-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    first   last             email  amount_paid customer_time\n",
       "0  asfand  saeed  asfand@gmail.com      1000.02    2022-01-22\n",
       "1  faizan    ali  faizan@gmail.com      1020.02    2022-02-22"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# pd.DataFrame([Row(**use) for use in user_list])\n",
    "pd.DataFrame(user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+-----------+-------------+\n",
      "| first| last|           email|amount_paid|customer_time|\n",
      "+------+-----+----------------+-----------+-------------+\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|   2022-01-22|\n",
      "|faizan|  ali|faizan@gmail.com|    1020.02|   2022-02-22|\n",
      "+------+-----+----------------+-----------+-------------+\n",
      "\n",
      "root\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- customer_time: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(pd.DataFrame(user_list))\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of Special Data Types in Spark**\n",
    "1. ARRAY (python list)\n",
    "2. MAP (python DIC)\n",
    "3. STRUCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "user_list=[\n",
    "    {\n",
    "        'first':'asfand',\n",
    "        'last':'saeed',\n",
    "        'email':'asfand@gmail.com',\n",
    "        'amount_paid':1000.02,\n",
    "        'phones': ['+923339555855','+923129677783'],\n",
    "        'customer_time':datetime.date(2022,1,22)\n",
    "        \n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'first':'faizan',\n",
    "        'last':'ali',\n",
    "        'email':'faizan@gmail.com',\n",
    "        'amount_paid':1020.02,\n",
    "        'phones':None,\n",
    "        'customer_time':datetime.date(2022,2,22)\n",
    "    }    \n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Due to Expolde the record that doesnt have phone number is eleminated and by using the explode_outer it is again present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- phones: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- customer_time: date (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      "\n",
      "+------+-----+----------------+-----------+--------------------+-------------+-------------+\n",
      "| first| last|           email|amount_paid|              phones|customer_time|        phone|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+-------------+\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|[+923339555855, +...|   2022-01-22|+923339555855|\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|[+923339555855, +...|   2022-01-22|+923129677783|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df=spark.createDataFrame([Row(**user) for user in user_list])\n",
    "dff=df.withColumn(\"phone\",explode(col('phones')))\n",
    "dff.printSchema()\n",
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+-----------+--------------------+-------------+-------------+-------------+\n",
      "| first| last|           email|amount_paid|              phones|customer_time|        Ufone|         Zong|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+-------------+-------------+\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|[+923339555855, +...|   2022-01-22|+923339555855|+923129677783|\n",
      "|faizan|  ali|faizan@gmail.com|    1020.02|                null|   2022-02-22|         null|         null|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([Row(**user) for user in user_list])\n",
    "dff=df.withColumn(\"Ufone\",col(\"phones\")[0]).withColumn(\"Zong\",col(\"phones\")[1])\n",
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- phones: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- customer_time: date (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- ufone: string (nullable = true)\n",
      "\n",
      "+------+-----+----------------+-----------+--------------------+-------------+-------------+-------------+\n",
      "| first| last|           email|amount_paid|              phones|customer_time|        phone|        ufone|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+-------------+-------------+\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|[+923339555855, +...|   2022-01-22|+923339555855|+923339555855|\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|[+923339555855, +...|   2022-01-22|+923129677783|+923339555855|\n",
      "|faizan|  ali|faizan@gmail.com|    1020.02|                null|   2022-02-22|         null|         null|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([Row(**user) for user in user_list])\n",
    "dff=df.withColumn(\"phone\",explode_outer(col('phones'))).withColumn(\"ufone\",col(\"phones\")[0])\n",
    "dff.printSchema()\n",
    "dff.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Map Type Columns in Spark Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "user_list=[\n",
    "    {\n",
    "        'first':'asfand',\n",
    "        'last':'saeed',\n",
    "        'email':'asfand@gmail.com',\n",
    "        'amount_paid':1000.02,\n",
    "        'phones': {'mobile': '+923339555855','home':'+923129677783'},\n",
    "        'customer_time':datetime.date(2022,1,22)\n",
    "        \n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'first':'faizan',\n",
    "        'last':'ali',\n",
    "        'email':'faizan@gmail.com',\n",
    "        'amount_paid':1020.02,\n",
    "        'phones':None,\n",
    "        'customer_time':datetime.date(2022,2,22)\n",
    "    }    \n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can not alias explode column in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+-----------+--------------------+-------------+\n",
      "| first| last|           email|amount_paid|              phones|customer_time|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|{mobile -> +92333...|   2022-01-22|\n",
      "|faizan|  ali|faizan@gmail.com|    1020.02|                null|   2022-02-22|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+\n",
      "\n",
      "root\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- phones: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- customer_time: date (nullable = true)\n",
      "\n",
      "+-------------+\n",
      "|         home|\n",
      "+-------------+\n",
      "|+923129677783|\n",
      "|         null|\n",
      "+-------------+\n",
      "\n",
      "+-------------+-------------+\n",
      "|       mobile|         home|\n",
      "+-------------+-------------+\n",
      "|+923339555855|+923129677783|\n",
      "|         null|         null|\n",
      "+-------------+-------------+\n",
      "\n",
      "+------+------+-------------+-------+-------------+\n",
      "| first|   key|        value|handset|       Number|\n",
      "+------+------+-------------+-------+-------------+\n",
      "|asfand|mobile|+923339555855| mobile|+923339555855|\n",
      "|asfand|  home|+923129677783|   home|+923129677783|\n",
      "+------+------+-------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([Row(**user) for user in user_list])\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.select (col('phones.home')).show()\n",
    "df.select (col('phones.mobile'),col('phones.home')).show()\n",
    "dff=df.select ('first',explode('phones')).withColumn('handset',col('key')).withColumn('Number',col('value'))\n",
    "dff.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Struct Type Columns in Spark Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "user_list=[\n",
    "    {\n",
    "        'first':'asfand',\n",
    "        'last':'saeed',\n",
    "        'email':'asfand@gmail.com',\n",
    "        'amount_paid':1000.02,\n",
    "        \"phones\": Row(mobile=\"+923339555855\",home=\"+923129677783\"),\n",
    "        # 'phones': {'mobile': '+923339555855','home':'+923129677783'},\n",
    "        'customer_time':datetime.date(2022,1,22)\n",
    "        \n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'first':'faizan',\n",
    "        'last':'ali',\n",
    "        'email':'faizan@gmail.com',\n",
    "        'amount_paid':1020.02,\n",
    "        \"phones\": Row(mobile=None,home=None),\n",
    "        'customer_time':datetime.date(2022,2,22)\n",
    "    }    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'first': 'asfand',\n",
       "  'last': 'saeed',\n",
       "  'email': 'asfand@gmail.com',\n",
       "  'amount_paid': 1000.02,\n",
       "  'phones': Row(mobile='+923339555855', home='+923129677783'),\n",
       "  'customer_time': datetime.date(2022, 1, 22)},\n",
       " {'first': 'faizan',\n",
       "  'last': 'ali',\n",
       "  'email': 'faizan@gmail.com',\n",
       "  'amount_paid': 1020.02,\n",
       "  'phones': Row(mobile=None, home=None),\n",
       "  'customer_time': datetime.date(2022, 2, 22)}]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+-----------+--------------------+-------------+\n",
      "| first| last|           email|amount_paid|              phones|customer_time|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|{+923339555855, +...|   2022-01-22|\n",
      "|faizan|  ali|faizan@gmail.com|    1020.02|        {null, null}|   2022-02-22|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "df=spark.createDataFrame([Row(**use) for use in user_list])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "| first|       mobile|\n",
      "+------+-------------+\n",
      "|asfand|+923339555855|\n",
      "|faizan|         null|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select ('first',col('phones.mobile')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------+\n",
      "| first|       mobile|         home|\n",
      "+------+-------------+-------------+\n",
      "|asfand|+923339555855|+923129677783|\n",
      "|faizan|         null|         null|\n",
      "+------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select ('first',col('phones.*')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "user_list=[\n",
    "    {\n",
    "        'first':'asfand',\n",
    "        'last':'saeed',\n",
    "        'email':'asfand@gmail.com',\n",
    "        'amount_paid':1000.02,\n",
    "        \"phones\": Row(mobile=\"+923339555855\",home=\"+923129677783\"),\n",
    "        # 'phones': {'mobile': '+923339555855','home':'+923129677783'},\n",
    "        'customer_time':datetime.date(2022,1,22),\n",
    "        'items':['boy','football']\n",
    "        \n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'first':'faizan',\n",
    "        'last':'ali',\n",
    "        'email':'faizan@gmail.com',\n",
    "        'amount_paid':1020.02,\n",
    "        \"phones\": Row(mobile=None,home=None),\n",
    "        'customer_time':datetime.date(2022,2,22),\n",
    "        'items':[]\n",
    "    }    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- phones: struct (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- home: string (nullable = true)\n",
      " |-- customer_time: date (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+\n",
      "| first| last|           email|amount_paid|              phones|customer_time|          items|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|{+923339555855, +...|   2022-01-22|[boy, football]|\n",
      "|faizan|  ali|faizan@gmail.com|    1020.02|        {null, null}|   2022-02-22|             []|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([Row(**use) for use in user_list])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **38. Overview of Select on Spark Data Frame**\n",
    "1. Select takes *\n",
    "2. select takes string\n",
    "3. select take list and combination of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+\n",
      "| first| last|           email|amount_paid|              phones|customer_time|          items|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|{+923339555855, +...|   2022-01-22|[boy, football]|\n",
      "|faizan|  ali|faizan@gmail.com|    1020.02|        {null, null}|   2022-02-22|             []|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| first| last|\n",
      "+------+-----+\n",
      "|asfand|saeed|\n",
      "|faizan|  ali|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select ('first','last').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+\n",
      "| first| last|           email|amount_paid|              phones|customer_time|          items|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|{+923339555855, +...|   2022-01-22|[boy, football]|\n",
      "|faizan|  ali|faizan@gmail.com|    1020.02|        {null, null}|   2022-02-22|             []|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| first| last|\n",
      "+------+-----+\n",
      "|asfand|saeed|\n",
      "|faizan|  ali|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "| first| last|\n",
      "+------+-----+\n",
      "|asfand|saeed|\n",
      "|faizan|  ali|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+-----------+\n",
      "| first| last|  full_name|\n",
      "+------+-----+-----------+\n",
      "|asfand|saeed|asfandsaeed|\n",
      "|faizan|  ali|  faizanali|\n",
      "+------+-----+-----------+\n",
      "\n",
      "+------+-----+\n",
      "| first| last|\n",
      "+------+-----+\n",
      "|asfand|saeed|\n",
      "|faizan|  ali|\n",
      "+------+-----+\n",
      "\n",
      "+----------------+-----------+\n",
      "|           email|amount_paid|\n",
      "+----------------+-----------+\n",
      "|asfand@gmail.com|    1000.02|\n",
      "|faizan@gmail.com|    1020.02|\n",
      "+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('u').select ('u.first','u.last').show()\n",
    "df.alias('u').select (col('u.first'),'u.last').show()\n",
    "df.alias('u').select ('u.first','u.last',concat(col('u.first'),col('u.last')).alias(\"full_name\")).show()\n",
    "df.alias('u').select (['u.first','u.last']).show()\n",
    "df.alias('u').select ([col('u.email'),col('u.amount_paid')]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **39. Overview of selectExpr on Spark Data Frame**\n",
    "1. *if we want to use function like SQL using it we can use selectExpr that wil help us.*\n",
    "2. *For selectExpr we don't need to import functions like concat,lit,alias etc it has built configuration.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------+\n",
      "| first| last|    FUll_Name|\n",
      "+------+-----+-------------+\n",
      "|asfand|saeed|asfand, saeed|\n",
      "|faizan|  ali|  faizan, ali|\n",
      "+------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.selectExpr('first','last',\"concat(first,', ',last) AS FUll_Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------+\n",
      "| first| last|    Full_name|\n",
      "+------+-----+-------------+\n",
      "|asfand|saeed|asfand, saeed|\n",
      "|faizan|  ali|  faizan, ali|\n",
      "+------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('tab1')\n",
    "spark.sql(\"\"\"select first,last,concat(first,', ',last) As Full_name from tab1\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **40. Referring Columns using Spark Data Frame Names**\n",
    "1. when we use df['first'] it returns column type object.\n",
    "2. when we use col('first') it return column type object.\n",
    "3. We cant use column type object df['first'],col('first') in SelectExpr.\n",
    "4. we can not use alias as u['first'] as column type object but we can use it as col('u.first')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| first| last|\n",
      "+------+-----+\n",
      "|asfand|saeed|\n",
      "|faizan|  ali|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('u').select ('u.first',col('u.last')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'first'>"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'first'>"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "| first|         Full|\n",
      "+------+-------------+\n",
      "|asfand|asfand, saeed|\n",
      "|faizan|  faizan, ali|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('u').selectExpr('u.first',\"concat(u.first,', ',u.last) as Full\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **41. Understanding col function in Spark**\n",
    "1. *all the function in pyspark.sql.functions applied on the column also returns column*.\n",
    "2. *select (\\*cols) if cols=[col('first'),col('last')] and alse select(cols) if cols=[col('first'),col('last')]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------------------------+\n",
      "|           email|date_format(customer_time, yyyyMMdd)|\n",
      "+----------------+------------------------------------+\n",
      "|asfand@gmail.com|                            20220122|\n",
      "|faizan@gmail.com|                            20220222|\n",
      "+----------------+------------------------------------+\n",
      "\n",
      "+------+-----+\n",
      "| first| last|\n",
      "+------+-----+\n",
      "|asfand|saeed|\n",
      "|faizan|  ali|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a=df['first']\n",
    "b=col('last')\n",
    "c=[col('email'),date_format(col('customer_time'),'yyyyMMdd')]\n",
    "df.select(c).show()\n",
    "df.select(a,b).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|           email|date_int|\n",
      "+----------------+--------+\n",
      "|asfand@gmail.com|20220122|\n",
      "|faizan@gmail.com|20220222|\n",
      "+----------------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(email,StringType,true),StructField(date_int,IntegerType,true)))"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=[col('email'),date_format(col('customer_time'),'yyyyMMdd').cast('int').alias(\"date_int\")]\n",
    "df.select(c).show()\n",
    "df.select(c).schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **42. Invoking Functions using Spark Column Objects**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **43. Understanding lit function in Spark**\n",
    "1. lit() returns column type object.\n",
    "2. In select we are using every element as column type object.\n",
    "3. Cast sort,filter,contains,alias are build on top of col type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asfandyar\\Desktop\\python_Asfandyar\\40 days\\chilla\\Spark\\Certification Spark\\Spark_developer.ipynb Cell 156\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m Row\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m user_list\u001b[39m=\u001b[39m[\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39masfand\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlast\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39msaeed\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39memail\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39masfand@gmail.com\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mamount_paid\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m1000.02\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mphones\u001b[39m\u001b[39m\"\u001b[39m: Row(mobile\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+923339555855\u001b[39m\u001b[39m\"\u001b[39m,home\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+923129677783\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m# 'phones': {'mobile': '+923339555855','home':'+923129677783'},\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcustomer_time\u001b[39m\u001b[39m'\u001b[39m:datetime\u001b[39m.\u001b[39mdate(\u001b[39m2022\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m22\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mitems\u001b[39m\u001b[39m'\u001b[39m:[\u001b[39m'\u001b[39m\u001b[39mboy\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mfootball\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     },\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mfaizan\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlast\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mali\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39memail\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mfaizan@gmail.com\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mamount_paid\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m1020.02\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mphones\u001b[39m\u001b[39m\"\u001b[39m: Row(mobile\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,home\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcustomer_time\u001b[39m\u001b[39m'\u001b[39m:datetime\u001b[39m.\u001b[39mdate(\u001b[39m2022\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m22\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mitems\u001b[39m\u001b[39m'\u001b[39m:[]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     }    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m df\u001b[39m=\u001b[39mspark\u001b[39m.\u001b[39mcreateDataFrame([Row(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39muse) \u001b[39mfor\u001b[39;00m use \u001b[39min\u001b[39;00m user_list])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y311sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m df\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "user_list=[\n",
    "    {\n",
    "        'first':'asfand',\n",
    "        'last':'saeed',\n",
    "        'email':'asfand@gmail.com',\n",
    "        'amount_paid':1000.02,\n",
    "        \"phones\": Row(mobile=\"+923339555855\",home=\"+923129677783\"),\n",
    "        # 'phones': {'mobile': '+923339555855','home':'+923129677783'},\n",
    "        'customer_time':datetime.date(2022,1,22),\n",
    "        'items':['boy','football']\n",
    "        \n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'first':'faizan',\n",
    "        'last':'ali',\n",
    "        'email':'faizan@gmail.com',\n",
    "        'amount_paid':1020.02,\n",
    "        \"phones\": Row(mobile=None,home=None),\n",
    "        'customer_time':datetime.date(2022,2,22),\n",
    "        'items':[]\n",
    "    }    \n",
    "]\n",
    "df=spark.createDataFrame([Row(**use) for use in user_list])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'25'>"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+\n",
      "| first| last|  total|\n",
      "+------+-----+-------+\n",
      "|asfand|saeed|1003.02|\n",
      "|faizan|  ali|1023.02|\n",
      "+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select ('first',col('last'),(col('amount_paid')+lit(3)).alias('total')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+\n",
      "| first| last|  total|\n",
      "+------+-----+-------+\n",
      "|asfand|saeed|1003.02|\n",
      "|faizan|  ali|1023.02|\n",
      "+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Please dont type column type object in selectExpr like df['first],col['first]\n",
    "##\n",
    "#\n",
    "df.selectExpr('first','last',\"amount_paid+3 as total\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **44. Overview of Renaming Spark Data Frame Columns or Expressions**\n",
    "1. withColumn() is used to give column name after Row level operations.\n",
    "2. alias to rename columns\n",
    "3. toDF is use to rename bunch of columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **45. Naming derived columns using withColumn**\n",
    "1. withColumn takes colname,col_type_object\n",
    "2. If we do replacement of the column names then we will see that the new column name location is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------+\n",
      "| first| last|         full|\n",
      "+------+-----+-------------+\n",
      "|asfand|saeed|asfand, saeed|\n",
      "|faizan|  ali|  faizan, ali|\n",
      "+------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('first','last').withColumn('full',concat('first',lit(', '),'last')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------+\n",
      "| first| last|         full|\n",
      "+------+-----+-------------+\n",
      "|asfand|saeed|asfand, saeed|\n",
      "|faizan|  ali|  faizan, ali|\n",
      "+------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('first','last').withColumn('full',concat(df['first'],lit(', '),df['last'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+------+\n",
      "| first| last|           email|amount_paid|              phones|customer_time|          items|  full|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+------+\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|{+923339555855, +...|   2022-01-22|[boy, football]|asfand|\n",
      "|faizan|  ali|faizan@gmail.com|    1020.02|        {null, null}|   2022-02-22|             []|faizan|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### dont use just the name of column as that is not col type\n",
    "##\n",
    "#\n",
    "df.withColumn('full',col('first')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **46. Renaming Columns using withColumnRenamed**\n",
    "1. columns withColumnRenamed will take 2 string one is of what column to replace name and the second what should be the column name.\n",
    "2. withColumnRenamed does not take any col type object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+\n",
      "|  full| last|           email|amount_paid|              phones|customer_time|          items|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+\n",
      "|asfand|saeed|asfand@gmail.com|    1000.02|{+923339555855, +...|   2022-01-22|[boy, football]|\n",
      "|faizan|  ali|faizan@gmail.com|    1020.02|        {null, null}|   2022-02-22|             []|\n",
      "+------+-----+----------------+-----------+--------------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('first','full').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumnRenamed('full').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **48. Renaming and Reordering multiple Spark Data Frame Columns**\n",
    "1. *It is hard to pass a string to convert the multiple columns so here toDF will help us alot.*\n",
    "2. *toDF will need to pass every column name as string it doesnt support any sort of list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_cols=df.columns\n",
    "targ_cols=['Sir','final','gmail','paid','number','times','things']\n",
    "# df.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+-------+--------------------+----------+---------------+\n",
      "|   Sir|final|           gmail|   paid|              number|     times|         things|\n",
      "+------+-----+----------------+-------+--------------------+----------+---------------+\n",
      "|asfand|saeed|asfand@gmail.com|1000.02|{+923339555855, +...|2022-01-22|[boy, football]|\n",
      "|faizan|  ali|faizan@gmail.com|1020.02|        {null, null}|2022-02-22|             []|\n",
      "+------+-----+----------------+-------+--------------------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(src_cols).toDF(*targ_cols).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **50. Predefined Functions using Spark Data Frame APIs**\n",
    "1. filter\\\\where ,groupBy,Sort,Date_Format,Select ,alias.\n",
    "2. In date_format we can pass date/timestamp/string to convert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+------------------+-------------+-------------------+\n",
      "| id| first| last|             email|date_purchase|      time_purchase|\n",
      "+---+------+-----+------------------+-------------+-------------------+\n",
      "|  1|Asfand|Saeed|asfand12@gmail.com|   2022-12-01|2022-12-01 11:22:00|\n",
      "|  2|   Ali|zafar|   ali34@gmail.com|   2021-02-01|2021-12-01 11:22:00|\n",
      "+---+------+-----+------------------+-------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- date_purchase: date (nullable = true)\n",
      " |-- time_purchase: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_list=[\n",
    "    (1,\n",
    "     \"Asfand\",\n",
    "     \"Saeed\",\n",
    "     \"asfand12@gmail.com\",\n",
    "     datetime.date(2022,12,1),\n",
    "     datetime.datetime(2022,12,1,11,22)),\n",
    "    (2,\n",
    "     \"Ali\",\n",
    "     \"zafar\",\n",
    "     \"ali34@gmail.com\",\n",
    "     datetime.date(2021,2,1),\n",
    "     datetime.datetime(2021,12,1,11,22))\n",
    "]\n",
    "scha=['id','first','last','email','date_purchase','time_purchase']\n",
    "df=spark.createDataFrame(names_list,scha)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- date_purchase: date (nullable = true)\n",
      " |-- time_purchase: timestamp (nullable = true)\n",
      " |-- date_int: integer (nullable = true)\n",
      "\n",
      "+---+------+-----+------------------+-------------+-------------------+--------+\n",
      "| id| first| last|             email|date_purchase|      time_purchase|date_int|\n",
      "+---+------+-----+------------------+-------------+-------------------+--------+\n",
      "|  1|Asfand|Saeed|asfand12@gmail.com|   2022-12-01|2022-12-01 11:22:00|  202212|\n",
      "|  2|   Ali|zafar|   ali34@gmail.com|   2021-02-01|2021-12-01 11:22:00|  202112|\n",
      "+---+------+-----+------------------+-------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### convert the timesstamp to int value as it is converted to string after date_formate()\n",
    "##\n",
    "#\n",
    "from pyspark.sql.functions import date_format\n",
    "dff=df.select('*',date_format(col('time_purchase'),'yyyyMM').cast ('int').alias(\"date_int\"))\n",
    "dff.printSchema()\n",
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----------------+-------------+-------------------+\n",
      "| id| first| last|           email|date_purchase|      time_purchase|\n",
      "+---+------+-----+----------------+-------------+-------------------+\n",
      "|  1|Asfand|Saeed|asfand@gmail.com|   2022-12-01|2022-12-01 11:22:00|\n",
      "+---+------+-----+----------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(date_format(col('time_purchase'),'yyyyMM').cast ('int').alias(\"date_int\")== '202212').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+\n",
      "|date_int|count|       now|\n",
      "+--------+-----+----------+\n",
      "|  202212|    1|2022-12-31|\n",
      "|  202112|    1|2022-12-31|\n",
      "+--------+-----+----------+\n",
      "\n",
      "root\n",
      " |-- date_int: integer (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- now: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "df.groupBy(date_format(col('time_purchase'),'yyyyMM').cast ('int').alias(\"date_int\")).count().withColumn ('now',current_date()).show()\n",
    "df.groupBy(date_format(col('time_purchase'),'yyyyMM').cast ('int').alias(\"date_int\")).count().withColumn ('now',current_date()).printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **52. Categories Of Functions to Manipulate Columns in Spark Data Frames**\n",
    "* Data_Sting manipulation functions:\n",
    "  * lower,upper\n",
    "  * length\n",
    "  * Extract Substring : substring,split\n",
    "  * Triming -trim,ltrim,rtrim\n",
    "  * Padding -lpad,rpad\n",
    "  * concat_string -concat,concat_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----------------+-------------+-------------------+\n",
      "| id| first| last|           email|date_purchase|      time_purchase|\n",
      "+---+------+-----+----------------+-------------+-------------------+\n",
      "|  1|Asfand|Saeed|asfand@gmail.com|   2022-12-01|2022-12-01 11:22:00|\n",
      "|  2|   Ali|zafar|   ali@gmail.com|   2021-02-01|2021-12-01 11:22:00|\n",
      "+---+------+-----+----------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### lower function should be passed a columns\n",
    "##\n",
    "#\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "| first|lower(last AS last)|\n",
      "+------+-------------------+\n",
      "|Asfand|              saeed|\n",
      "|   Ali|              zafar|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower,concat,concat_ws\n",
    "df.select ('first', lower(col('last').alias(\"last\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "| first|        last|\n",
      "+------+------------+\n",
      "|Asfand|Asfand-Saeed|\n",
      "|   Ali|   Ali-zafar|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select ('first', concat_ws('-',col('first'),col('last')).alias(\"last\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "| first|        last|\n",
      "+------+------------+\n",
      "|Asfand|asfand-SAEED|\n",
      "|   Ali|   ali-ZAFAR|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select ('first', concat_ws('-',lower(col('first')),upper(col('last'))).alias(\"last\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------+\n",
      "| first|       total|lengths|\n",
      "+------+------------+-------+\n",
      "|Asfand|Asfand saeed|     12|\n",
      "|   Ali|   Ali zafar|      9|\n",
      "+------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### initcap use to make first letter capital,length\n",
    "##\n",
    "#\n",
    "from pyspark.sql.functions import initcap,split\n",
    "df.select ('first',concat(initcap(col('first')),lit(' '),lower('last')).alias('total')).withColumn (\"lengths\",length(col('total'))).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **56. Extracting Strings using substring from Spark Data Frame Columns**\n",
    "1. *This is use to take character combination from fixlength columns but we can do for variable col as wel.*\n",
    "2. *substring(col,position_start,total_words)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----------------+-------------+-------------------+\n",
      "| id| first| last|           email|date_purchase|      time_purchase|\n",
      "+---+------+-----+----------------+-------------+-------------------+\n",
      "|  1|Asfand|Saeed|asfand@gmail.com|   2022-12-01|2022-12-01 11:22:00|\n",
      "|  2|   Ali|zafar|   ali@gmail.com|   2021-02-01|2021-12-01 11:22:00|\n",
      "+---+------+-----+----------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+------------------+-------------+-------------------+-----------+\n",
      "| id| first| last|             email|date_purchase|      time_purchase|email_users|\n",
      "+---+------+-----+------------------+-------------+-------------------+-----------+\n",
      "|  1|Asfand|Saeed|asfand12@gmail.com|   2022-12-01|2022-12-01 11:22:00|  gmail.com|\n",
      "|  2|   Ali|zafar|   ali34@gmail.com|   2021-02-01|2021-12-01 11:22:00|  gmail.com|\n",
      "+---+------+-----+------------------+-------------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').withColumn('email_users',substring (col('email'),-9,9)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **57. Extracting Strings using split from Spark Data Frame Columns**\n",
    "1. substring will take the characters, split(col,delimeter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+------------------+-------------+-------------------+-------+\n",
      "| id| first| last|             email|date_purchase|      time_purchase|user_id|\n",
      "+---+------+-----+------------------+-------------+-------------------+-------+\n",
      "|  1|Asfand|Saeed|asfand12@gmail.com|   2022-12-01|2022-12-01 11:22:00|  asfan|\n",
      "|  2|   Ali|zafar|   ali34@gmail.com|   2021-02-01|2021-12-01 11:22:00|  ali34|\n",
      "+---+------+-----+------------------+-------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').withColumn('user_id',substring((split(col('email'),'@')[0]),1,5)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **58. Padding Characters around strings in Spark Data Frame Columns**\n",
    "1. lpad (col,length_of_character_after_which_padding_is_to_initiate,'+92')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+\n",
      "| id|        first| last|             email|date_purchase|      time_purchase|       phones|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+\n",
      "|  1|   Asfand    |Saeed|asfand12@gmail.com|   2022-12-01|2022-12-01 11:22:00|   3339555855|\n",
      "|  2|         Ali.|zafar|   ali34@gmail.com|   2021-02-01|2021-12-01 11:22:00|+923129677783|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- date_purchase: date (nullable = true)\n",
      " |-- time_purchase: timestamp (nullable = true)\n",
      " |-- phones: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_list=[\n",
    "    (1,\n",
    "     \"   Asfand    \",\n",
    "     \"Saeed\",\n",
    "     \"asfand12@gmail.com\",\n",
    "     datetime.date(2022,12,1),\n",
    "     datetime.datetime(2022,12,1,11,22),\n",
    "     '3339555855'),\n",
    "    (2,\n",
    "     \"  Ali.\",\n",
    "     \"zafar\",\n",
    "     \"ali34@gmail.com\",\n",
    "     datetime.date(2021,2,1),\n",
    "     datetime.datetime(2021,12,1,11,22),\n",
    "     '+923129677783')\n",
    "]\n",
    "scha=['id','first','last','email','date_purchase','time_purchase','phones']\n",
    "df=spark.createDataFrame(names_list,scha)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+------------------+-------------+-------------------+-------------+-------------+\n",
      "| id| first| last|             email|date_purchase|      time_purchase|       phones|        phone|\n",
      "+---+------+-----+------------------+-------------+-------------------+-------------+-------------+\n",
      "|  1|Asfand|Saeed|asfand12@gmail.com|   2022-12-01|2022-12-01 11:22:00|   3339555855|+923339555855|\n",
      "|  2|   Ali|zafar|   ali34@gmail.com|   2021-02-01|2021-12-01 11:22:00|+923129677783|+923129677783|\n",
      "+---+------+-----+------------------+-------------+-------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select('*').withColumn('phone',lpad(col('phones'),13,'+92')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **59. Trimming Characters from strings in Spark Data Frame Columns**\n",
    "1. ltrim for removing spaces from left.\n",
    "2. rtrim for removing spaces from right\n",
    "3. trim for removing spaces from both side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+----------+---------+------+\n",
      "| id|        first| last|             email|date_purchase|      time_purchase|       phones|     ltrim|    rtrim|  Both|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+----------+---------+------+\n",
      "|  1|   Asfand    |Saeed|asfand12@gmail.com|   2022-12-01|2022-12-01 11:22:00|   3339555855|Asfand    |   Asfand|Asfand|\n",
      "|  2|         Ali.|zafar|   ali34@gmail.com|   2021-02-01|2021-12-01 11:22:00|+923129677783|      Ali.|     Ali.|  Ali.|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('ltrim',ltrim('first')).withColumn(\"rtrim\",rtrim('first')).withColumn('Both',trim('first')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+---------+\n",
      "| id|        first| last|             email|date_purchase|      time_purchase|       phones|    rtrim|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+---------+\n",
      "|  1|   Asfand    |Saeed|asfand12@gmail.com|   2022-12-01|2022-12-01 11:22:00|   3339555855|   Asfand|\n",
      "|  2|         Ali.|zafar|   ali34@gmail.com|   2021-02-01|2021-12-01 11:22:00|+923129677783|      Ali|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Expr is used to get the extra out of spark sql API\n",
    "##\n",
    "#\n",
    "df.withColumn('rtrim',expr(\"trim(TRAILING '.' FROM rtrim(first))\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2354ce52050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **60. Date and Time Manipulation Functions using Spark Data Frames**\n",
    "1. Standard date format yyy-MM-dd.\n",
    "2. Standard timestamp format yyy-yMM-dd HH:mm:ss:SSS.\n",
    "3. hours are bydefault 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+\n",
      "| id|        first| last|             email|date_purchase|      time_purchase|       phones|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+\n",
      "|  1|   Asfand    |Saeed|asfand12@gmail.com|   2022-12-01|2022-12-01 11:22:00|   3339555855|\n",
      "|  2|         Ali.|zafar|   ali34@gmail.com|   2021-02-01|2021-12-01 11:22:00|+923129677783|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+---------------------+\n",
      "|id |first        |last |email             |date_purchase|time_purchase      |phones       |date    |time                 |\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+---------------------+\n",
      "|1  |   Asfand    |Saeed|asfand12@gmail.com|2022-12-01   |2022-12-01 11:22:00|3339555855   |20220110|20220111 11:22:21:234|\n",
      "|2  |  Ali.       |zafar|ali34@gmail.com   |2021-02-15   |2021-12-16 11:22:00|+923129677783|20220112|20220113 13:22:21:234|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+---------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- date_purchase: date (nullable = true)\n",
      " |-- time_purchase: timestamp (nullable = true)\n",
      " |-- phones: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "names_list=[\n",
    "    (1,\n",
    "     \"   Asfand    \",\n",
    "     \"Saeed\",\n",
    "     \"asfand12@gmail.com\",\n",
    "     datetime.date(2022,12,1),\n",
    "     datetime.datetime(2022,12,1,11,22),\n",
    "     '3339555855',\n",
    "     '20220110',\n",
    "     '20220111 11:22:21:234'),\n",
    "    (2,\n",
    "     \"  Ali.\",\n",
    "     \"zafar\",\n",
    "     \"ali34@gmail.com\",\n",
    "     datetime.date(2021,2,15),\n",
    "     datetime.datetime(2021,12,16,11,22),\n",
    "     '+923129677783',\n",
    "     '20220112',\n",
    "     '20220113 13:22:21:234')\n",
    "]\n",
    "scha=['id','first','last','email','date_purchase','time_purchase','phones','date','time']\n",
    "df=spark.createDataFrame(names_list,scha)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+-----------------------+----------+\n",
      "|id |first        |last |email             |date_purchase|time_purchase      |phones       |date    |time                   |dates     |\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+-----------------------+----------+\n",
      "|1  |   Asfand    |Saeed|asfand12@gmail.com|2022-12-01   |2022-12-01 11:22:00|3339555855   |20220110|2022-01-11 11:22:21.234|2022-01-10|\n",
      "|2  |  Ali.       |zafar|ali34@gmail.com   |2021-02-15   |2021-12-16 11:22:00|+923129677783|20220112|2022-01-13 13:22:21.234|2022-01-12|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+-----------------------+----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- date_purchase: date (nullable = true)\n",
      " |-- time_purchase: timestamp (nullable = true)\n",
      " |-- phones: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- dates: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('dates',to_date(col('date'),'yyyyMMdd')).withColumn('time',to_timestamp(col('time'),'yyyyMMdd HH:mm:ss:SSS')).show(truncate=False)\n",
    "df.withColumn('dates',to_date(col('date'),'yyyyMMdd')).withColumn('time',to_timestamp(col('time'),'yyyyMMdd HH:mm:ss:SSS')).printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **62. Using date and time trunc functions on Spark Data Frames**\n",
    "1. trunc(col(),formate)\n",
    "2. date_trunc(format,col())\n",
    "3. withColumn('days',date_trunc(\"dd\",col(\"time_purchase\"))) start of day\n",
    "4. date_trunc('HOUR',col('time_purchase')) start of the hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+--------------------+-----------+----------+\n",
      "| id|        first| last|             email|date_purchase|      time_purchase|       phones|    date|                time|month_start|year_start|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+--------------------+-----------+----------+\n",
      "|  1|   Asfand    |Saeed|asfand12@gmail.com|   2022-12-01|2022-12-01 11:22:00|   3339555855|20220110|20220111 11:22:21...| 2022-12-01|2022-01-01|\n",
      "|  2|         Ali.|zafar|   ali34@gmail.com|   2021-02-15|2021-12-16 11:22:00|+923129677783|20220112|20220113 13:22:21...| 2021-02-01|2021-01-01|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+--------------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.withColumn('month_start',trunc(col('date_purchase'),'mon')).withColumn('year_start',trunc(col('time_purchase'),'year')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+--------------------+-------------------+-------------------+\n",
      "| id|        first| last|             email|date_purchase|      time_purchase|       phones|    date|                time|        month_start|         year_start|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+--------------------+-------------------+-------------------+\n",
      "|  1|   Asfand    |Saeed|asfand12@gmail.com|   2022-12-01|2022-12-01 11:22:00|   3339555855|20220110|20220111 11:22:21...|2022-12-01 00:00:00|2022-01-01 00:00:00|\n",
      "|  2|         Ali.|zafar|   ali34@gmail.com|   2021-02-15|2021-12-16 11:22:00|+923129677783|20220112|20220113 13:22:21...|2021-02-01 00:00:00|2021-01-01 00:00:00|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### date_trunc(format,col) this return timestamp when given date.\n",
    "##\n",
    "#\n",
    "df.withColumn('month_start',date_trunc('mon',col('date_purchase'))).withColumn('year_start',date_trunc('year',col('time_purchase'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+--------------------+-------------------+-------------------+-------------------+\n",
      "| id|        first| last|             email|date_purchase|      time_purchase|       phones|    date|                time|        month_start|         year_start|               days|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+--------------------+-------------------+-------------------+-------------------+\n",
      "|  1|   Asfand    |Saeed|asfand12@gmail.com|   2022-12-01|2022-12-01 11:22:00|   3339555855|20220110|20220111 11:22:21...|2022-12-01 00:00:00|2022-12-01 11:00:00|2022-12-01 00:00:00|\n",
      "|  2|         Ali.|zafar|   ali34@gmail.com|   2021-02-15|2021-12-16 11:22:00|+923129677783|20220112|20220113 13:22:21...|2021-02-15 00:00:00|2021-12-16 11:00:00|2021-12-16 00:00:00|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('month_start',date_trunc('HOUR',col('date_purchase'))).withColumn('year_start',date_trunc('HOUR',col('time_purchase'))).\\\n",
    "    withColumn('days',date_trunc(\"dd\",col(\"time_purchase\"))).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **63. Date and Time Extract Functions on Spark Data Frames**\n",
    "1. year\n",
    "2. month\n",
    "3. weekofyear\n",
    "4. dayofyear\n",
    "5. dayofmonth\n",
    "6. dayofweek\n",
    "7. hour\n",
    "8. minute\n",
    "9. second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+-----+----------+---------+---------+----+-------+------+\n",
      "|current                |year|month|weekofyear|dayofyear|dayofweek|hour|minutes|second|\n",
      "+-----------------------+----+-----+----------+---------+---------+----+-------+------+\n",
      "|2022-12-31 16:53:59.764|2022|12   |52        |365      |7        |16  |53     |59    |\n",
      "|2022-12-31 16:53:59.764|2022|12   |52        |365      |7        |16  |53     |59    |\n",
      "+-----------------------+----+-----+----------+---------+---------+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select (current_timestamp().alias('current')).withColumn('year',year(col('current'))).\\\n",
    "    withColumn('year',year(col('current'))).\\\n",
    "    withColumn('month',month(col('current'))).\\\n",
    "    withColumn('weekofyear',weekofyear(col('current'))).\\\n",
    "    withColumn('dayofyear',dayofyear(col('current'))).\\\n",
    "    withColumn('dayofweek',dayofweek(col('current'))).\\\n",
    "    withColumn('hour',hour(col('current'))).\\\n",
    "    withColumn('minutes',minute(col('current'))).\\\n",
    "    withColumn('second',second(col('current'))).\\\n",
    "                    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- current: timestamp (nullable = false)\n",
      " |-- year: integer (nullable = false)\n",
      " |-- month: integer (nullable = false)\n",
      " |-- weekofyear: integer (nullable = false)\n",
      " |-- dayofyear: integer (nullable = false)\n",
      " |-- dayofweek: integer (nullable = false)\n",
      " |-- hour: integer (nullable = false)\n",
      " |-- minutes: integer (nullable = false)\n",
      " |-- second: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (current_timestamp().alias('current')).withColumn('year',year(col('current'))).\\\n",
    "    withColumn('year',year(col('current'))).\\\n",
    "    withColumn('month',month(col('current'))).\\\n",
    "    withColumn('weekofyear',weekofyear(col('current'))).\\\n",
    "    withColumn('dayofyear',dayofyear(col('current'))).\\\n",
    "    withColumn('dayofweek',dayofweek(col('current'))).\\\n",
    "    withColumn('hour',hour(col('current'))).\\\n",
    "    withColumn('minutes',minute(col('current'))).\\\n",
    "    withColumn('second',second(col('current'))).\\\n",
    "                    printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **64. Using to_date and to_timestamp on Spark Data Frames**\n",
    "1. standard date yyyy-MM-dd\n",
    "2. standard timestamp yyyy-MM-dd HH:mm:ss.SSS\n",
    "3. to_date use to convert the non standard date to standard.\n",
    "4. to_timestamp use to convert the non standard timestamp to standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|    date|   to_date|\n",
      "+--------+----------+\n",
      "|20220122|2022-01-22|\n",
      "|20220122|2022-01-22|\n",
      "+--------+----------+\n",
      "\n",
      "root\n",
      " |-- date: string (nullable = false)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (lit('20220122').alias('date')).withColumn('to_date',to_date(col('date'),'yyyyMMdd')).show()\n",
    "df.select (lit('20220122').alias('date')).withColumn('to_date',to_date(col('date'),'yyyyMMdd')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|    date|            to_date|\n",
      "+--------+-------------------+\n",
      "|20220122|2022-01-22 00:00:00|\n",
      "|20220122|2022-01-22 00:00:00|\n",
      "+--------+-------------------+\n",
      "\n",
      "root\n",
      " |-- date: string (nullable = false)\n",
      " |-- to_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (lit('20220122').alias('date')).withColumn('to_date',to_timestamp(col('date'),'yyyyMMdd')).show()\n",
    "df.select (lit('20220122').alias('date')).withColumn('to_date',to_timestamp(col('date'),'yyyyMMdd')).printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|         date|   to_date|\n",
      "+-------------+----------+\n",
      "|02,March 2022|2022-03-02|\n",
      "|02,March 2022|2022-03-02|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lit('02,March 2022').alias('date')).withColumn('to_date',to_date(col('date'),'dd,MMMM yyyy')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                date|   to_date|\n",
      "+--------------------+----------+\n",
      "|02,March 2022 13:...|2022-03-02|\n",
      "|02,March 2022 13:...|2022-03-02|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lit('02,March 2022 13:22:22.234').alias('date')).withColumn('to_date',to_date(col('date'),'dd,MMMM yyyy HH:mm:ss.SSS')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                date|        to_timestamp|\n",
      "+--------------------+--------------------+\n",
      "|02,March 2022 13:...|2022-03-02 13:22:...|\n",
      "|02,March 2022 13:...|2022-03-02 13:22:...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- date: string (nullable = false)\n",
      " |-- to_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lit('02,March 2022 13:22:22.234').alias('date')).withColumn('to_timestamp',to_timestamp(col('date'),'dd,MMMM yyyy HH:mm:ss.SSS')).show()\n",
    "df.select(lit('02,March 2022 13:22:22.234').alias('date')).withColumn('to_timestamp',to_timestamp(col('date'),'dd,MMMM yyyy HH:mm:ss.SSS')).printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **65. Using date_format Function on Spark Data Frames**\n",
    "1. date_format will change the datetype and timestamp column to the desired format of the date\n",
    "2. We can use the function of year month weekofmonth etc discussed above but we can do thing with this as well.\n",
    "3. yyyy\n",
    "4. MM\n",
    "5. MMM (mar,apr formate)\n",
    "6. MMMM (march,April)\n",
    "7. dd (day of month)\n",
    "8. DD (day of year)\n",
    "9. HH (24 hour format)\n",
    "10. hh (12 hour format)\n",
    "11. mm (minute)\n",
    "12. ss (sec)\n",
    "13. SSS (mili sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+---+----+------+---+-------+--------------+----------------+\n",
      "|                date|year|month|day|Hour|minute|sec|12 hour|month in alpha|month in 3 alpha|\n",
      "+--------------------+----+-----+---+----+------+---+-------+--------------+----------------+\n",
      "|2022-12-31 17:52:...|2022|   12| 31|  17|    52| 13|     05|      December|             Dec|\n",
      "|2022-12-31 17:52:...|2022|   12| 31|  17|    52| 13|     05|      December|             Dec|\n",
      "+--------------------+----+-----+---+----+------+---+-------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lit(current_timestamp()).alias('date')).\\\n",
    "    withColumn('year',date_format(col('date'),'yyyy')).\\\n",
    "    withColumn('month',date_format(col('date'),'MM')).\\\n",
    "    withColumn('day',date_format(col('date'),'dd')).\\\n",
    "    withColumn('Hour',date_format(col('date'),'HH')).\\\n",
    "    withColumn('minute',date_format(col('date'),'mm')).\\\n",
    "    withColumn('sec',date_format(col('date'),'ss')).\\\n",
    "    withColumn('12 hour',date_format(col('date'),'hh')).\\\n",
    "    withColumn('month in alpha',date_format(col('date'),'MMMM')).\\\n",
    "    withColumn('month in 3 alpha',date_format(col('date'),'MMM')).\\\n",
    "show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|                date|year|\n",
      "+--------------------+----+\n",
      "|2022-12-31 17:40:...|2022|\n",
      "|2022-12-31 17:40:...|2022|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lit(current_timestamp()).alias('date')).\\\n",
    "    withColumn('year',date_format(col('date'),'yyyy')).\\\n",
    "    show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **66. Dealing with Unix Timestamp in Spark Data Frames**\n",
    "1. unix_timestamp() to convert the timestamp to unix\n",
    "2. in unix_timestamp()  you have to pass string/timestamp or if there is any non standard you have to show format to unix and if you pass date still pass format\n",
    "3. from_timestamp() to convert to time_stamp from unix and if no format is shown it will return standard timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+--------------------+\n",
      "|date_string|   strings| date_type|           timestamp|\n",
      "+-----------+----------+----------+--------------------+\n",
      "| 2022-02-11|1672491840|2022-12-31|2022-12-31 18:10:...|\n",
      "| 2022-02-11|1672491840|2022-12-31|2022-12-31 18:10:...|\n",
      "+-----------+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff=df.select(lit('2022-02-11').alias('date_string'),lit('1672491840').alias('strings'),lit(current_date()).alias('date_type'),lit(current_timestamp()).alias('timestamp'))\n",
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+--------------------+--------------+------------+------------+\n",
      "|date_string|   strings| date_type|           timestamp|to_unix_string|to_unix_date|to_unix_time|\n",
      "+-----------+----------+----------+--------------------+--------------+------------+------------+\n",
      "| 2022-02-11|1672491840|2022-12-31|2022-12-31 18:10:...|    1644519600|  1672426800|  1672492214|\n",
      "| 2022-02-11|1672491840|2022-12-31|2022-12-31 18:10:...|    1644519600|  1672426800|  1672492214|\n",
      "+-----------+----------+----------+--------------------+--------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unix=dff.withColumn('to_unix_string',unix_timestamp(col('date_string'),'yyyy-MM-dd')).\\\n",
    "    withColumn('to_unix_date',unix_timestamp(col('date_type'))).\\\n",
    "        withColumn('to_unix_time',unix_timestamp(col('timestamp')))\n",
    "unix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+--------------------+--------------+------------+------------+-------------------+-------------------+\n",
      "|date_string|   strings| date_type|           timestamp|to_unix_string|to_unix_date|to_unix_time| from_time_standard|   from_string_unix|\n",
      "+-----------+----------+----------+--------------------+--------------+------------+------------+-------------------+-------------------+\n",
      "| 2022-02-11|1672491840|2022-12-31|2022-12-31 18:11:...|    1644519600|  1672426800|  1672492311|2022-12-31 18:11:51|2022-12-31 18:04:00|\n",
      "| 2022-02-11|1672491840|2022-12-31|2022-12-31 18:11:...|    1644519600|  1672426800|  1672492311|2022-12-31 18:11:51|2022-12-31 18:04:00|\n",
      "+-----------+----------+----------+--------------------+--------------+------------+------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unix.withColumn('from_time_standard',from_unixtime(col('to_unix_time'))).\\\n",
    "    withColumn('from_string_unix',from_unixtime(col('strings'))).\\\n",
    "    show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **67. Dealing with nulls in Spark Data Frames**\n",
    "1. coalesce()\n",
    "2. nvl()\n",
    "3. na.fill(0.0)\n",
    "4. df.fillna(0.0,'salary')\n",
    "5. na.fill (value=some,subset=[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2| 13|\n",
      "|  3| 18|\n",
      "|  4| 60|\n",
      "|  5|120|\n",
      "|  6|  0|\n",
      "|  7| 12|\n",
      "|  8|160|\n",
      "+---+---+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person=[\n",
    "    (1,1),\n",
    "    (2,13),\n",
    "    (3,18),\n",
    "    (4,60),\n",
    "    (5,120),\n",
    "    (6,0),\n",
    "    (7,12),\n",
    "    (8,160)\n",
    "    \n",
    "]\n",
    "col=['id','age']\n",
    "df=spark.createDataFrame(person,schema=col)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NaN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asfandyar\\Desktop\\python_Asfandyar\\40 days\\chilla\\Spark\\Certification Spark\\Spark_developer.ipynb Cell 221\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatetime\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m names_list\u001b[39m=\u001b[39m[\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     (\u001b[39m1\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m      \u001b[39m\"\u001b[39m\u001b[39m   Asfand    \u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m      \u001b[39m\"\u001b[39m\u001b[39mSaeed\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m      \u001b[39m\"\u001b[39m\u001b[39masfand12@gmail.com\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m      datetime\u001b[39m.\u001b[39mdate(\u001b[39m2022\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m1\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m      datetime\u001b[39m.\u001b[39mdatetime(\u001b[39m2022\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m11\u001b[39m,\u001b[39m22\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m      \u001b[39m'\u001b[39m\u001b[39m3339555855\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m      \u001b[39m'\u001b[39m\u001b[39m20220110\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m      \u001b[39m'\u001b[39m\u001b[39m20220111 11:22:21:234\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m      \u001b[39m3000\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     (\u001b[39m2\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m      \u001b[39m\"\u001b[39m\u001b[39m  Ali.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m      \u001b[39m\"\u001b[39m\u001b[39mzafar\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m      \u001b[39m\"\u001b[39m\u001b[39mali34@gmail.com\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m      datetime\u001b[39m.\u001b[39mdate(\u001b[39m2021\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m15\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m      datetime\u001b[39m.\u001b[39mdatetime(\u001b[39m2021\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m16\u001b[39m,\u001b[39m11\u001b[39m,\u001b[39m22\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m      \u001b[39m'\u001b[39m\u001b[39m+923129677783\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m      \u001b[39m'\u001b[39m\u001b[39m20220112\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m      \u001b[39m'\u001b[39m\u001b[39m20220113 13:22:21:234\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m      \u001b[39m2000\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     (\u001b[39m3\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m      \u001b[39m\"\u001b[39m\u001b[39m  Aliz.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m      \u001b[39m\"\u001b[39m\u001b[39msasa\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m      \u001b[39m\"\u001b[39m\u001b[39masa@gmail.com\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m      datetime\u001b[39m.\u001b[39mdate(\u001b[39m2021\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m15\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m      datetime\u001b[39m.\u001b[39mdatetime(\u001b[39m2021\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m16\u001b[39m,\u001b[39m11\u001b[39m,\u001b[39m22\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m      \u001b[39m'\u001b[39m\u001b[39m+924122677783\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m      \u001b[39m'\u001b[39m\u001b[39m20220112\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m      \u001b[39m'\u001b[39m\u001b[39m20220113 13:22:21:234\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m      NaN)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m scha\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlast\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39memail\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdate_purchase\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtime_purchase\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mphones\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mamount\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y443sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m df\u001b[39m=\u001b[39mspark\u001b[39m.\u001b[39mcreateDataFrame(names_list,scha)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NaN' is not defined"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "names_list=[\n",
    "    (1,\n",
    "     \"   Asfand    \",\n",
    "     \"Saeed\",\n",
    "     \"asfand12@gmail.com\",\n",
    "     datetime.date(2022,12,1),\n",
    "     datetime.datetime(2022,12,1,11,22),\n",
    "     '3339555855',\n",
    "     '20220110',\n",
    "     '20220111 11:22:21:234',\n",
    "     3000),\n",
    "    (2,\n",
    "     \"  Ali.\",\n",
    "     \"zafar\",\n",
    "     \"ali34@gmail.com\",\n",
    "     datetime.date(2021,2,15),\n",
    "     datetime.datetime(2021,12,16,11,22),\n",
    "     '+923129677783',\n",
    "     '20220112',\n",
    "     '20220113 13:22:21:234',\n",
    "     2000),\n",
    "    (3,\n",
    "     \"  Aliz.\",\n",
    "     \"sasa\",\n",
    "     \"asa@gmail.com\",\n",
    "     datetime.date(2021,2,15),\n",
    "     datetime.datetime(2021,12,16,11,22),\n",
    "     '+924122677783',\n",
    "     '20220112',\n",
    "     '20220113 13:22:21:234',\n",
    "     NaN)\n",
    "]\n",
    "scha=['id','first','last','email','date_purchase','time_purchase','phones','date','time','amount']\n",
    "df=spark.createDataFrame(names_list,scha)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---------------+-------------+-------------------+-------------+--------+--------------------+------+\n",
      "| id| first| last|          email|date_purchase|      time_purchase|       phones|    date|                time|amount|\n",
      "+---+------+-----+---------------+-------------+-------------------+-------------+--------+--------------------+------+\n",
      "|  2|  Ali.|zafar|ali34@gmail.com|   2021-02-15|2021-12-16 11:22:00|+923129677783|20220112|20220113 13:22:21...|  2000|\n",
      "+---+------+-----+---------------+-------------+-------------------+-------------+--------+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asfandyar\\Desktop\\python_Asfandyar\\40 days\\chilla\\Spark\\Certification Spark\\Spark_developer.ipynb Cell 222\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y444sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39mfilter(\u001b[39m\"\u001b[39m\u001b[39mamount == \u001b[39m\u001b[39m'\u001b[39m\u001b[39m2000\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mshow()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y444sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mfilter(col(\u001b[39m'\u001b[39;49m\u001b[39mamount\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39misin(\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "df.filter(\"amount == '2000'\").show()\n",
    "df.filter(col('amount').isin(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asfandyar\\Desktop\\python_Asfandyar\\40 days\\chilla\\Spark\\Certification Spark\\Spark_developer.ipynb Cell 221\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Spark_developer.ipynb#Y436sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m,when ((col(\u001b[39m'\u001b[39;49m\u001b[39mage\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m&\u001b[39m (col(\u001b[39m'\u001b[39m\u001b[39mage\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m<\u001b[39m\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\u001b[39m'\u001b[39m\u001b[39mNew_Born\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39motherwise(\u001b[39m\"\u001b[39m\u001b[39mno\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "df.withColumn('category',when ((col('age')>=0) & (col('age')<=2),'New_Born').otherwise(\"no\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+---------------------+\n",
      "|id |first        |last |email             |date_purchase|time_purchase      |phones       |date    |time                 |\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+---------------------+\n",
      "|1  |   Asfand    |Saeed|asfand12@gmail.com|2022-12-01   |2022-12-01 11:22:00|3339555855   |20220110|20220111 11:22:21:234|\n",
      "|2  |  Ali.       |zafar|ali34@gmail.com   |2021-02-15   |2021-12-16 11:22:00|+923129677783|20220112|20220113 13:22:21:234|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+---------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- date_purchase: date (nullable = true)\n",
      " |-- time_purchase: timestamp (nullable = true)\n",
      " |-- phones: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "names_list=[\n",
    "    (1,\n",
    "     \"   Asfand    \",\n",
    "     \"Saeed\",\n",
    "     \"asfand12@gmail.com\",\n",
    "     datetime.date(2022,12,1),\n",
    "     datetime.datetime(2022,12,1,11,22),\n",
    "     '3339555855',\n",
    "     '20220110',\n",
    "     '20220111 11:22:21:234'),\n",
    "    (2,\n",
    "     \"  Ali.\",\n",
    "     \"zafar\",\n",
    "     \"ali34@gmail.com\",\n",
    "     datetime.date(2021,2,15),\n",
    "     datetime.datetime(2021,12,16,11,22),\n",
    "     '+923129677783',\n",
    "     '20220112',\n",
    "     '20220113 13:22:21:234')\n",
    "]\n",
    "scha=['id','first','last','email','date_purchase','time_purchase','phones','date','time']\n",
    "df=spark.createDataFrame(names_list,scha)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+--------------------+\n",
      "| id|        first| last|             email|date_purchase|      time_purchase|       phones|    date|                time|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+--------------------+\n",
      "|  1|   Asfand    |Saeed|asfand12@gmail.com|   2022-12-01|2022-12-01 11:22:00|   3339555855|20220110|20220111 11:22:21...|\n",
      "|  2|         Ali.|zafar|   ali34@gmail.com|   2021-02-15|2021-12-16 11:22:00|+923129677783|20220112|20220113 13:22:21...|\n",
      "+---+-------------+-----+------------------+-------------+-------------------+-------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"id BETWEEN 1 AND 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"id IN (1 , 2)\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SQL Advance**\n",
    "### **TASK 1**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "courses_df has\n",
      "\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n",
      "users_df has\n",
      "\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "\n",
      "course_enrolments_df has\n",
      "\n",
      "+-------------------+-------+---------+----------+\n",
      "|course_enrolment_id|user_id|course_id|price_paid|\n",
      "+-------------------+-------+---------+----------+\n",
      "|                  1|     10|        2|      9.99|\n",
      "|                  2|      5|        2|      9.99|\n",
      "|                  3|      7|        5|     10.99|\n",
      "|                  4|      9|        2|      9.99|\n",
      "|                  5|      8|        2|      9.99|\n",
      "|                  6|      5|        5|     10.99|\n",
      "|                  7|      4|        5|     10.99|\n",
      "|                  8|      7|        3|     10.99|\n",
      "|                  9|      8|        5|     10.99|\n",
      "|                 10|      3|        3|     10.99|\n",
      "|                 11|      7|        5|     10.99|\n",
      "|                 12|      3|        2|      9.99|\n",
      "|                 13|      5|        2|      9.99|\n",
      "|                 14|      4|        3|     10.99|\n",
      "|                 15|      8|        2|      9.99|\n",
      "+-------------------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./joins_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|summons_number|violation_time|\n",
      "+--------------+--------------+\n",
      "|    1447152402|         1011A|\n",
      "|    1447152554|         0107A|\n",
      "|    1447152580|         0300A|\n",
      "|    1447152724|         0653A|\n",
      "|    1447152992|         0515P|\n",
      "|    1447153315|         0524P|\n",
      "|    1447153327|         0601P|\n",
      "|    1447153340|         0935A|\n",
      "|    1447153352|         1217P|\n",
      "|    1447153649|         0049A|\n",
      "+--------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc=['summons_number','violation_time']\n",
    "data=[('1447152402','1011A'),\n",
    "('1447152554','0107A'),('1447152580','0300A'),('1447152724','0653A'),('1447152992','0515P'),('1447153315','0524P'),('1447153327','0601P'),\n",
    "('1447153340','0935A'),('1447153352','1217P'),('1447153649','0049A'),('1447153789','0046A'),('1447153790','0124A'),('1447153819','0500A'),\n",
    "('1447153820','1221A'),('1447153844','1241A'),('1447154198','1110A'),('1447154411','0105A'),('1447155294','1000P'),('1447155427','0843A'),\n",
    "('1447155750','0955P'),('1447155877','0508P'),('1447156183','1100A'),('1447156195','1100A'),('1447156201','1100A'),('1447156614','0718A'),\n",
    "('1447156626','0722A'),('1447156638','0726A'),('1447157000','0430A'),('1447157060','0615P'),('1447168951','0115A'),('1447168975','0350A')]\n",
    "df=spark.createDataFrame(data,schema=sc)\n",
    "df.show(n=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+---+\n",
      "|summons_number|violation_time|new|\n",
      "+--------------+--------------+---+\n",
      "|    1447152402|         1011A|  1|\n",
      "|    1447152554|         0107A|  1|\n",
      "|    1447152580|         0300A|  1|\n",
      "|    1447152724|         0653A|  1|\n",
      "|    1447152992|         0515P|  0|\n",
      "+--------------+--------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new',expr(\"case when violation_time rlike '\\\\\\\\d\\\\\\\\d\\\\\\\\d\\\\\\\\dA' then 1 else 0 end\" )).show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+---+\n",
      "|summons_number|violation_time|new|\n",
      "+--------------+--------------+---+\n",
      "|    1447152402|         1011A|  1|\n",
      "|    1447152554|         0107A|  1|\n",
      "|    1447152580|         0300A|  1|\n",
      "|    1447152724|         0653A|  1|\n",
      "|    1447152992|         0515P|  0|\n",
      "+--------------+--------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new',when(col('violation_time').rlike(r'\\d\\d\\d\\dA$'),1).otherwise(0)).show(n=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TASK-2**\n",
    "* replace Alphabet with - in plate_id column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=['summons_number',\t'plate_id']\n",
    "data=[(1447152396\t,'JET2661'),(1447152402\t,'JCV6523'),(1447152554\t,'GMK6954'),(1447152580\t,'JGX1641'),(1447152724\t,'GDM8069'),\n",
    "(1447152992\t,'HXH5242'),(1447153315\t,'HXM3470'),(1447153327\t,'GWH9640'),(1447153340\t,'HKB1769'),(1447153352\t,'GDH2184'),(1447153649\t,'JCA5331'),\n",
    "(1447153789\t,'JFW5006'),(1447153790\t,'HGR2634'),(1447153819\t,'GYM7645'),(1447153820\t,'KHW5523'),(1447153844\t,'JGX7169'),(1447154198\t,'JGL4948'),\n",
    "(1447154411\t,'JKSN62'  ),(1447155294\t,'GMC1999'),(1447155427\t,'JEL8372'),(1447155750\t,'GHP8968'),(1447155877\t,'KHG6053'),(1447156183\t,'JCC9316'),\n",
    "(1447156195\t,'HMP3112'),(1447156201\t,'JFG3676'),(1447156614\t,'JCR3754'),(1447156626\t,'HXH7367'),(1447156638\t,'HXH5563'),(1447157000\t,'HPA3075'),\n",
    "(1447157060\t,'GFT5393'),(1447168951\t,'HDH9057'),(1447168975\t,'JHU5285'),(1447169335\t,'JHX7405'),(1447170593\t,'HZU1090'),(1447171275\t,'JJG2614'),\n",
    "(1447171287\t,'GYU6351'),(1447174100\t,'L12LEG ' ),(1447174150\t,'GXT1533'),(1447174409\t,'HSC7008'),(1447177113\t,'JHS4392'),(1447182868\t,'GTW6638'),\n",
    "(1447184282\t,'JET2379'),(1447184294\t,'GXT3109'),(1447185640\t,'MINTED'  ),(1447185651,\t'MINTED'  ),(1447195747\t,'HCH5849'),(1447199789\t,'HUG9376')]\n",
    "df=spark.createDataFrame(data,schema=sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|summons_number|plate_id|\n",
      "+--------------+--------+\n",
      "|    1447152396| JET2661|\n",
      "|    1447152402| JCV6523|\n",
      "|    1447152554| GMK6954|\n",
      "|    1447152580| JGX1641|\n",
      "|    1447152724| GDM8069|\n",
      "+--------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------+\n",
      "|summons_number|plate_id|plate_id_new|\n",
      "+--------------+--------+------------+\n",
      "|    1447152396| JET2661|     ---2661|\n",
      "|    1447152402| JCV6523|     ---6523|\n",
      "|    1447152554| GMK6954|     ---6954|\n",
      "|    1447152580| JGX1641|     ---1641|\n",
      "|    1447152724| GDM8069|     ---8069|\n",
      "+--------------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('plate_id_new',regexp_replace(col('plate_id'),'[A-Z]','-')).show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+----+\n",
      "|summons_number|plate_id|code|\n",
      "+--------------+--------+----+\n",
      "|    1447152396| JET2661|J300|\n",
      "|    1447152402| JCV6523|J100|\n",
      "|    1447152554| GMK6954|G520|\n",
      "|    1447152580| JGX1641|J000|\n",
      "|    1447152724| GDM8069|G350|\n",
      "+--------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('code',soundex(col('plate_id'))).show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------+---------------------+--------------------------------+----------+\n",
      "|vehicle_color|comparison_vehicle_color|vehicle_color_soundex|comparison_vehicle_color_soundex|difference|\n",
      "+-------------+------------------------+---------------------+--------------------------------+----------+\n",
      "|        GREEN|                    GREY|                 G650|                            G600|         1|\n",
      "+-------------+------------------------+---------------------+--------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create a Spark DataFrame\n",
    "df = spark.createDataFrame([(\"GREEN\", \"GREY\")], [\"vehicle_color\", \"comparison_vehicle_color\"])\n",
    "\n",
    "# Convert the vehicle_color and comparison_vehicle_color columns to soundex codes\n",
    "df = df.withColumn(\"vehicle_color_soundex\", F.soundex(df.vehicle_color))\n",
    "df = df.withColumn(\"comparison_vehicle_color_soundex\", F.soundex(df.comparison_vehicle_color))\n",
    "\n",
    "# Calculate the Levenshtein distance between the vehicle_color and comparison_vehicle_color columns\n",
    "df = df.withColumn(\"difference\", F.levenshtein(df.vehicle_color_soundex, df.comparison_vehicle_color_soundex))\n",
    "\n",
    "# Print the result\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d=394\n",
      "k=396\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "s=97\n",
    "d=1\n",
    "k=99\n",
    "s_piece=[1,2,1]\n",
    "d_piece=[2,2,2]\n",
    "k_sum=[]\n",
    "k_sum_sum=0\n",
    "s_sum_sum=0\n",
    "d_sum_sum=0\n",
    "s_sum=[]\n",
    "d_sum=[]\n",
    "for i in piece:\n",
    "    k_sum.append(i*k)\n",
    "k_sum_sum=sum(k_sum)\n",
    "for ii in piece:\n",
    "    s_sum.append(ii*s)\n",
    "s_sum_sum=sum(s_sum)\n",
    "for iii in d_piece:\n",
    "    d_sum.append(iii*d)\n",
    "d_sum_sum=sum(d_sum)\n",
    "print(f'd={d_sum_sum+s_sum_sum}\\nk={k_sum_sum}')\n",
    "if k_sum_sum<=d_sum_sum+s_sum_sum:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')\n",
    "# if s_sum+d_sum >= k_sum_sum:\n",
    "    # print('yes')\n",
    "# else:\n",
    "    # print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd044dfe803f410830d2077cd20a7505658c0f80bea76037d58d809c64c95f16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
