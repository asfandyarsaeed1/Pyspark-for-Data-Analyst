{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spark For Data Engineers and Data Scientists**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Frame using Python collection and Pandas Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21a0245bb20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "! pip install --force-reinstall 'sqlalchemy < 2.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions in c:\\users\\asfandyar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\asfandyar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: typing_extensions\n",
      "Version: 4.7.1\n",
      "Summary: Backported and Experimental Type Hints for Python 3.7+\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \"Guido van Rossum, Jukka Lehtosalo, Łukasz Langa, Michael Lee\" <levkivskyi@gmail.com>\n",
      "License: \n",
      "Location: c:\\users\\asfandyar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: black, huggingface-hub, pydantic, SQLAlchemy\n",
      "Files removed: 1\n"
     ]
    }
   ],
   "source": [
    "# ! pip install pandas sqlalchemy mysql-connector-python\n",
    "! pip install --upgrade typing-extensions\n",
    "\n",
    "! pip uninstall typing-extensions\n",
    "! pip install typing-extensions\n",
    "! pip show typing-extensions\n",
    "! pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USER', 'HOST', 'CURRENT_CONNECTIONS', 'TOTAL_CONNECTIONS']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(None, None, 38, 53),\n",
       " ('event_scheduler', 'localhost', 1, 1),\n",
       " ('root', 'localhost', 3, 9)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# establishing the connection\n",
    "conn = mysql.connector.connect(\n",
    "    user='root',password='Huawei@1234qwe', host='localhost', port=3306, database='performance_schema')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('select * from accounts')\n",
    "result = cursor.fetchall()\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Display column names and data\n",
    "print(column_names)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"USER\", StringType(), True),\n",
    "    StructField(\"HOST\", StringType(), True),\n",
    "    StructField(\"CURRENT_CONNECTIONS\", IntegerType(), True),\n",
    "    StructField(\"TOTAL_CONNECTIONS\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=[(None, None, 38, 53),\n",
    " ('event_scheduler', 'localhost', 1, 1),\n",
    " ('root', 'localhost', 3, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              USER       HOST  CURRENT_CONNECTIONS  TOTAL_CONNECTIONS\n",
      "0             None       None                   38                 53\n",
      "1  event_scheduler  localhost                    1                  1\n",
      "2             root  localhost                    3                  8\n"
     ]
    }
   ],
   "source": [
    "pa=pd.DataFrame(nn,columns=column_names)\n",
    "print(pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = spark.createDataFrame(nn,cols)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------------------+-----------------+\n",
      "|           USER|     HOST|CURRENT_CONNECTIONS|TOTAL_CONNECTIONS|\n",
      "+---------------+---------+-------------------+-----------------+\n",
      "|           null|     null|                 38|               53|\n",
      "|event_scheduler|localhost|                  1|                1|\n",
      "|           root|localhost|                  3|                8|\n",
      "+---------------+---------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = spark.createDataFrame(nn,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FacadeDict({})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(mysql+pymysql://root:***@1234qwe@localhost:3306/mydb)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.bind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_list=[1,2,4,5,6,7,8]\n",
    "type(age_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a DataFrame\n",
    "We will use createDataFrame which is availabe on top of spark object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x17f59c77b20>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "    \n",
      "    When ``schema`` is a list of column names, the type of each column\n",
      "    will be inferred from ``data``.\n",
      "    \n",
      "    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "    from ``data``, which should be an RDD of either :class:`Row`,\n",
      "    :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      "    the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      "    Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "    \n",
      "    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      "    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 2.1.0\n",
      "       Added verifySchema.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`RDD` or iterable\n",
      "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      "        :class:`pandas.DataFrame`.\n",
      "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is None.  The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      "        ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
      "        We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
      "    samplingRatio : float, optional\n",
      "        the sample ratio of rows used for inferring\n",
      "    verifySchema : bool, optional\n",
      "        verify data types of every row against schema. Enabled by default.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> l = [('Alice', 1)]\n",
      "    >>> spark.createDataFrame(l).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).collect()\n",
      "    [Row(age=1, name='Alice')]\n",
      "    \n",
      "    >>> rdd = sc.parallelize(l)\n",
      "    >>> spark.createDataFrame(rdd).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      "    >>> df.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> person = rdd.map(lambda r: Person(*r))\n",
      "    >>> df2 = spark.createDataFrame(person)\n",
      "    >>> df2.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> df3 = spark.createDataFrame(rdd, schema)\n",
      "    >>> df3.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "    [Row(name='Alice', age=1)]\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    [Row(0=1, 1=2)]\n",
      "    \n",
      "    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "    [Row(a='Alice', b=1)]\n",
      "    >>> rdd = rdd.map(lambda row: row[1])\n",
      "    >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      "    [Row(value=1)]\n",
      "    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    Py4JJavaError: ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help (spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    4|\n",
      "|    5|\n",
      "|    6|\n",
      "|    7|\n",
      "|    8|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(age_list,'int').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    4|\n",
      "|    5|\n",
      "|    6|\n",
      "|    7|\n",
      "|    8|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "spark.createDataFrame(age_list,IntegerType()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = ['asfand','saeed','ali','salman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=spark.createDataFrame(names_list,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|asfand|\n",
      "| saeed|\n",
      "|   ali|\n",
      "|salman|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating Multi column using Python List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating single column out of list of tuples\n",
    "\n",
    "\n",
    "user_lists= [(2,),(1,),(3,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list=[(120,),(122,),(123,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (user_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark will throw an error if I pass just int and dont specify the name of the column\n",
    "sp=spark.createDataFrame(user_list,'id int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|120|\n",
      "|122|\n",
      "|123|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating multi column dataframe from list of tuples\n",
    "users_list=[(1,'asfand'),(2,'saeed'),(3,'ali')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|id int|name string|\n",
      "+------+-----------+\n",
      "|    11|     asfand|\n",
      "|    22|      saeed|\n",
      "|    33|       khan|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user=[(11,'asfand'),(22,'saeed'),(33,'khan')]\n",
    "spark.createDataFrame(user,['id int','name string']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp=spark.createDataFrame(users_list,['id int','name string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|id int|name string|\n",
      "+------+-----------+\n",
      "|     1|     asfand|\n",
      "|     2|      saeed|\n",
      "|     3|        ali|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id int: long (nullable = true)\n",
      " |-- name string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp=spark.createDataFrame(users_list,'id int,name string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Row Detail**\n",
    "* Spark data frame is nothing its collection of row objects. To see in detail I have used collect function that will convert the dataframe into python list of rows.\n",
    "* Row is constructed under SQL module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='asfand'), Row(id=2, name='saeed'), Row(id=3, name='ali')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql    import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row(*args, **kwargs)\n",
    "* First is list of arguments\n",
    "* Second is list of key , values arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23c6b497b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=[Row(id=1,name='asfand')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saeed'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows1=Row(2,'saeed')\n",
    "rows1[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(rows).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asfand'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asfand'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Converting List of List into Data Frame using ROW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Row(1, 'asfand')>, <Row(2, 'saeed')>, <Row(3, 'khan')>]\n",
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nas=[[1,'asfand'],[2,'saeed'],[3,'khan']]\n",
    "### convert into row and then to dataframe\n",
    "nass=[ Row(*i)  for i in nas]\n",
    "print (nass)\n",
    "spark.createDataFrame(nass,'id int,name string').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Converting List of Tuples into Data Frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_list=[(1,'asfand'),(2,'saeed'),(3,'ali')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Converting the list of tuples into dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23c6b497b20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|id int|names string|\n",
      "+------+------------+\n",
      "|     1|      asfand|\n",
      "|     2|       saeed|\n",
      "|     3|         ali|\n",
      "+------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'asfand'), (2, 'saeed'), (3, 'ali')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(users_list,['id int','names string']).show()\n",
    "users_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp=spark.createDataFrame(users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| _1|    _2|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Converting List of Tuples into List of Rows and Converting into DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumy (*list):\n",
    "    print(list)\n",
    "    print (len(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 'asfand'),)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# if we dont put * at the start it will make it single unit as an out put.\n",
    "users=(1,'asfand')\n",
    "dumy (users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'asfand')\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# by having the number of argument 2 we can make it two columns with one row \n",
    "dumy (*users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do the row comprehansions\n",
    "user= [Row(*us) for us in users_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Row(1, 'asfand')>, <Row(2, 'saeed')>, <Row(3, 'ali')>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we go with the list of Rows\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp=spark.createDataFrame(user,'id int , names string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| names|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- names: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert List of Dicts ito Spark DataFrame Using Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dicts=[\n",
    "    {'id' : 1 , 'name' : 'asfand'}\n",
    "    ,{'id' : 2 , 'name' :'saeed'}\n",
    "    ,{'id' : 3 , 'name' : 'ali'}\n",
    "    ,{'id' : 4 , 'name' :'khan'}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(user_dicts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Converting the list of dicts via \\*args and \\*\\*args**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([2, 'saeed'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To understand take one element and convert it into Row then we will use for loop for whole list of Dicts\n",
    "usr=user_dicts[1].values()\n",
    "usr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(*list):\n",
    "    print(list)\n",
    "    print(len(list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(dict_values([2, 'saeed']),)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "dump(usr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'saeed')\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "dump(*usr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr1=[Row(*us.values()) for us in user_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Row(1, 'asfand')>, <Row(2, 'saeed')>, <Row(3, 'ali')>, <Row(4, 'khan')>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp=spark.createDataFrame(usr1, 'id int ,user_name string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|user_name|\n",
      "+---+---------+\n",
      "|  1|   asfand|\n",
      "|  2|    saeed|\n",
      "|  3|      ali|\n",
      "|  4|     khan|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2, 'name': 'saeed'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_dicts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr11= user_dicts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumps(**ars):\n",
    "    print (ars)\n",
    "    print(len(ars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 2, 'name': 'saeed'}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# this has considered id and name as 2 different arguments\n",
    "dumps(**user_dicts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all the dataset we have below things.\n",
    "dp=[Row (**us) for us in user_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='asfand'),\n",
       " Row(id=2, name='saeed'),\n",
       " Row(id=3, name='ali'),\n",
       " Row(id=4, name='khan')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp1=spark.createDataFrame(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dp1.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **List of Dic to Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1=[\n",
    "    {\"id\":1,\n",
    "     \"name\": 'asfand',\n",
    "     \"email\" : 'asfand@addo.ai',\n",
    "     \"is_customer\": True,\n",
    "     \"amount\":1000.32,\n",
    "     'customr_from':datetime.datetime(2021,1,15),\n",
    "     'last_update': datetime.datetime(2021,2,10,1,15,0)       \n",
    "    },\n",
    " {   \"id\":2,\n",
    "     \"name\": 'saeed',\n",
    "     \"email\" : 'saeed@addo.ai',\n",
    "     \"is_customer\": True,\n",
    "     \"amount\":900.32,\n",
    "     'customr_from':datetime.datetime(2021,2,25),\n",
    "     'last_update': datetime.datetime(2021,3,11,2,10,0)\n",
    "},\n",
    "  {   \"id\":3,\n",
    "     \"name\": 'ali',\n",
    "     \"email\" : 'ali@addo.ai',\n",
    "     \"is_customer\": False,\n",
    "     \"amount\":None,\n",
    "     'customr_from':None,\n",
    "     'last_update': datetime.datetime(2021,3,11,2,10,0)\n",
    "},\n",
    "    {   \"id\":4,\n",
    "     \"name\": 'khan',\n",
    "     \"email\" : 'khan@addo.ai',\n",
    "     \"is_customer\": False,\n",
    "     \"amount\":None,\n",
    "     'customr_from':None,\n",
    "     'last_update': datetime.datetime(2021,3,12,4,10,0)\n",
    "},\n",
    "        {   \"id\":5,\n",
    "     \"name\": 'He',\n",
    "     \"email\" : 'he@addo.ai',\n",
    "     \"is_customer\": False,\n",
    "     \"amount\":None,\n",
    "     'customr_from':None,\n",
    "     'last_update': datetime.datetime(2022,4,12,4,10,0)\n",
    "}   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros=[Row (**us) for us in user1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='asfand', email='asfand@addo.ai', is_customer=True, amount=1000.32, customr_from=datetime.datetime(2021, 1, 15, 0, 0), last_update=datetime.datetime(2021, 2, 10, 1, 15)),\n",
       " Row(id=2, name='saeed', email='saeed@addo.ai', is_customer=True, amount=900.32, customr_from=datetime.datetime(2021, 2, 25, 0, 0), last_update=datetime.datetime(2021, 3, 11, 2, 10)),\n",
       " Row(id=3, name='ali', email='ali@addo.ai', is_customer=False, amount=None, customr_from=None, last_update=datetime.datetime(2021, 3, 11, 2, 10)),\n",
       " Row(id=4, name='khan', email='khan@addo.ai', is_customer=False, amount=None, customr_from=None, last_update=datetime.datetime(2021, 3, 12, 4, 10)),\n",
       " Row(id=5, name='He', email='he@addo.ai', is_customer=False, amount=None, customr_from=None, last_update=datetime.datetime(2022, 4, 12, 4, 10))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------+---+-----------+-------------------+------+\n",
      "| amount|       customr_from|         email| id|is_customer|        last_update|  name|\n",
      "+-------+-------------------+--------------+---+-----------+-------------------+------+\n",
      "|1000.32|2021-01-15 00:00:00|asfand@addo.ai|  1|       true|2021-02-10 01:15:00|asfand|\n",
      "| 900.32|2021-02-25 00:00:00| saeed@addo.ai|  2|       true|2021-03-11 02:10:00| saeed|\n",
      "|   null|               null|   ali@addo.ai|  3|      false|2021-03-11 02:10:00|   ali|\n",
      "|   null|               null|  khan@addo.ai|  4|      false|2021-03-12 04:10:00|  khan|\n",
      "|   null|               null|    he@addo.ai|  5|      false|2022-04-12 04:10:00|    He|\n",
      "+-------+-------------------+--------------+---+-----------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(user1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr12=[Row(**us) for us in user1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='asfand', email='asfand@addo.ai', is_customer=True, amount=1000.32, customr_from=datetime.datetime(2021, 1, 15, 0, 0), last_update=datetime.datetime(2021, 2, 10, 1, 15)),\n",
       " Row(id=2, name='saeed', email='saeed@addo.ai', is_customer=True, amount=900.32, customr_from=datetime.datetime(2021, 2, 25, 0, 0), last_update=datetime.datetime(2021, 3, 11, 2, 10)),\n",
       " Row(id=3, name='ali', email='ali@addo.ai', is_customer=False, amount=None, customr_from=None, last_update=datetime.datetime(2021, 3, 11, 2, 10)),\n",
       " Row(id=4, name='khan', email='khan@addo.ai', is_customer=False, amount=None, customr_from=None, last_update=datetime.datetime(2021, 3, 12, 4, 10)),\n",
       " Row(id=5, name='He', email='he@addo.ai', is_customer=False, amount=None, customr_from=None, last_update=datetime.datetime(2022, 4, 12, 4, 10))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usr12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp = spark.createDataFrame(usr12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------+-----------+-------+-------------------+-------------------+\n",
      "| id|  name|         email|is_customer| amount|       customr_from|        last_update|\n",
      "+---+------+--------------+-----------+-------+-------------------+-------------------+\n",
      "|  1|asfand|asfand@addo.ai|       true|1000.32|2021-01-15 00:00:00|2021-02-10 01:15:00|\n",
      "|  2| saeed| saeed@addo.ai|       true| 900.32|2021-02-25 00:00:00|2021-03-11 02:10:00|\n",
      "|  3|   ali|   ali@addo.ai|      false|   null|               null|2021-03-11 02:10:00|\n",
      "|  4|  khan|  khan@addo.ai|      false|   null|               null|2021-03-12 04:10:00|\n",
      "|  5|    He|    he@addo.ai|      false|   null|               null|2022-04-12 04:10:00|\n",
      "+---+------+--------------+-----------+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'email', 'is_customer', 'amount', 'customr_from', 'last_update']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'),\n",
       " ('name', 'string'),\n",
       " ('email', 'string'),\n",
       " ('is_customer', 'boolean'),\n",
       " ('amount', 'double'),\n",
       " ('customr_from', 'timestamp'),\n",
       " ('last_update', 'timestamp')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssp.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Specify Schema As String**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that all the values on there position must have same datatype\n",
    "\n",
    "usr_string=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     1000.2,\n",
    "     datetime.datetime(2022,4,12,4,10,0)\n",
    "        ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     922.2,\n",
    "     datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     111.2,\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "     112.2,\n",
    "     datetime.datetime(2022,4,21,5,10,0))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schema as string\n",
    "usrs='''\n",
    "id int,\n",
    "name string,\n",
    "is_customer boolean,\n",
    "amount float,\n",
    "updated_instance timestamp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1=spark.createDataFrame(usr_string,schema=usrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+------+-------------------+\n",
      "| id|  name|is_customer|amount|   updated_instance|\n",
      "+---+------+-----------+------+-------------------+\n",
      "|  1|asfand|       true|1000.2|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true| 922.2|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false| 111.2|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false| 112.2|2022-04-21 05:10:00|\n",
      "+---+------+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Specify Schema for Spark Dataframe Using List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that all the values on there position must have same datatype\n",
    "\n",
    "usr_schema_list=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     1000.2,\n",
    "     datetime.datetime(2022,4,12,4,10,0)\n",
    "        ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     922.2,\n",
    "     datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     111.2,\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "     112.2,\n",
    "     datetime.datetime(2022,4,21,5,10,0))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schema via list of columns when such things happens we just have to pass list of column name and datatype will be infered\n",
    "user_schema=[\n",
    "    'id',\n",
    "    'name',\n",
    "    'is_customer',\n",
    "    'amount',\n",
    "    'update_record' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2=spark.createDataFrame(usr_schema_list,schema=user_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+------+-------------------+\n",
      "| id|  name|is_customer|amount|      update_record|\n",
      "+---+------+-----------+------+-------------------+\n",
      "|  1|asfand|       true|1000.2|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true| 922.2|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false| 111.2|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false| 112.2|2022-04-21 05:10:00|\n",
      "+---+------+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- update_record: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp2.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note::\\\n",
    "> The only difference between list of columns and string of columns with data type is that we cant specify any data type while we are using List of columns strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Specify Schema Using Spark Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that all the values on there position must have same datatype\n",
    "\n",
    "usr_schema_type=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     1000.2,\n",
    "     datetime.datetime(2022,4,12,4,10,0)\n",
    "        ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     922.2,\n",
    "     datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     111.2,\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "     112.2,\n",
    "     datetime.datetime(2022,4,21,5,10,0))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "she=StructType([\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('name', StringType()),\n",
    "    StructField('is_customer', BooleanType()),\n",
    "    StructField('amount', FloatType()),\n",
    "    StructField('update', TimestampType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp3=spark.createDataFrame(usr_schema_type,she)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+------+-------------------+\n",
      "| id|  name|is_customer|amount|             update|\n",
      "+---+------+-----------+------+-------------------+\n",
      "|  1|asfand|       true|1000.2|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true| 922.2|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false| 111.2|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false| 112.2|2022-04-21 05:10:00|\n",
      "+---+------+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2a8726c7b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Spark DataFrame Using Pandas Data Frame**\n",
    "* when there is any missing row in the dict,tuple and list then you will not be able to make spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_panda=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'amount': 1000.2,\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'amount':922.2,\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'amount':111.2,\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'amount':112.2}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert it into collection of ROW list and than convert to spark dataframe\n",
    "user_row=[Row(**us)  for us in usr_panda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='asfand', is_customer=True, amount=1000.2, update=datetime.datetime(2022, 4, 12, 4, 10)),\n",
       " Row(id=2, name='saeed', is_customer=True, amount=922.2, update=datetime.datetime(2021, 4, 11, 4, 10)),\n",
       " Row(id=3, name='ali', is_customer=False, amount=111.2, update=datetime.datetime(2021, 4, 22, 12, 10)),\n",
       " Row(id=4, name='khan', is_customer=False, amount=112.2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert to sparkdataframe you will find error as there is one row missing\n",
    "sp4=spark.createDataFrame(user_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to tickle this issue first of all go for pandas as the missing rows will be replaced by NaN\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>is_customer</th>\n",
       "      <th>amount</th>\n",
       "      <th>update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>asfand</td>\n",
       "      <td>True</td>\n",
       "      <td>1000.2</td>\n",
       "      <td>2022-04-12 04:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>saeed</td>\n",
       "      <td>True</td>\n",
       "      <td>922.2</td>\n",
       "      <td>2021-04-11 04:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ali</td>\n",
       "      <td>False</td>\n",
       "      <td>111.2</td>\n",
       "      <td>2021-04-22 12:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>khan</td>\n",
       "      <td>False</td>\n",
       "      <td>112.2</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    name  is_customer  amount              update\n",
       "0   1  asfand         True  1000.2 2022-04-12 04:10:00\n",
       "1   2   saeed         True   922.2 2021-04-11 04:10:00\n",
       "2   3     ali        False   111.2 2021-04-22 12:10:00\n",
       "3   4    khan        False   112.2                 NaT"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(usr_panda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp5=spark.createDataFrame(pd.DataFrame(usr_panda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+------+-------------------+\n",
      "| id|  name|is_customer|amount|             update|\n",
      "+---+------+-----------+------+-------------------+\n",
      "|  1|asfand|       true|1000.2|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true| 922.2|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false| 111.2|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false| 112.2|               null|\n",
      "+---+------+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd=spark.read.option('multiline','true').json('iris.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-------+\n",
      "|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "|        1.4|       0.2|        5.1|       3.5| setosa|\n",
      "|        1.4|       0.2|        4.9|       3.0| setosa|\n",
      "|        1.3|       0.2|        4.7|       3.2| setosa|\n",
      "|        1.5|       0.2|        4.6|       3.1| setosa|\n",
      "|        1.4|       0.2|        5.0|       3.6| setosa|\n",
      "|        1.7|       0.4|        5.4|       3.9| setosa|\n",
      "|        1.4|       0.3|        4.6|       3.4| setosa|\n",
      "|        1.5|       0.2|        5.0|       3.4| setosa|\n",
      "|        1.4|       0.2|        4.4|       2.9| setosa|\n",
      "|        1.5|       0.1|        4.9|       3.1| setosa|\n",
      "|        1.5|       0.2|        5.4|       3.7| setosa|\n",
      "|        1.6|       0.2|        4.8|       3.4| setosa|\n",
      "|        1.4|       0.1|        4.8|       3.0| setosa|\n",
      "|        1.1|       0.1|        4.3|       3.0| setosa|\n",
      "|        1.2|       0.2|        5.8|       4.0| setosa|\n",
      "|        1.5|       0.4|        5.7|       4.4| setosa|\n",
      "|        1.3|       0.4|        5.4|       3.9| setosa|\n",
      "|        1.4|       0.3|        5.1|       3.5| setosa|\n",
      "|        1.7|       0.3|        5.7|       3.8| setosa|\n",
      "|        1.5|       0.3|        5.1|       3.8| setosa|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Arry Type Columns in Spark DataFrame**\n",
    "* Here we use Explode the data and Explode_outer the data.\n",
    "* Explode have the requirement of removing the null value row from the output where as Explode_out will include the null values rows as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Arry=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': ['+92329677783','+9234245672'],\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': ['+92329672283','+9234236672'],\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': ['+92329634283','+9234242222'],\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': None,#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "use=[Row(**us) for us in usr_Arry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='asfand', is_customer=True, phone=['+92329677783', '+9234245672'], update=datetime.datetime(2022, 4, 12, 4, 10)),\n",
       " Row(id=2, name='saeed', is_customer=True, phone=['+92329672283', '+9234236672'], update=datetime.datetime(2021, 4, 11, 4, 10)),\n",
       " Row(id=3, name='ali', is_customer=False, phone=['+92329634283', '+9234242222'], update=datetime.datetime(2021, 4, 22, 12, 10)),\n",
       " Row(id=4, name='khan', is_customer=False, phone=None, update=None)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp1=spark.createDataFrame(use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+---------------------------+-------------------+\n",
      "|id |name  |is_customer|phone                      |update             |\n",
      "+---+------+-----------+---------------------------+-------------------+\n",
      "|1  |asfand|true       |[+92329677783, +9234245672]|2022-04-12 04:10:00|\n",
      "|2  |saeed |true       |[+92329672283, +9234236672]|2021-04-11 04:10:00|\n",
      "|3  |ali   |false      |[+92329634283, +9234242222]|2021-04-22 12:10:00|\n",
      "|4  |khan  |false      |null                       |null               |\n",
      "+---+------+-----------+---------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|               phone|\n",
      "+---+--------------------+\n",
      "|  1|[+92329677783, +9...|\n",
      "|  2|[+92329672283, +9...|\n",
      "|  3|[+92329634283, +9...|\n",
      "|  4|                null|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp1.select('id','phone').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'is_customer', 'phone', 'update']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data first import the col and then separate the first and second number from eachother\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+-------------+\n",
      "| id|               phone| firs_number|second_number|\n",
      "+---+--------------------+------------+-------------+\n",
      "|  1|[+92329677783, +9...|+92329677783|  +9234245672|\n",
      "|  2|[+92329672283, +9...|+92329672283|  +9234236672|\n",
      "|  3|[+92329634283, +9...|+92329634283|  +9234242222|\n",
      "|  4|                null|        null|         null|\n",
      "+---+--------------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp1.select('id','phone',col('phone')[0].alias('firs_number'),col('phone')[1].alias('second_number')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+\n",
      "| id|first_number| 2nd_number|\n",
      "+---+------------+-----------+\n",
      "|  1|+92329677783|+9234245672|\n",
      "|  2|+92329672283|+9234236672|\n",
      "|  3|+92329634283|+9234242222|\n",
      "|  4|        null|       null|\n",
      "+---+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp1.select('id',col('phone')[0].alias('first_number'),col('phone')[1].alias('2nd_number')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-------------------+------------+\n",
      "| id|  name|is_customer|             update|   new_phone|\n",
      "+---+------+-----------+-------------------+------------+\n",
      "|  1|asfand|       true|2022-04-12 04:10:00|+92329677783|\n",
      "|  1|asfand|       true|2022-04-12 04:10:00| +9234245672|\n",
      "|  2| saeed|       true|2021-04-11 04:10:00|+92329672283|\n",
      "|  2| saeed|       true|2021-04-11 04:10:00| +9234236672|\n",
      "|  3|   ali|      false|2021-04-22 12:10:00|+92329634283|\n",
      "|  3|   ali|      false|2021-04-22 12:10:00| +9234242222|\n",
      "+---+------+-----------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now apply explode function\n",
    "spp1.withColumn('new_phone',explode('phone')).drop('phone').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-------------------+------------+\n",
      "| id|  name|is_customer|             update|   new_phone|\n",
      "+---+------+-----------+-------------------+------------+\n",
      "|  1|asfand|       true|2022-04-12 04:10:00|+92329677783|\n",
      "|  1|asfand|       true|2022-04-12 04:10:00| +9234245672|\n",
      "|  2| saeed|       true|2021-04-11 04:10:00|+92329672283|\n",
      "|  2| saeed|       true|2021-04-11 04:10:00| +9234236672|\n",
      "|  3|   ali|      false|2021-04-22 12:10:00|+92329634283|\n",
      "|  3|   ali|      false|2021-04-22 12:10:00| +9234242222|\n",
      "|  4|  khan|      false|               null|        null|\n",
      "+---+------+-----------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp1.withColumn('new_phone',explode_outer('phone')).drop('phone').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **List of Tuple to Pandas and then to Spark DataFrame**\n",
    "* First We will add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Map=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     {\"+92329677783\",\"+92342454672\"},\n",
    "      datetime.datetime(2022,4,12,4,10,0)\n",
    "    ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     {\"+92329622283\", \"+92342442312\"},\n",
    "      datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     {\"+92329644483\", \"+92342445552\"},\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "      None,#['+92329633456','+9234247654'],\n",
    "     None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType,BooleanType,TimestampType,ArrayType\n",
    "\n",
    "scha=StructType([StructField('id', IntegerType()),\n",
    "    StructField('name', StringType()),\n",
    "    StructField('is_customer', BooleanType()),\n",
    "    StructField('phone', StringType()),\n",
    "    StructField('update', TimestampType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>is_customer</th>\n",
       "      <th>phone</th>\n",
       "      <th>update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>asfand</td>\n",
       "      <td>True</td>\n",
       "      <td>{+92329677783, +92342454672}</td>\n",
       "      <td>2022-04-12 04:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>saeed</td>\n",
       "      <td>True</td>\n",
       "      <td>{+92342442312, +92329622283}</td>\n",
       "      <td>2021-04-11 04:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ali</td>\n",
       "      <td>False</td>\n",
       "      <td>{+92329644483, +92342445552}</td>\n",
       "      <td>2021-04-22 12:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>khan</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    name  is_customer                         phone              update\n",
       "0   1  asfand         True  {+92329677783, +92342454672} 2022-04-12 04:10:00\n",
       "1   2   saeed         True  {+92342442312, +92329622283} 2021-04-11 04:10:00\n",
       "2   3     ali        False  {+92329644483, +92342445552} 2021-04-22 12:10:00\n",
       "3   4    khan        False                          None                 NaT"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### now creating pandas Dataframe\n",
    "df=pd.DataFrame(usr_Map,columns=['id','name','is_customer','phone','update'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|[+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|[+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|[+92342445552, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|                null|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp=spark.createDataFrame(df,scha)\n",
    "sp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Map Type Columns in Spark DataFrame**\n",
    "* When we have Key value nest it is called Map\n",
    "\n",
    "> Note ::\\\n",
    ">  we can use explode function just with map and array type column data not struct ie. row('name'='asfand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Map=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': {\"first\":\"+92329677783\", \"Second\":\"+92342454672\"},\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': {\"first\":\"+92329622283\", \"Second\":\"+92342442312\"},\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': {\"first\":\"+92329644483\", \"Second\":\"+92342445552\"},\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': None,#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import Row\n",
    "# Now convert to Row list\n",
    "uses=[Row(**us) for us in usr_Map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_Maps=spark.createDataFrame(uses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-----------------------------------------------+-------------------+\n",
      "|id |name  |is_customer|phone                                          |update             |\n",
      "+---+------+-----------+-----------------------------------------------+-------------------+\n",
      "|1  |asfand|true       |{Second -> +92342454672, first -> +92329677783}|2022-04-12 04:10:00|\n",
      "|2  |saeed |true       |{Second -> +92342442312, first -> +92329622283}|2021-04-11 04:10:00|\n",
      "|3  |ali   |false      |{Second -> +92342445552, first -> +92329644483}|2021-04-22 12:10:00|\n",
      "|4  |khan  |false      |null                                           |null               |\n",
      "+---+------+-----------+-----------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+-------------+\n",
      "| id|  name|is_customer|               phone|             update|phone[Second]|\n",
      "+---+------+-----------+--------------------+-------------------+-------------+\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00| +92342454672|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00| +92342442312|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00| +92342445552|\n",
      "|  4|  khan|      false|                null|               null|         null|\n",
      "+---+------+-----------+--------------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select (\"*\",col('phone')['Second']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "| id|  name|is_customer|               phone|             update|      Second|\n",
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00|+92342454672|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00|+92342442312|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00|+92342445552|\n",
      "|  4|  khan|      false|                null|               null|        null|\n",
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select(\"*\",col('phone.Second')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+-------------+\n",
      "| id|  name|is_customer|               phone|             update|phone[Second]|\n",
      "+---+------+-----------+--------------------+-------------------+-------------+\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00| +92342454672|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00| +92342442312|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00| +92342445552|\n",
      "|  4|  khan|      false|                null|               null|         null|\n",
      "+---+------+-----------+--------------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select(\"*\",use_Maps['phone']['Second']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-----------------------------------------------+-------------------+\n",
      "|id |name  |is_customer|phone                                          |update             |\n",
      "+---+------+-----------+-----------------------------------------------+-------------------+\n",
      "|1  |asfand|true       |{Second -> +92342454672, first -> +92329677783}|2022-04-12 04:10:00|\n",
      "|2  |saeed |true       |{Second -> +92342442312, first -> +92329622283}|2021-04-11 04:10:00|\n",
      "|3  |ali   |false      |{Second -> +92342445552, first -> +92329644483}|2021-04-22 12:10:00|\n",
      "|4  |khan  |false      |null                                           |null               |\n",
      "+---+------+-----------+-----------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select (\"*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------------+\n",
      "| id|  name| types|      phones|\n",
      "+---+------+------+------------+\n",
      "|  1|asfand|Second|+92342454672|\n",
      "|  1|asfand| first|+92329677783|\n",
      "|  2| saeed|Second|+92342442312|\n",
      "|  2| saeed| first|+92329622283|\n",
      "|  3|   ali|Second|+92342445552|\n",
      "|  3|   ali| first|+92329644483|\n",
      "+---+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select('id','name',explode(col('phone'))).withColumnRenamed('key','types').withColumnRenamed('value','phones').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'phone[Second]'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_Maps['phone']['Second']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+------------+\n",
      "| id|  name|phone[Second]|       first|\n",
      "+---+------+-------------+------------+\n",
      "|  1|asfand| +92342454672|+92329677783|\n",
      "|  2| saeed| +92342442312|+92329622283|\n",
      "|  3|   ali| +92342445552|+92329644483|\n",
      "|  4|  khan|         null|        null|\n",
      "+---+------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select('id','name',use_Maps['phone']['Second'],col('phone.first')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-------------------+------------+------------+\n",
      "| id|  name|is_customer|             update|     first_n|      Second|\n",
      "+---+------+-----------+-------------------+------------+------------+\n",
      "|  1|asfand|       true|2022-04-12 04:10:00|+92329677783|+92342454672|\n",
      "|  2| saeed|       true|2021-04-11 04:10:00|+92329622283|+92342442312|\n",
      "|  3|   ali|      false|2021-04-22 12:10:00|+92329644483|+92342445552|\n",
      "|  4|  khan|      false|               null|        null|        null|\n",
      "+---+------+-----------+-------------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select (\"*\").withColumn('first_n',col('phone.first')).withColumn('Second',col('phone.Second')).drop('phone').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "| id|  name|is_customer|               phone|             update|     first_n|\n",
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00|+92329677783|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00|+92329622283|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00|+92329644483|\n",
      "|  4|  khan|      false|                null|               null|        null|\n",
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select(\"*\").withColumn('first_n', col('phone')['first']).show()\n",
    "# .withColumn('Second',col(['phone'])['Second']).drop('phone').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+----------+------------+\n",
      "| id|  name|is_customer|               phone|             update|Phone_Type|      Number|\n",
      "+---+------+-----------+--------------------+-------------------+----------+------------+\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00|    Second|+92342454672|\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00|     first|+92329677783|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00|    Second|+92342442312|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00|     first|+92329622283|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00|    Second|+92342445552|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00|     first|+92329644483|\n",
      "+---+------+-----------+--------------------+-------------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using Explode function we get the column of key and values\n",
    "use_Maps.select(\"*\",explode(\"phone\")).\\\n",
    "    withColumnRenamed(\"key\",\"Phone_Type\").\\\n",
    "    withColumnRenamed(\"value\",\"Number\").\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+----------+------------+\n",
      "| id|  name|is_customer|               phone|             update|Phone_Type|      Number|\n",
      "+---+------+-----------+--------------------+-------------------+----------+------------+\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00|    Second|+92342454672|\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00|     first|+92329677783|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00|    Second|+92342442312|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00|     first|+92329622283|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00|    Second|+92342445552|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00|     first|+92329644483|\n",
      "+---+------+-----------+--------------------+-------------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select(\"*\",explode(\"phone\")).\\\n",
    "    withColumnRenamed(\"key\",\"Phone_Type\").\\\n",
    "    withColumnRenamed(\"value\",\"Number\").\\\n",
    "    show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Spark Data Frame to Select and Rename Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=None), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_struct= [Row(**us) for us in usr_Struct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp2=spark.createDataFrame(user_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: struct (nullable = true)\n",
      " |    |-- first: string (nullable = true)\n",
      " |    |-- Second: string (nullable = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+\n",
      "| id|       first|          bb|\n",
      "+---+------------+------------+\n",
      "|  1|+92329677783|+92329677783|\n",
      "|  2|+92329622283|+92329622283|\n",
      "|  3|+92329644483|+92329644483|\n",
      "|  4|        null|        null|\n",
      "+---+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.select('id',col('phone.first'),col('phone')['first'].alias('bb')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+\n",
      "| id|       first|      Second|\n",
      "+---+------------+------------+\n",
      "|  1|+92329677783|+92342454672|\n",
      "|  2|+92329622283|+92342442312|\n",
      "|  3|+92329644483|+92342445552|\n",
      "|  4|        null|        null|\n",
      "+---+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.select('id','phone.first','phone.Second').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+----------------------------+-------------------+\n",
      "|id |name  |is_customer|phone                       |update             |\n",
      "+---+------+-----------+----------------------------+-------------------+\n",
      "|1  |asfand|true       |{+92329677783, +92342454672}|2022-04-12 04:10:00|\n",
      "|2  |saeed |true       |{+92329622283, +92342442312}|2021-04-11 04:10:00|\n",
      "|3  |ali   |false      |{+92329644483, +92342445552}|2021-04-22 12:10:00|\n",
      "|4  |khan  |false      |{null, null}                |null               |\n",
      "+---+------+-----------+----------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.select('*').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+\n",
      "| id|       first|      Second|\n",
      "+---+------------+------------+\n",
      "|  1|+92329677783|+92342454672|\n",
      "|  2|+92329622283|+92342442312|\n",
      "|  3|+92329644483|+92342445552|\n",
      "|  4|        null|        null|\n",
      "+---+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.select ('id',col('phone.*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+\n",
      "| id|first_contact_number|      Second|\n",
      "+---+--------------------+------------+\n",
      "|  1|        +92329677783|+92342454672|\n",
      "|  2|        +92329622283|+92342442312|\n",
      "|  3|        +92329644483|+92342445552|\n",
      "|  4|                null|        null|\n",
      "+---+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.select ('id',col('phone.*')).withColumnRenamed('first','first_contact_number').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Renaming Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2399eb19e40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  id|\n",
      "+--------------------+\n",
      "|5.000,1.0,0,,IDD_...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usr_Struct=[\n",
    "    {'id':'5.000,1.0,0,,IDD_SMS_KSA_IC,16384,null,0,586,NewBasic,NewBasic,null;1.00,1.0,0,,IDD_SMS_KSA_IC,128,null,1,20005252,null,null,null;-5.000,1.0,0,,IDD_SMS_KSA_IC,128,null,1,586,null,null,null'}]\n",
    "import pandas as pd\n",
    "df=spark.createDataFrame(pd.DataFrame(usr_Struct))\n",
    "df.show()\n",
    "df.createOrReplaceTempView('users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id                                                                                                                                                                                          |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|5.000,1.0,0,,IDD_SMS_KSA_IC,16384,null,0,586,NewBasic,NewBasic,null;1.00,1.0,0,,IDD_SMS_KSA_IC,128,null,1,20005252,null,null,null;-5.000,1.0,0,,IDD_SMS_KSA_IC,128,null,1,586,null,null,null|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from users \"\"\").show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|num_semicolons|\n",
      "+--------------+\n",
      "|             2|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_semicolons = spark.sql(\"SELECT (LENGTH(id) - LENGTH(REPLACE(id, ';', ''))) AS num_semicolons FROM users\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "num_semicolons.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>is_customer</th>\n",
       "      <th>phone</th>\n",
       "      <th>update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>asfand</td>\n",
       "      <td>True</td>\n",
       "      <td>(+92329677783, +92342454672)</td>\n",
       "      <td>2022-04-12 04:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>saeed</td>\n",
       "      <td>True</td>\n",
       "      <td>(+92329622283, +92342442312)</td>\n",
       "      <td>2021-04-11 04:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ali</td>\n",
       "      <td>False</td>\n",
       "      <td>(+92329644483, +92342445552)</td>\n",
       "      <td>2021-04-22 12:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>khan</td>\n",
       "      <td>False</td>\n",
       "      <td>(None, None)</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    name  is_customer                         phone              update\n",
       "0   1  asfand         True  (+92329677783, +92342454672) 2022-04-12 04:10:00\n",
       "1   2   saeed         True  (+92329622283, +92342442312) 2021-04-11 04:10:00\n",
       "2   3     ali        False  (+92329644483, +92342445552) 2021-04-22 12:10:00\n",
       "3   4    khan        False                  (None, None)                 NaT"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(usr_Struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=spark.createDataFrame(pd.DataFrame(usr_Struct))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('u').select ('u.*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias ('s').select (['s.id','s.name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------------------+\n",
      "|id |name  |phone                       |\n",
      "+---+------+----------------------------+\n",
      "|1  |asfand|{+92329677783, +92342454672}|\n",
      "|2  |saeed |{+92329622283, +92342442312}|\n",
      "|3  |ali   |{+92329644483, +92342445552}|\n",
      "|4  |khan  |{null, null}                |\n",
      "+---+------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('u').select(['u.id','u.name','u.phone']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+\n",
      "| id|  name|               phone|\n",
      "+---+------+--------------------+\n",
      "|  1|asfand|{+92329677783, +9...|\n",
      "|  2| saeed|{+92329622283, +9...|\n",
      "|  3|   ali|{+92329644483, +9...|\n",
      "|  4|  khan|        {null, null}|\n",
      "+---+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import concat,lit\n",
    "df.select(col('id'),col('name'),col('phone')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+\n",
      "| id|  name|user_name|\n",
      "+---+------+---------+\n",
      "|  1|asfand| asfand_1|\n",
      "|  2| saeed|  saeed_2|\n",
      "|  3|   ali|    ali_3|\n",
      "|  4|  khan|   khan_4|\n",
      "+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (col('id'),col('name'),concat(df['name'],lit('_'),col('id')).alias('user_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id| name_id|\n",
      "+---+--------+\n",
      "|  1|asfand_1|\n",
      "|  2| saeed_2|\n",
      "|  3|   ali_3|\n",
      "|  4|  khan_4|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('id'),concat(col('name'),lit(\"_\"),col('id')).alias('name_id')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note::\\\n",
    "> We will use col() function allot when it comes to applying functions on top of columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SelectExpr**\n",
    "* It takes strings not col type object like df['id'] and col('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method selectExpr in module pyspark.sql.dataframe:\n",
      "\n",
      "selectExpr(*expr) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      "    \n",
      "    This is a variant of :func:`select` that accepts SQL expressions.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      "    [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.selectExpr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can only use spark SQL function while using selectEXPR not pyspark.sql.functions. When we are using this we don't need to import the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1dab5864b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=spark.createDataFrame(pd.DataFrame(usr_Struct))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select ('id','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (col('id'),col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['id'],df['name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (['id','name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+\n",
      "| id|length(name)| name_id|\n",
      "+---+------------+--------+\n",
      "|  1|           6|asfand_1|\n",
      "|  2|           5| saeed_2|\n",
      "|  3|           3|   ali_3|\n",
      "|  4|           4|  khan_4|\n",
      "+---+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id',length(col('name')),concat(col('name'),lit('_'),col('id')).alias('name_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+\n",
      "|(id * 2)|length(name)| name_id|\n",
      "+--------+------------+--------+\n",
      "|       2|           6|asfand_1|\n",
      "|       4|           5| saeed_2|\n",
      "|       6|           3|   ali_3|\n",
      "|       8|           4|  khan_4|\n",
      "+--------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('id*2',\"length(name)\",\"concat(name,'_',id) as name_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|(id * 2)|  name|\n",
      "+--------+------+\n",
      "|       2|asfand|\n",
      "|       4| saeed|\n",
      "|       6|   ali|\n",
      "|       8|  khan|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('id*2','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|     ids|\n",
      "+--------+\n",
      "|1_asfand|\n",
      "| 2_saeed|\n",
      "|   3_ali|\n",
      "|  4_khan|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"concat(id,'_',name) as ids\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+--------+\n",
      "| id|  name|               phone|     ids|\n",
      "+---+------+--------------------+--------+\n",
      "|  1|asfand|{+92329677783, +9...|asfand_1|\n",
      "|  2| saeed|{+92329622283, +9...| saeed_2|\n",
      "|  3|   ali|{+92329644483, +9...|   ali_3|\n",
      "|  4|  khan|        {null, null}|  khan_4|\n",
      "+---+------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('id','name','phone',\"concat(name,'_',id) as ids\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+\n",
      "| id|  name|     ids|\n",
      "+---+------+--------+\n",
      "|  1|asfand|asfand_1|\n",
      "|  2| saeed| saeed_2|\n",
      "|  3|   ali|   ali_3|\n",
      "|  4|  khan|  khan_4|\n",
      "+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('users')\n",
    "spark.sql('''select id,name,concat(name,'_',id) as ids from users''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|     ids|\n",
      "+---+--------+\n",
      "|  1|asfand_1|\n",
      "|  2| saeed_2|\n",
      "|  3|   ali_3|\n",
      "|  4|  khan_4|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_name=concat(col('name'),lit('_'),col('id')).alias('ids')\n",
    "df.select('id',full_name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding col function in Spark**\n",
    "* Select works on column type object that is col('column name') and df['column name'] and string as column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27289184b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=spark.createDataFrame(pd.DataFrame(usr_Struct))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'name'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'name'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'), ('order_date', 'int')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "dates=date_format('update','yyyyMMdd').cast('int').alias('order_date')\n",
    "df.select('id',dates).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'is_customer', 'phone', 'update']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "colum=['id', 'name', 'is_customer', 'phone', 'update']\n",
    "dff=df.select (*colum).withColumn('date_froms',to_date(col('update')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: struct (nullable = true)\n",
      " |    |-- first: string (nullable = true)\n",
      " |    |-- Second: string (nullable = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      " |-- date_froms: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data_format will get date,timestamp and string as input and as output it will how string what so ever formate you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+----------+-------------+\n",
      "| id|  name|is_customer|               phone|             update|date_froms|date_from_int|\n",
      "+---+------+-----------+--------------------+-------------------+----------+-------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|2022-04-12|     20220412|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|2021-04-11|     20210411|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|2021-04-22|     20210422|\n",
      "|  4|  khan|      false|        {null, null}|               null|      null|         null|\n",
      "+---+------+-----------+--------------------+-------------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.select (\"*\").withColumn('date_from_int',date_format(col('date_froms'),'yyyyMMdd')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: struct (nullable = true)\n",
      " |    |-- first: string (nullable = true)\n",
      " |    |-- Second: string (nullable = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      " |-- date_froms: date (nullable = true)\n",
      " |-- date_from_int: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.select (\"*\").withColumn('date_from_int',date_format(col('date_froms'),'yyyyMMdd')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: struct (nullable = true)\n",
      " |    |-- first: string (nullable = true)\n",
      " |    |-- Second: string (nullable = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      " |-- date_froms: date (nullable = true)\n",
      " |-- date_from_int: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.select (\"*\").withColumn('date_from_int',date_format(col('date_froms'),'yyyyMMdd').cast('integer')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   from_dates\n",
      "0  2023-09-09\n",
      "1  2023-09-01\n",
      "2  2023-09-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asfandyar\\AppData\\Local\\Temp\\ipykernel_27504\\274203270.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_test=df_test.append({'from_dates':'2023-09-03'},ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df_test=pd.DataFrame(columns=['from_dates'])\n",
    "df_test['from_dates']=['2023-09-09','2023-09-01']\n",
    "df_test=df_test.append({'from_dates':'2023-09-03'},ignore_index=True)\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from_dates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_spark=spark.createDataFrame(df_test)\n",
    "df_test_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|from_dates|     dates|\n",
      "+----------+----------+\n",
      "|2023-09-09|2023/09/09|\n",
      "|2023-09-01|2023/09/01|\n",
      "|2023-09-03|2023/09/03|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_spark.select ('*').withColumn('dates',date_format(col('from_dates'),'yyyy/MM/dd')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'date_format(2023-09-09, yyyy/MM/dd)'>\n"
     ]
    }
   ],
   "source": [
    "print(date_format('2023-09-09','yyyy/MM/dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_format in module pyspark.sql.functions:\n",
      "\n",
      "date_format(date, format)\n",
      "    Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "    format given by the second argument.\n",
      "    \n",
      "    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "    pattern letters of `datetime pattern`_. can be used.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Whenever possible, use specialized functions like `year`.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "    [Row(date='04/08/2015')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- order_date: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id',dates).printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Understanding lit() in Spark**\n",
    "* If we want to add ,subtract or do any sort of operation in spark we can do that by lit as it returns column type object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d0e0a127d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql  import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "     \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=spark.createDataFrame(pd.DataFrame(usr_Struct))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('id','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|sum_20|\n",
      "+---+------+------+\n",
      "|  1|asfand|    21|\n",
      "|  2| saeed|    22|\n",
      "|  3|   ali|    23|\n",
      "|  4|  khan|    24|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (col('id'),col('name'),(col('id')+lit(20)).alias ('sum_20')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Renaming Spark DataFrame Columns or Expression**\n",
    "* If we want to add new column we will use withColumn function.\n",
    "* WithColumn function can help in renaming of the existing columns as well but it will add up new column which we need to drop in future where as withColumnRenamed will just rename the selected column.\n",
    "* withColumnRenamed function will help in renaming the columns.\n",
    "* Alias is also used to rename the column.\n",
    "* to_DF is use to rename bunch of columns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **WithColumn and WithColumnRenamed**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task is to use withColumn and withColumnRenamed to show difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method withColumn in module pyspark.sql.dataframe:\n",
      "\n",
      "withColumn(colName, col) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      "    existing column that has the same name.\n",
      "    \n",
      "    The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      "    a column from some other :class:`DataFrame` will raise an error.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    colName : str\n",
      "        string, name of the new column.\n",
      "    col : :class:`Column`\n",
      "        a :class:`Column` expression for the new column.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This method introduces a projection internally. Therefore, calling it multiple\n",
      "    times, for instance, via loops in order to add multiple columns can generate big\n",
      "    plans which can cause performance issues and even `StackOverflowException`.\n",
      "    To avoid this, use :func:`select` with the multiple columns at once.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.withColumn('age2', df.age + 2).collect()\n",
      "    [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.withColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0),\n",
    "     'course':[1,2]\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0),\n",
    "     'course':[1,3,2]},\n",
    "    \n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0),\n",
    "     'course':[1,2,4,5]\n",
    "     \n",
    "     },\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None, #datetime.datetime(2022,5,1,10,10,0)}\n",
    "     'course':[]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>is_customer</th>\n",
       "      <th>phone</th>\n",
       "      <th>update</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>asfand</td>\n",
       "      <td>True</td>\n",
       "      <td>(+92329677783, +92342454672)</td>\n",
       "      <td>2022-04-12 04:10:00</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>saeed</td>\n",
       "      <td>True</td>\n",
       "      <td>(+92329622283, +92342442312)</td>\n",
       "      <td>2021-04-11 04:10:00</td>\n",
       "      <td>[1, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ali</td>\n",
       "      <td>False</td>\n",
       "      <td>(+92329644483, +92342445552)</td>\n",
       "      <td>2021-04-22 12:10:00</td>\n",
       "      <td>[1, 2, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>khan</td>\n",
       "      <td>False</td>\n",
       "      <td>(None, None)</td>\n",
       "      <td>NaT</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    name  is_customer                         phone              update  \\\n",
       "0   1  asfand         True  (+92329677783, +92342454672) 2022-04-12 04:10:00   \n",
       "1   2   saeed         True  (+92329622283, +92342442312) 2021-04-11 04:10:00   \n",
       "2   3     ali        False  (+92329644483, +92342445552) 2021-04-22 12:10:00   \n",
       "3   4    khan        False                  (None, None)                 NaT   \n",
       "\n",
       "         course  \n",
       "0        [1, 2]  \n",
       "1     [1, 3, 2]  \n",
       "2  [1, 2, 4, 5]  \n",
       "3            []  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa=pd.DataFrame(usr_Struct)\n",
    "pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n",
      "| id|  name|courses|\n",
      "+---+------+-------+\n",
      "|  1|asfand|      2|\n",
      "|  2| saeed|      3|\n",
      "|  3|   ali|      4|\n",
      "|  4|  khan|      0|\n",
      "+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select ('id','name','course').withColumn('courses',size(col('course'))).drop(col('course')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+\n",
      "| id|  name| dep_courses|\n",
      "+---+------+------------+\n",
      "|  1|asfand|      [1, 2]|\n",
      "|  2| saeed|   [1, 3, 2]|\n",
      "|  3|   ali|[1, 2, 4, 5]|\n",
      "|  4|  khan|          []|\n",
      "+---+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Lets rename the columns\n",
    "df.select('id','name','course').withColumnRenamed('course','dep_courses').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "df.select('id','name','phone').withColumn('cou',size('phone')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+\n",
      "|student_id|student_name|students|\n",
      "+----------+------------+--------+\n",
      "|         1|      asfand|asfand_1|\n",
      "|         2|       saeed| saeed_2|\n",
      "|         3|         ali|   ali_3|\n",
      "|         4|        khan|  khan_4|\n",
      "+----------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (col('id').alias('student_id'),col('name').alias('student_name'),concat(col('name'),lit('_'),col('id')).alias('students')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+\n",
      "|student_id|student_name|students|\n",
      "+----------+------------+--------+\n",
      "|         1|      asfand|asfand_1|\n",
      "|         2|       saeed| saeed_2|\n",
      "|         3|         ali|   ali_3|\n",
      "|         4|        khan|  khan_4|\n",
      "+----------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (df['id'].alias('student_id'),df['name'].alias('student_name'),concat(df['name'],lit('_'),df['id']).alias('students')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **using toDF**\n",
    "* toDF is use to convert list of tuple to spark dataframe and rdd to spark dataframe.\n",
    "* Here we will use toDF for bulk renaming of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[('Alice', 1), ('Bob', 2), ('Carol', 3)]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2), (\"Carol\", 3)])\n",
    "print(type(rdd.collect()))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('name', 'string'), ('id', 'bigint')]\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(rdd.toDF(['name','id']).dtypes)\n",
    "print (type(rdd.toDF(['name','id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "     \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=spark.createDataFrame(pd.DataFrame(usr_Struct))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "original=['id','name','is_customer','phone','update']\n",
    "new=['user_id','user_name','user_is_customer','user_phone','user_update']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------------+----------------------------+-------------------+\n",
      "|user_id|user_name|user_is_customer|user_phone                  |user_update        |\n",
      "+-------+---------+----------------+----------------------------+-------------------+\n",
      "|1      |asfand   |true            |{+92329677783, +92342454672}|2022-04-12 04:10:00|\n",
      "|2      |saeed    |true            |{+92329622283, +92342442312}|2021-04-11 04:10:00|\n",
      "|3      |ali      |false           |{+92329644483, +92342445552}|2021-04-22 12:10:00|\n",
      "|4      |khan     |false           |{null, null}                |null               |\n",
      "+-------+---------+----------------+----------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(original).toDF('user_id','user_name','user_is_customer','user_phone','user_update').show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------------+----------------------------+-------------------+\n",
      "|user_id|user_name|user_is_customer|user_phone                  |user_update        |\n",
      "+-------+---------+----------------+----------------------------+-------------------+\n",
      "|1      |asfand   |true            |{+92329677783, +92342454672}|2022-04-12 04:10:00|\n",
      "|2      |saeed    |true            |{+92329622283, +92342442312}|2021-04-11 04:10:00|\n",
      "|3      |ali      |false           |{+92329644483, +92342445552}|2021-04-22 12:10:00|\n",
      "|4      |khan     |false           |{null, null}                |null               |\n",
      "+-------+---------+----------------+----------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(original).toDF(*new).show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sour=['id','name','is_customer','phone','update']\n",
    "target=['s_id','s_name','s_is_customer','s_phone','s_update']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------------+--------------------+-------------------+\n",
      "|s_id|s_name|s_is_customer|             s_phone|           s_update|\n",
      "+----+------+-------------+--------------------+-------------------+\n",
      "|   1|asfand|         true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|   2| saeed|         true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|   3|   ali|        false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|   4|  khan|        false|        {null, null}|               null|\n",
      "+----+------+-------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (sour).toDF(*target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('toDF' in dir(pd.DataFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', '_AXIS_LEN', '_AXIS_NAMES', '_AXIS_NUMBERS', '_AXIS_ORDERS', '_AXIS_TO_AXIS_NUMBER', '_HANDLED_TYPES', '__abs__', '__add__', '__and__', '__annotations__', '__array__', '__array_priority__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__divmod__', '__doc__', '__eq__', '__finalize__', '__floordiv__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__imod__', '__imul__', '__init__', '__init_subclass__', '__invert__', '__ior__', '__ipow__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '__xor__', '_accessors', '_accum_func', '_add_numeric_operations', '_agg_by_level', '_agg_examples_doc', '_agg_summary_and_see_also_doc', '_align_frame', '_align_series', '_append', '_arith_method', '_as_manager', '_box_col_values', '_can_fast_transpose', '_check_inplace_and_allows_duplicate_labels', '_check_inplace_setting', '_check_is_chained_assignment_possible', '_check_label_or_level_ambiguity', '_check_setitem_copy', '_clear_item_cache', '_clip_with_one_bound', '_clip_with_scalar', '_cmp_method', '_combine_frame', '_consolidate', '_consolidate_inplace', '_construct_axes_dict', '_construct_axes_from_arguments', '_construct_result', '_constructor', '_constructor_sliced', '_convert', '_count_level', '_data', '_dir_additions', '_dir_deletions', '_dispatch_frame_op', '_drop_axis', '_drop_labels_or_levels', '_ensure_valid_index', '_find_valid_index', '_from_arrays', '_from_mgr', '_get_agg_axis', '_get_axis', '_get_axis_name', '_get_axis_number', '_get_axis_resolvers', '_get_block_manager_axis', '_get_bool_data', '_get_cleaned_column_resolvers', '_get_column_array', '_get_index_resolvers', '_get_item_cache', '_get_label_or_level_values', '_get_numeric_data', '_get_value', '_getitem_bool_array', '_getitem_multilevel', '_gotitem', '_hidden_attrs', '_indexed_same', '_info_axis', '_info_axis_name', '_info_axis_number', '_info_repr', '_init_mgr', '_inplace_method', '_internal_names', '_internal_names_set', '_is_copy', '_is_homogeneous_type', '_is_label_or_level_reference', '_is_label_reference', '_is_level_reference', '_is_mixed_type', '_is_view', '_iset_item', '_iset_item_mgr', '_iset_not_inplace', '_iter_column_arrays', '_ixs', '_join_compat', '_logical_func', '_logical_method', '_maybe_cache_changed', '_maybe_update_cacher', '_metadata', '_min_count_stat_function', '_needs_reindex_multi', '_protect_consolidate', '_reduce', '_reduce_axis1', '_reindex_axes', '_reindex_columns', '_reindex_index', '_reindex_multi', '_reindex_with_indexers', '_rename', '_replace_columnwise', '_repr_data_resource_', '_repr_fits_horizontal_', '_repr_fits_vertical_', '_repr_html_', '_repr_latex_', '_reset_cache', '_reset_cacher', '_sanitize_column', '_series', '_set_axis', '_set_axis_name', '_set_axis_nocheck', '_set_is_copy', '_set_item', '_set_item_frame_value', '_set_item_mgr', '_set_value', '_setitem_array', '_setitem_frame', '_setitem_slice', '_slice', '_stat_axis', '_stat_axis_name', '_stat_axis_number', '_stat_function', '_stat_function_ddof', '_take_with_is_copy', '_to_dict_of_blocks', '_typ', '_update_inplace', '_validate_dtype', '_values', '_where', 'abs', 'add', 'add_prefix', 'add_suffix', 'agg', 'aggregate', 'align', 'all', 'any', 'append', 'apply', 'applymap', 'asfreq', 'asof', 'assign', 'astype', 'at', 'at_time', 'attrs', 'axes', 'backfill', 'between_time', 'bfill', 'bool', 'boxplot', 'clip', 'columns', 'combine', 'combine_first', 'compare', 'convert_dtypes', 'copy', 'corr', 'corrwith', 'count', 'cov', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'div', 'divide', 'dot', 'drop', 'drop_duplicates', 'droplevel', 'dropna', 'dtypes', 'duplicated', 'empty', 'eq', 'equals', 'eval', 'ewm', 'expanding', 'explode', 'ffill', 'fillna', 'filter', 'first', 'first_valid_index', 'flags', 'floordiv', 'from_dict', 'from_records', 'ge', 'get', 'groupby', 'gt', 'head', 'hist', 'iat', 'idxmax', 'idxmin', 'iloc', 'index', 'infer_objects', 'info', 'insert', 'interpolate', 'isin', 'isna', 'isnull', 'items', 'iteritems', 'iterrows', 'itertuples', 'join', 'keys', 'kurt', 'kurtosis', 'last', 'last_valid_index', 'le', 'loc', 'lookup', 'lt', 'mad', 'mask', 'max', 'mean', 'median', 'melt', 'memory_usage', 'merge', 'min', 'mod', 'mode', 'mul', 'multiply', 'ndim', 'ne', 'nlargest', 'notna', 'notnull', 'nsmallest', 'nunique', 'pad', 'pct_change', 'pipe', 'pivot', 'pivot_table', 'plot', 'pop', 'pow', 'prod', 'product', 'quantile', 'query', 'radd', 'rank', 'rdiv', 'reindex', 'reindex_like', 'rename', 'rename_axis', 'reorder_levels', 'replace', 'resample', 'reset_index', 'rfloordiv', 'rmod', 'rmul', 'rolling', 'round', 'rpow', 'rsub', 'rtruediv', 'sample', 'select_dtypes', 'sem', 'set_axis', 'set_flags', 'set_index', 'shape', 'shift', 'size', 'skew', 'slice_shift', 'sort_index', 'sort_values', 'sparse', 'squeeze', 'stack', 'std', 'style', 'sub', 'subtract', 'sum', 'swapaxes', 'swaplevel', 'tail', 'take', 'to_clipboard', 'to_csv', 'to_dict', 'to_excel', 'to_feather', 'to_gbq', 'to_hdf', 'to_html', 'to_json', 'to_latex', 'to_markdown', 'to_numpy', 'to_parquet', 'to_period', 'to_pickle', 'to_records', 'to_sql', 'to_stata', 'to_string', 'to_timestamp', 'to_xarray', 'to_xml', 'transform', 'transpose', 'truediv', 'truncate', 'tshift', 'tz_convert', 'tz_localize', 'unstack', 'update', 'value_counts', 'values', 'var', 'where', 'xs']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pd.DataFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.DataFrame.toDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(sour).toDf(*target).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Manipulating Columns in Spark Data Frames**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25d0b264b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('orders.csv',header=True)\n",
    "# ,sc?hema='Row_ID int,Order_ID int,Order_Date string,Ship_Date string,Ship_Mode string,Customer_ID int,Customer_Name string,Segment string,Location string,State string,Postal_Code int,Region string,Product_ID string,Category string,`Sub-Category` string,Product_Name string,Sales  decimal(18,4),Quantity int,Discount  decimal(18,4),Profit decimal(18,4)',header=False).\\\n",
    "    # show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------+--------------+-----------+------------------+-----------+--------------------+--------------+-----------+-------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|Row ID|      Order ID|Order Date| Ship Date|     Ship Mode|Customer ID|     Customer Name|    Segment|            Location|         State|Postal Code| Region|     Product ID|       Category|Sub-Category|        Product Name|   Sales|Quantity|Discount|  Profit|\n",
      "+------+--------------+----------+----------+--------------+-----------+------------------+-----------+--------------------+--------------+-----------+-------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|     1|CA-2016-152156|15/04/2017|1101102016|  Second Class|   CG-12520|       Claire Gute|   Consumer|United States,Hen...|      Kentucky|      42420|  South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|  261.96|       2|       0| 41.9136|\n",
      "|     2|CA-2016-152156|15/04/2018|1101102016|  Second Class|   CG-12520|       Claire Gute|   Consumer|United States,Hen...|      Kentucky|      42420|  South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|  731.94|       3|       0| 219.582|\n",
      "|     3|CA-2016-138688|15/04/2019| 601602016|  Second Class|   DV-13045|   Darrin Van Huff|  Corporate|United States,Los...|    California|      90036|   West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|   14.62|       2|       0|  6.8714|\n",
      "|     4|US-2015-108966|15/04/2020|1001802015|Standard Class|   SO-20335|    Sean O'Donnell|   Consumer|United States,For...|       Florida|      33311|  South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|957.5775|       5|    0.45|-383.031|\n",
      "|     5|US-2015-108966|15/04/2021|1001802015|Standard Class|   SO-20335|    Sean O'Donnell|   Consumer|United States,For...|       Florida|      33311|  South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|  22.368|       2|     0.2|  2.5164|\n",
      "|     6|CA-2014-115812|15/04/2022| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|FUR-FU-10001487|      Furniture| Furnishings|Eldon Expressions...|   48.86|       7|       0| 14.1694|\n",
      "|     7|CA-2014-115812|15/04/2023| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|OFF-AR-10002833|Office Supplies|         Art|          Newell 322|    7.28|       4|       0|  1.9656|\n",
      "|     8|CA-2014-115812|15/04/2024| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|TEC-PH-10002275|     Technology|      Phones|Mitel 5320 IP Pho...| 907.152|       6|     0.2| 90.7152|\n",
      "|     9|CA-2014-115812|15/04/2025| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|OFF-BI-10003910|Office Supplies|     Binders|DXL Angle-View Bi...|  18.504|       3|     0.2|  5.7825|\n",
      "|    10|CA-2014-115812|15/04/2026| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|OFF-AP-10002892|Office Supplies|  Appliances|Belkin F5C206VTEL...|   114.9|       5|       0|   34.47|\n",
      "|    11|CA-2014-115812|15/04/2027| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|FUR-TA-10001539|      Furniture|      Tables|Chromcraft Rectan...|1706.184|       9|     0.2| 85.3092|\n",
      "|    12|CA-2014-115812|15/04/2028| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|TEC-PH-10002033|     Technology|      Phones|Konftel 250 Confe...| 911.424|       4|     0.2| 68.3568|\n",
      "|    13|CA-2017-114412|15/04/2017| 402002017|Standard Class|   AA-10480|      Andrew Allen|   Consumer|United States,Con...|North Carolina|      28027|  South|OFF-PA-10002365|Office Supplies|       Paper|          Xerox 1967|  15.552|       3|     0.2|  5.4432|\n",
      "|    14|CA-2016-161389|15/04/2017|1201002016|Standard Class|   IM-15070|      Irene Maddox|   Consumer|United States,Sea...|    Washington|      98103|   West|OFF-BI-10003656|Office Supplies|     Binders|Fellowes PB200 Pl...| 407.976|       3|     0.2|132.5922|\n",
      "|    15|US-2015-118983|22/11/2015|1102602015|Standard Class|   HP-14815|     Harold Pawlan|Home Office|United States,For...|         Texas|      76106|Central|OFF-AP-10002311|Office Supplies|  Appliances|Holmes Replacemen...|   68.81|       5|     0.8|-123.858|\n",
      "|    16|US-2015-118983|22/11/2015|1102602015|Standard Class|   HP-14815|     Harold Pawlan|Home Office|United States,For...|         Texas|      76106|Central|OFF-BI-10000756|Office Supplies|     Binders|Storex DuraTech R...|   2.544|       3|     0.8|  -3.816|\n",
      "|    17|CA-2014-105893|15/04/2017|1101802014|Standard Class|   PK-19075|         Pete Kriz|   Consumer|United States,Mad...|     Wisconsin|      53711|Central|OFF-ST-10004186|Office Supplies|     Storage|\"Stur-D-Stor Shel...|  665.88|       6|       0| 13.3176|\n",
      "|    18|CA-2014-167164|13/05/2014| 501502014|  Second Class|   AG-10270|   Alejandro Grove|   Consumer|United States,Wes...|          Utah|      84084|   West|OFF-ST-10000107|Office Supplies|     Storage|Fellowes Super St...|    55.5|       2|       0|    9.99|\n",
      "|    19|CA-2014-143336|27/08/2014|  90102014|  Second Class|   ZD-21925|Zuschuss Donatelli|   Consumer|United States,San...|    California|      94109|   West|OFF-AR-10003056|Office Supplies|         Art|          Newell 341|    8.56|       2|       0|  2.4824|\n",
      "|    20|CA-2014-143336|27/08/2014|  90102014|  Second Class|   ZD-21925|Zuschuss Donatelli|   Consumer|United States,San...|    California|      94109|   West|TEC-PH-10001949|     Technology|      Phones|Cisco SPA 501G IP...|  213.48|       3|     0.2|  16.011|\n",
      "+------+--------------+----------+----------+--------------+-----------+------------------+-----------+--------------------+--------------+-----------+-------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: string (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Ship Mode: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Customer Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Postal Code: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Product Name: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Discount: string (nullable = true)\n",
      " |-- Profit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.select('Row ID','Order ID','Ship Date','Order Date').withColumn('order_month',to_date(col('Order Date'),'dd/MM/yyyy').alias('order_month')).dropna(subset=['order_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------+-----------+----------+\n",
      "|Row ID|      Order ID| Ship Date|Order Date|order_month|month_data|\n",
      "+------+--------------+----------+----------+-----------+----------+\n",
      "|     1|CA-2016-152156|1101102016|15/04/2017| 2017-04-15|    201704|\n",
      "|     2|CA-2016-152156|1101102016|15/04/2018| 2018-04-15|    201804|\n",
      "|     3|CA-2016-138688| 601602016|15/04/2019| 2019-04-15|    201904|\n",
      "|     4|US-2015-108966|1001802015|15/04/2020| 2020-04-15|    202004|\n",
      "|     5|US-2015-108966|1001802015|15/04/2021| 2021-04-15|    202104|\n",
      "|     6|CA-2014-115812| 601402014|15/04/2022| 2022-04-15|    202204|\n",
      "|     7|CA-2014-115812| 601402014|15/04/2023| 2023-04-15|    202304|\n",
      "|     8|CA-2014-115812| 601402014|15/04/2024| 2024-04-15|    202404|\n",
      "|     9|CA-2014-115812| 601402014|15/04/2025| 2025-04-15|    202504|\n",
      "|    10|CA-2014-115812| 601402014|15/04/2026| 2026-04-15|    202604|\n",
      "|    11|CA-2014-115812| 601402014|15/04/2027| 2027-04-15|    202704|\n",
      "|    12|CA-2014-115812| 601402014|15/04/2028| 2028-04-15|    202804|\n",
      "|    13|CA-2017-114412| 402002017|15/04/2017| 2017-04-15|    201704|\n",
      "|    14|CA-2016-161389|1201002016|15/04/2017| 2017-04-15|    201704|\n",
      "|    15|US-2015-118983|1102602015|22/11/2015| 2015-11-22|    201511|\n",
      "|    16|US-2015-118983|1102602015|22/11/2015| 2015-11-22|    201511|\n",
      "|    17|CA-2014-105893|1101802014|15/04/2017| 2017-04-15|    201704|\n",
      "|    18|CA-2014-167164| 501502014|13/05/2014| 2014-05-13|    201405|\n",
      "|    19|CA-2014-143336|  90102014|27/08/2014| 2014-08-27|    201408|\n",
      "|    20|CA-2014-143336|  90102014|27/08/2014| 2014-08-27|    201408|\n",
      "+------+--------------+----------+----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df1.select('Row ID','Order ID','Ship Date','Order Date','order_month').withColumn('month_data',date_format(col('order_month'),'yyyyMM'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|order_month|count|\n",
      "+-----------+-----+\n",
      "| 2014-05-21|   10|\n",
      "| 2014-05-30|    8|\n",
      "| 2014-05-26|    8|\n",
      "| 2014-05-23|    7|\n",
      "| 2014-05-20|    7|\n",
      "| 2014-05-13|    6|\n",
      "| 2014-05-27|    5|\n",
      "| 2014-05-18|    4|\n",
      "| 2014-05-25|    3|\n",
      "| 2014-05-28|    3|\n",
      "| 2014-05-31|    2|\n",
      "| 2014-05-16|    2|\n",
      "| 2014-05-22|    2|\n",
      "| 2014-05-19|    2|\n",
      "| 2014-05-14|    1|\n",
      "| 2014-05-24|    1|\n",
      "| 2014-05-17|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(col('month_data')=='201405').groupBy(col('order_month')).count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **lower Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------------------------+--------------+-----------+-----------------------------+-----------------------------+\n",
      "|Row ID|Order ID      |Location                     |State         |Postal Code|loctions                     |Locations                    |\n",
      "+------+--------------+-----------------------------+--------------+-----------+-----------------------------+-----------------------------+\n",
      "|1     |CA-2016-152156|United States,Henderson      |Kentucky      |42420      |united states,henderson      |United States,henderson      |\n",
      "|2     |CA-2016-152156|United States,Henderson      |Kentucky      |42420      |united states,henderson      |United States,henderson      |\n",
      "|3     |CA-2016-138688|United States,Los Angeles    |California    |90036      |united states,los angeles    |United States,los Angeles    |\n",
      "|4     |US-2015-108966|United States,Fort Lauderdale|Florida       |33311      |united states,fort lauderdale|United States,fort Lauderdale|\n",
      "|5     |US-2015-108966|United States,Fort Lauderdale|Florida       |33311      |united states,fort lauderdale|United States,fort Lauderdale|\n",
      "|6     |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|7     |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|8     |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|9     |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|10    |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|11    |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|12    |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|13    |CA-2017-114412|United States,Concord        |North Carolina|28027      |united states,concord        |United States,concord        |\n",
      "|14    |CA-2016-161389|United States,Seattle        |Washington    |98103      |united states,seattle        |United States,seattle        |\n",
      "|15    |US-2015-118983|United States,Fort Worth     |Texas         |76106      |united states,fort worth     |United States,fort Worth     |\n",
      "|16    |US-2015-118983|United States,Fort Worth     |Texas         |76106      |united states,fort worth     |United States,fort Worth     |\n",
      "|17    |CA-2014-105893|United States,Madison        |Wisconsin     |53711      |united states,madison        |United States,madison        |\n",
      "|18    |CA-2014-167164|United States,West Jordan    |Utah          |84084      |united states,west jordan    |United States,west Jordan    |\n",
      "|19    |CA-2014-143336|United States,San Francisco  |California    |94109      |united states,san francisco  |United States,san Francisco  |\n",
      "|20    |CA-2014-143336|United States,San Francisco  |California    |94109      |united states,san francisco  |United States,san Francisco  |\n",
      "+------+--------------+-----------------------------+--------------+-----------+-----------------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Row ID','Order ID','Location','State','Postal Code').withColumn('loctions',lower(col('Location'))).withColumn('Locations',initcap(col('loctions'))).show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concat_ws Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------------------------+--------------+-----------------------------+--------------+-----------+--------------------------------------+\n",
      "|Row ID|Order ID      |Location                     |State         |Location                     |State         |Postal Code|combine                               |\n",
      "+------+--------------+-----------------------------+--------------+-----------------------------+--------------+-----------+--------------------------------------+\n",
      "|1     |CA-2016-152156|United States,Henderson      |Kentucky      |United States,Henderson      |Kentucky      |42420      |United States,Henderson_Kentucky      |\n",
      "|2     |CA-2016-152156|United States,Henderson      |Kentucky      |United States,Henderson      |Kentucky      |42420      |United States,Henderson_Kentucky      |\n",
      "|3     |CA-2016-138688|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90036      |United States,Los Angeles_California  |\n",
      "|4     |US-2015-108966|United States,Fort Lauderdale|Florida       |United States,Fort Lauderdale|Florida       |33311      |United States,Fort Lauderdale_Florida |\n",
      "|5     |US-2015-108966|United States,Fort Lauderdale|Florida       |United States,Fort Lauderdale|Florida       |33311      |United States,Fort Lauderdale_Florida |\n",
      "|6     |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|7     |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|8     |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|9     |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|10    |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|11    |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|12    |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|13    |CA-2017-114412|United States,Concord        |North Carolina|United States,Concord        |North Carolina|28027      |United States,Concord_North Carolina  |\n",
      "|14    |CA-2016-161389|United States,Seattle        |Washington    |United States,Seattle        |Washington    |98103      |United States,Seattle_Washington      |\n",
      "|15    |US-2015-118983|United States,Fort Worth     |Texas         |United States,Fort Worth     |Texas         |76106      |United States,Fort Worth_Texas        |\n",
      "|16    |US-2015-118983|United States,Fort Worth     |Texas         |United States,Fort Worth     |Texas         |76106      |United States,Fort Worth_Texas        |\n",
      "|17    |CA-2014-105893|United States,Madison        |Wisconsin     |United States,Madison        |Wisconsin     |53711      |United States,Madison_Wisconsin       |\n",
      "|18    |CA-2014-167164|United States,West Jordan    |Utah          |United States,West Jordan    |Utah          |84084      |United States,West Jordan_Utah        |\n",
      "|19    |CA-2014-143336|United States,San Francisco  |California    |United States,San Francisco  |California    |94109      |United States,San Francisco_California|\n",
      "|20    |CA-2014-143336|United States,San Francisco  |California    |United States,San Francisco  |California    |94109      |United States,San Francisco_California|\n",
      "+------+--------------+-----------------------------+--------------+-----------------------------+--------------+-----------+--------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select ('Row ID','Order ID','Location','State','Location','State','Postal Code').withColumn('combine',concat_ws('_','Location','State')).show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "| id|  name|is_customer|               phone|             update|   dates|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|20220412|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|20210411|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|20210422|\n",
      "|  4|  khan|      false|        {null, null}|               null|    null|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "df.withColumn('dates',date_format('update','yyyyMMdd')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    year|count|\n",
      "+--------+-----+\n",
      "|20220412|    1|\n",
      "|20210411|    1|\n",
      "|20210422|    1|\n",
      "|    null|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Group by\n",
    "df.groupBy(date_format('update','yyyyMMdd').alias('year')).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(date_format('update','yyyyMMdd').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col('name').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df['name'].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+-----+\n",
      "| id|  name|is_customer|               phone|             update|names|\n",
      "+---+------+-----------+--------------------+-------------------+-----+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00| null|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00| null|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00| null|\n",
      "|  4|  khan|      false|        {null, null}|               null| null|\n",
      "+---+------+-----------+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('names','id'+lit(5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+-----+\n",
      "| id|  name|is_customer|               phone|             update|names|\n",
      "+---+------+-----------+--------------------+-------------------+-----+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|    6|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|    7|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|    8|\n",
      "|  4|  khan|      false|        {null, null}|               null|    9|\n",
      "+---+------+-----------+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('names',col('id')+lit(5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "| id|  name|is_customer|               phone|             update|  new_id|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|asfand_1|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00| saeed_2|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|   ali_3|\n",
      "|  4|  khan|      false|        {null, null}|               null|  khan_4|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_id',concat(df.name,lit('_'),df.id)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "| id|  name|is_customer|               phone|             update| new_ids|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|asfand_1|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00| saeed_2|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|   ali_3|\n",
      "|  4|  khan|      false|        {null, null}|               null|  khan_4|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_ids',concat(\"name\",lit('_'),\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "| id|  name|is_customer|               phone|             update| new_ids|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|asfand_1|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00| saeed_2|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|   ali_3|\n",
      "|  4|  khan|      false|        {null, null}|               null|  khan_4|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_ids',concat_ws(\"_\",\"name\",\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "| id|  name|is_customer|               phone|             update|   new_phone|\n",
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|+92329677783|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|+92329622283|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|+92329644483|\n",
      "|  4|  khan|      false|        {null, null}|               null|        null|\n",
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take the first element from struct type column\n",
    "df.withColumn('new_phone',df.phone.first).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Substrings**\n",
    "* If we are working on fixed length record and we want to extract information from that we can use substring.\n",
    "* Columns like product name ,ID , Status etc are best examples of using substring on top of substring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23529a74b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task is to get the first country code of phone numbers and customer number should be in different column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "     \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=pd.DataFrame(usr_Struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+----------------------------+-------------------+\n",
      "|id |name  |is_customer|phone                       |update             |\n",
      "+---+------+-----------+----------------------------+-------------------+\n",
      "|1  |asfand|true       |{+92329677783, +92342454672}|2022-04-12 04:10:00|\n",
      "|2  |saeed |true       |{+92329622283, +92342442312}|2021-04-11 04:10:00|\n",
      "|3  |ali   |false      |{+92329644483, +92342445552}|2021-04-22 12:10:00|\n",
      "|4  |khan  |false      |{null, null}                |null               |\n",
      "+---+------+-----------+----------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Taking position in +ve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+--------------+----+\n",
      "| id|  name|is_customer|               phone|             update|phone_customer|code|\n",
      "+---+------+-----------+--------------------+-------------------+--------------+----+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|     329677783| +92|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|     329622283| +92|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|     329644483| +92|\n",
      "|  4|  khan|      false|        {null, null}|               null|          null|null|\n",
      "+---+------+-----------+--------------------+-------------------+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').withColumn('phone_customer',substring(col('phone')['first'],4,10)).withColumn('code',substring(col('phone.first'),1,3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+--------------+----+\n",
      "| id|  name|is_customer|               phone|             update|phone_customer|code|\n",
      "+---+------+-----------+--------------------+-------------------+--------------+----+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|     329677783| +92|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|     329622283| +92|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|     329644483| +92|\n",
      "|  4|  khan|      false|        {null, null}|               null|          null|null|\n",
      "+---+------+-----------+--------------------+-------------------+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').withColumn('phone_customer',substring(col('phone')['first'],-9,10)).withColumn('code',substring(col('phone.first'),-12,3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+-------+\n",
      "| id|  name|is_customer|               phone|             update|country|\n",
      "+---+------+-----------+--------------------+-------------------+-------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|    +92|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|    +92|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|    +92|\n",
      "|  4|  khan|      false|        {null, null}|               null|   null|\n",
      "+---+------+-----------+--------------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('country',substring(df.phone.first,1,3)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extracting Strings Using Split from Spark Data Frame**\n",
    "* If we are working with variable length column then its best way to first split the data by delimiter and then apply operations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example 1::**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "     \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+-----------+\n",
      "| id|  name|is_customer|               phone|             update|  new_phone|\n",
      "+---+------+-----------+--------------------+-------------------+-----------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|00232967778|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|00232962228|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|00232964448|\n",
      "|  4|  khan|      false|        {null, null}|               null|       null|\n",
      "+---+------+-----------+--------------------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_phone',lpad(substring(df.phone.first,3,9),11,'0')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example 2::**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     \"+92329677783,+92342454672\",\n",
    "     datetime.datetime(2022,4,12,4,10,0)\n",
    "    ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     \"+92329622283,+92342442312\",\n",
    "     datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     \"+92329644483,+92342445552\", ## Predefined structure as a Row\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "     \",\",\n",
    "     None)#['+92329633456','+9234247654'],\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-------------------------+-------------------+\n",
      "|id |name  |is_customer|phone                    |dates              |\n",
      "+---+------+-----------+-------------------------+-------------------+\n",
      "|1  |asfand|true       |+92329677783,+92342454672|2022-04-12 04:10:00|\n",
      "|2  |saeed |true       |+92329622283,+92342442312|2021-04-11 04:10:00|\n",
      "|3  |ali   |false      |+92329644483,+92342445552|2021-04-22 12:10:00|\n",
      "|4  |khan  |false      |,                        |null               |\n",
      "+---+------+-----------+-------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sec=\"\"\"\n",
    "id int,\n",
    "name string,\n",
    "is_customer boolean,\n",
    "phone string,\n",
    "dates timestamp\n",
    "\n",
    "\"\"\"\n",
    "df=spark.createDataFrame(usr_Struct,schema=sec)\n",
    "df.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- dates: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(usr_Struct,schema=sec).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-------------------------+-------------------+------------+\n",
      "|id |name  |is_customer|phone                    |dates              |phones      |\n",
      "+---+------+-----------+-------------------------+-------------------+------------+\n",
      "|1  |asfand|true       |+92329677783,+92342454672|2022-04-12 04:10:00|+92329677783|\n",
      "|1  |asfand|true       |+92329677783,+92342454672|2022-04-12 04:10:00|+92342454672|\n",
      "|2  |saeed |true       |+92329622283,+92342442312|2021-04-11 04:10:00|+92329622283|\n",
      "|2  |saeed |true       |+92329622283,+92342442312|2021-04-11 04:10:00|+92342442312|\n",
      "|3  |ali   |false      |+92329644483,+92342445552|2021-04-22 12:10:00|+92329644483|\n",
      "|3  |ali   |false      |+92329644483,+92342445552|2021-04-22 12:10:00|+92342445552|\n",
      "|4  |khan  |false      |,                        |null               |            |\n",
      "|4  |khan  |false      |,                        |null               |            |\n",
      "+---+------+-----------+-------------------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (\"*\").withColumn('phones',explode(split(col('phone'),','))).show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now lets count the number of phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|id |count|\n",
      "+---+-----+\n",
      "|1  |2    |\n",
      "|2  |2    |\n",
      "|3  |2    |\n",
      "|4  |2    |\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\").withColumn('phone',explode(split(col('phone'),','))).groupBy(col('id')).count().show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Padding Characters around Strings in Spark DataFrame Columns**\n",
    "* When we want to make fix length columns we use padding.\n",
    "* It is good for main frame systems.\n",
    "\n",
    "### Task ::\n",
    "* Id should be padded with zeros if the length is not 3.\n",
    "* The name should be right padded ie '-' on right side.\n",
    "* The phone num should me right paddded if its less then 9 with '-'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     \"+92329677783,+92342454672\",\n",
    "     datetime.datetime(2022,4,12,4,10,0)\n",
    "    ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     \"+92329622283,+92342442312\",\n",
    "     datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     \"+92329644483,+92342445552\", ## Predefined structure as a Row\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "     \",\",\n",
    "     None)#['+92329633456','+9234247654'],\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------------------------+-------------------+\n",
      "|_1 |_2    |_3   |_4                       |_5                 |\n",
      "+---+------+-----+-------------------------+-------------------+\n",
      "|1  |asfand|true |+92329677783,+92342454672|2022-04-12 04:10:00|\n",
      "|2  |saeed |true |+92329622283,+92342442312|2021-04-11 04:10:00|\n",
      "|3  |ali   |false|+92329644483,+92342445552|2021-04-22 12:10:00|\n",
      "|4  |khan  |false|,                        |null               |\n",
      "+---+------+-----+-------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(usr_Struct).show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(usr_Struct).toDF(\"id\",\"name\",\"is_customer\",\"phone\",\"dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+-------------+\n",
      "|id |name  |phone       |phones       |\n",
      "+---+------+------------+-------------+\n",
      "|001|asfand|+92329677783|++92329677783|\n",
      "|001|asfand|+92342454672|++92342454672|\n",
      "|002|saeed-|+92329622283|++92329622283|\n",
      "|002|saeed-|+92342442312|++92342442312|\n",
      "|003|ali---|+92329644483|++92329644483|\n",
      "|003|ali---|+92342445552|++92342445552|\n",
      "|004|khan--|            |+++++++++++++|\n",
      "|004|khan--|            |+++++++++++++|\n",
      "+---+------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (lpad(col('id'),3,'0').alias('id'),rpad(col('name'),6,'-').alias(\"name\"),explode(split(col('phone'),',')).alias('phone')).withColumn('phones',lpad('phone',13,'+')).show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Trimming Characters from strings in Spark Data Frame Columns**\n",
    "* We use rtrim ,ltrim and trim to remove the spaces on right ,left and on both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function rtrim in module pyspark.sql.functions:\n",
      "\n",
      "rtrim(col)\n",
      "    Trim the spaces from right end for the specified string value.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(rtrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ltrim in module pyspark.sql.functions:\n",
      "\n",
      "ltrim(col)\n",
      "    Trim the spaces from left end for the specified string value.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ltrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function trim in module pyspark.sql.functions:\n",
      "\n",
      "trim(col)\n",
      "    Trim the spaces from both ends for the specified string column.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Function: trim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.StringTrim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Usage: \\n    trim(str) - Removes the leading and trailing space characters from `str`.\\n\\n    trim(BOTH FROM str) - Removes the leading and trailing space characters from `str`.\\n\\n    trim(LEADING FROM str) - Removes the leading space characters from `str`.\\n\\n    trim(TRAILING FROM str) - Removes the trailing space characters from `str`.\\n\\n    trim(trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\\n\\n    trim(BOTH trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\\n\\n    trim(LEADING trimStr FROM str) - Remove the leading `trimStr` characters from `str`.\\n\\n    trim(TRAILING trimStr FROM str) - Remove the trailing `trimStr` characters from `str`.\\n  |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE FUNCTION trim').show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To remove special character on right side we can use trim(LEADING '.' From col('name')).\n",
    "* To remove special character on left side we can use trim(TRAILING '' From col('name')).\n",
    "* To remove special character on both side we can use trim(BOTH '' From col('name'))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     \"+92329677783,+92342454672\",\n",
    "     datetime.datetime(2022,4,12,4,10,0)\n",
    "    ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     \"+92329622283,+92342442312\",\n",
    "     datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     \"+92329644483,+92342445552\", ## Predefined structure as a Row\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "     \",\",\n",
    "     None)#['+92329633456','+9234247654'],\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|              dates|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|+92329677783,+923...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|+92329622283,+923...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|+92329644483,+923...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|                   ,|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(usr_Struct).toDF(\"id\",\"name\",\"is_customer\",\"phone\",\"dates\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+-------------+\n",
      "| id|  name|       phone|       phones|\n",
      "+---+------+------------+-------------+\n",
      "|001|asfand|+92329677783|++92329677783|\n",
      "|001|asfand|+92342454672|++92342454672|\n",
      "|002|saeed-|+92329622283|++92329622283|\n",
      "|002|saeed-|+92342442312|++92342442312|\n",
      "|003|ali---|+92329644483|++92329644483|\n",
      "|003|ali---|+92342445552|++92342445552|\n",
      "|004|khan--|            |+++++++++++++|\n",
      "|004|khan--|            |+++++++++++++|\n",
      "+---+------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trim_df=df.select (lpad(col('id'),3,'0').alias('id'),rpad(col('name'),6,'-').alias(\"name\"),explode(split(col('phone'),',')).alias('phone')).withColumn('phones',lpad('phone',13,'+'))\n",
    "trim_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-----------+\n",
      "| id|  name|      phone|     phones|\n",
      "+---+------+-----------+-----------+\n",
      "|001|asfand|92329677783|92329677783|\n",
      "|001|asfand|92342454672|92342454672|\n",
      "|002| saeed|92329622283|92329622283|\n",
      "|002| saeed|92342442312|92342442312|\n",
      "|003|   ali|92329644483|92329644483|\n",
      "|003|   ali|92342445552|92342445552|\n",
      "|004|  khan|           |           |\n",
      "|004|  khan|           |           |\n",
      "+---+------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trim_df.withColumn('name',expr(\"trim(TRAILING '-' FROM trim(name))\")).\\\n",
    "    withColumn('phone',expr(\"trim(LEADING '+' FROM trim(phone))\")).\\\n",
    "        withColumn('phones',expr(\"trim(BOTH '+' FROM phones)\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dealing with String type data and Converting to time and date datatype**\n",
    "* If we get any type of time that is in form of string we can convert it to date and timestamp by using to_date() and to_timestamp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame([('X',),('Y',)]).toDF('sa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n",
      "|sa |dates     |times                  |\n",
      "+---+----------+-----------------------+\n",
      "|X  |2022-01-01|2022-01-01 01:01:01.111|\n",
      "|Y  |2022-01-01|2022-01-01 01:01:01.111|\n",
      "+---+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "df.withColumn('dates',to_date(lit('20220101010101111'),'yyyyMMddHHmmssSSS')).withColumn('times',to_timestamp(lit('20220101010101111'),'yyyyMMddHHmmssSSS')).show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sa: string (nullable = true)\n",
      " |-- dates: date (nullable = true)\n",
      " |-- times: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('dates',to_date(lit('20220101010101111'),'yyyyMMddHHmmssSSS')).withColumn('times',to_timestamp(lit('20220101010101111'),'yyyyMMddHHmmssSSS')).printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Date and Time Arithmetic using Spark Data Frames**\n",
    "* Now as we have gone through how to make a date column and timestamp column out of a string of date we will do arithmatic operation on top of the data.\n",
    "* date_add,date_sub,date_diff,month_between,add_month,sub_month,next_month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x29c3c8a4b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|dates     |times                  |\n",
      "+----------+-----------------------+\n",
      "|2014-01-05|2014-01-05 06:20:01 000|\n",
      "|2013-02-05|2016-01-05 12:32:01 000|\n",
      "|2015-05-08|2018-07-04 11:00:01 000|\n",
      "|2018-03-07|2016-04-07 10:22:01 000|\n",
      "|2019-09-05|2011-05-05 09:32:01 000|\n",
      "+----------+-----------------------+\n",
      "\n",
      "root\n",
      " |-- dates: string (nullable = true)\n",
      " |-- times: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dates=[\n",
    "    ('2014-01-05','2014-01-05 06:20:01 000'),\n",
    "    ('2013-02-05','2016-01-05 12:32:01 000'),\n",
    "    ('2015-05-08','2018-07-04 11:00:01 000'),\n",
    "    ('2018-03-07','2016-04-07 10:22:01 000'),\n",
    "    ('2019-09-05','2011-05-05 09:32:01 000')   \n",
    "]\n",
    "us=\"\"\"dates String,times String\"\"\"\n",
    "df=spark.createDataFrame(dates,us)\n",
    "df.show(truncate=0)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+----------+----------+----------+---------+--------------+\n",
      "|dates     |times                  |add_dates |add_times |sub_date  |sub_time  |add_month |diff_date|months_between|\n",
      "+----------+-----------------------+----------+----------+----------+----------+----------+---------+--------------+\n",
      "|2014-01-05|2014-01-05 06:20:01 000|2014-01-08|2014-01-08|2014-01-02|2014-01-02|2014-04-05|3521     |115.71        |\n",
      "|2013-02-05|2016-01-05 12:32:01 000|2013-02-08|2016-01-08|2013-02-02|2016-01-02|2013-05-05|3855     |126.71        |\n",
      "|2015-05-08|2018-07-04 11:00:01 000|2015-05-11|2018-07-07|2015-05-05|2018-07-01|2015-08-08|3033     |99.61         |\n",
      "|2018-03-07|2016-04-07 10:22:01 000|2018-03-10|2016-04-10|2018-03-04|2016-04-04|2018-06-07|1999     |65.65         |\n",
      "|2019-09-05|2011-05-05 09:32:01 000|2019-09-08|2011-05-08|2019-09-02|2011-05-02|2019-12-05|1452     |47.71         |\n",
      "+----------+-----------------------+----------+----------+----------+----------+----------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('add_dates',date_add(col('dates'),3)).\\\n",
    "    withColumn('add_times',date_add(col('times'),3)).\\\n",
    "        withColumn('sub_date',date_sub(col('dates'),3)).\\\n",
    "            withColumn('sub_date',date_sub(col('dates'),3)).\\\n",
    "                withColumn('sub_time',date_sub(col('times'),3)).\\\n",
    "                    withColumn('add_month',add_months(col('dates'),3)).\\\n",
    "                        withColumn('diff_date',datediff(current_date(),col('dates'))).\\\n",
    "                            withColumn('months_between',round(months_between(current_date(),col('dates')),2)).\\\n",
    "        show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date_Functions\n",
    "* *date_format()*: Takes date/timestamp/string and convert to desired pattern.\n",
    "* *to_date*: Takes string or col and then we tell him what value show what and then convert it into yyyy-MM-dd\n",
    "* *to_timestamp*: Takes string or col and then we tell him what value show what and then convert it into yyyy-MM-dd hh:mm:ss SSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_timestamp in module pyspark.sql.functions:\n",
      "\n",
      "to_timestamp(col, format=None)\n",
      "    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\n",
      "    using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "    By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n",
      "    is omitted. Equivalent to ``col.cast(\"timestamp\")``.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 2.2.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n",
      "    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n",
      "    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(to_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x217583d5f30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|     DATES|              TIMES|\n",
      "+----------+-------------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|\n",
      "|2018-02-12|2018-02-12 09:10:23|\n",
      "|2019-03-11|2019-03-11 11:03:22|\n",
      "|2022-04-15|2022-04-15 10:11:21|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_s=[(\"2021-01-14\",\"2021-01-14 10:00:21\"),\n",
    "(\"2018-02-12\",\"2018-02-12 09:10:23\"),\n",
    "(\"2019-03-11\",\"2019-03-11 11:03:22\"),\n",
    "(\"2022-04-15\",\"2022-04-15 10:11:21\")]\n",
    "data=spark.createDataFrame(date_s,schema=\"DATES STRING,TIMES STRING\")\n",
    "data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Time operation::\n",
    "* *date_add*: It will add number of days to the date and then give the result.\n",
    "* *date_sub*: It will subtract number of days to the date and gie result.\n",
    "* *date_diff*: It will subtract the dates and give you int value.\n",
    "* *add_month*: It will add number of months and show the date not timestamp.\n",
    "* *months_between*:It will show the number of months between the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----------+----------+-------------------+----------+-----------+\n",
      "|     DATES|              TIMES|  date_add|      date|             timess|  date_sub|dates_to_go|\n",
      "+----------+-------------------+----------+----------+-------------------+----------+-----------+\n",
      "|2021-01-14|2021-01-14 10:00:21|2021-01-21|2021-01-14|2021-01-14 00:00:00|2021-01-06| 2021-05-14|\n",
      "|2018-02-12|2018-02-12 09:10:23|2018-02-19|2018-02-12|2018-02-12 00:00:00|2018-02-04| 2018-06-12|\n",
      "|2019-03-11|2019-03-11 11:03:22|2019-03-18|2019-03-11|2019-03-11 00:00:00|2019-03-03| 2019-07-11|\n",
      "|2022-04-15|2022-04-15 10:11:21|2022-04-22|2022-04-15|2022-04-15 00:00:00|2022-04-07| 2022-08-15|\n",
      "+----------+-------------------+----------+----------+-------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('date_add',date_add(\"DATES\",7)).\\\n",
    " withColumn('date',to_date('DATES','yyyy-MM-dd')).\\\n",
    "     withColumn('timess',to_timestamp('DATES','yyyy-MM-dd')).\\\n",
    "         withColumn('date_sub',date_sub('TIMES',8)).\\\n",
    "             withColumn('dates_to_go',datediff(\"DATES\",\"date_add\")).\\\n",
    "                 withColumn('dates_to_go',round(months_between(\"DATES\",\"date_add\"),2)).\\\n",
    "                      withColumn('dates_to_go',add_months(\"TIMES\",4)).\\\n",
    "     show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time Trunc Function\n",
    "* *trunc()*: will return date and you can take 'MM' as month first date and 'yyyy' as year first date. It will return date there is no timestamp.\n",
    "* *date_trunc()*: will return timestamp and its very flexible as we can get 'mm' as month first timestamp,'yy' or 'yyyy' as year first timestamp,'HOUR' first timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function trunc in module pyspark.sql.functions:\n",
      "\n",
      "trunc(date, format)\n",
      "    Returns date truncated to the unit specified by the format.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    date : :class:`~pyspark.sql.Column` or str\n",
      "    format : str\n",
      "        'year', 'yyyy', 'yy' to truncate by year,\n",
      "        or 'month', 'mon', 'mm' to truncate by month\n",
      "        Other options are: 'week', 'quarter'\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
      "    >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
      "    [Row(year=datetime.date(1997, 1, 1))]\n",
      "    >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
      "    [Row(month=datetime.date(1997, 2, 1))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|     DATES|              TIMES|\n",
      "+----------+-------------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|\n",
      "|2018-02-12|2018-02-12 09:10:23|\n",
      "|2019-03-11|2019-03-11 11:03:22|\n",
      "|2022-04-15|2022-04-15 10:11:21|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trunc,date_trunc\n",
    "date_s=[(\"2021-01-14\",\"2021-01-14 10:00:21\"),\n",
    "(\"2018-02-12\",\"2018-02-12 09:10:23\"),\n",
    "(\"2019-03-11\",\"2019-03-11 11:03:22\"),\n",
    "(\"2022-04-15\",\"2022-04-15 10:11:21\")]\n",
    "data=spark.createDataFrame(date_s,schema=\"DATES STRING,TIMES STRING\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------------+----------------+----------------+---------------+\n",
      "|     DATES|              TIMES|first_year_date|first_year1_date|first_month_date|first_week_date|\n",
      "+----------+-------------------+---------------+----------------+----------------+---------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|     2021-01-01|      2021-01-01|      2021-01-01|     2021-01-11|\n",
      "|2018-02-12|2018-02-12 09:10:23|     2018-01-01|      2018-01-01|      2018-02-01|     2018-02-12|\n",
      "|2019-03-11|2019-03-11 11:03:22|     2019-01-01|      2019-01-01|      2019-03-01|     2019-03-11|\n",
      "|2022-04-15|2022-04-15 10:11:21|     2022-01-01|      2022-01-01|      2022-04-01|     2022-04-11|\n",
      "+----------+-------------------+---------------+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('first_year_date',trunc(col('DATES'),'yy')).\\\n",
    "    withColumn('first_year1_date',trunc(col('DATES'),'yyyy')).\\\n",
    "    withColumn('first_month_date',trunc(col('DATES'),'month')).\\\n",
    "    withColumn('first_week_date',trunc(col('DATES'),'week')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_trunc in module pyspark.sql.functions:\n",
      "\n",
      "date_trunc(format, timestamp)\n",
      "    Returns timestamp truncated to the unit specified by the format.\n",
      "    \n",
      "    .. versionadded:: 2.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    format : str\n",
      "        'year', 'yyyy', 'yy' to truncate by year,\n",
      "        'month', 'mon', 'mm' to truncate by month,\n",
      "        'day', 'dd' to truncate by day,\n",
      "        Other options are:\n",
      "        'microsecond', 'millisecond', 'second', 'minute', 'hour', 'week', 'quarter'\n",
      "    timestamp : :class:`~pyspark.sql.Column` or str\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
      "    >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n",
      "    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n",
      "    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n",
      "    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|     DATES|              TIMES|               year|              month|            quarter|               hour|             second|               week|\n",
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|2021-01-01 00:00:00|2021-01-01 00:00:00|2021-01-01 00:00:00|2021-01-14 10:00:00|2021-01-14 10:00:21|2021-01-11 00:00:00|\n",
      "|2018-02-12|2018-02-12 09:10:23|2018-01-01 00:00:00|2018-02-01 00:00:00|2018-01-01 00:00:00|2018-02-12 09:00:00|2018-02-12 09:10:23|2018-02-12 00:00:00|\n",
      "|2019-03-11|2019-03-11 11:03:22|2019-01-01 00:00:00|2019-03-01 00:00:00|2019-01-01 00:00:00|2019-03-11 11:00:00|2019-03-11 11:03:22|2019-03-11 00:00:00|\n",
      "|2022-04-15|2022-04-15 10:11:21|2022-01-01 00:00:00|2022-04-01 00:00:00|2022-04-01 00:00:00|2022-04-15 10:00:00|2022-04-15 10:11:21|2022-04-11 00:00:00|\n",
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('year',date_trunc('yy',col('TIMES'))).\\\n",
    "    withColumn('month',date_trunc('month',col('TIMES'))).\\\n",
    "    withColumn('quarter',date_trunc('quarter',col('TIMES'))).\\\n",
    "    withColumn('hour',date_trunc('hour',col('TIMES'))).\\\n",
    "    withColumn('second',date_trunc('second',col('TIMES'))).\\\n",
    "    withColumn('week',date_trunc('week',col('TIMES'))).\\\n",
    "    show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Time extract Function\n",
    "As we have worked on Date and timestamps now lets work on extracting information from date and timestamps.\n",
    "* year\n",
    "* month\n",
    "* weekofyear\n",
    "* dayofyear\n",
    "* dayofmonth\n",
    "* dayofweek\n",
    "* hour\n",
    "* minute\n",
    "* second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1eab8b3d330>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|     DATES|              TIMES|\n",
      "+----------+-------------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|\n",
      "|2018-02-12|2018-02-12 09:10:23|\n",
      "|2019-03-11|2019-03-11 11:03:22|\n",
      "|2022-04-15|2022-04-15 10:11:21|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year,month,weekofyear,dayofmonth,dayofweek,dayofyear,hour,minute,second\n",
    "date_s=[(\"2021-01-14\",\"2021-01-14 10:00:21\"),\n",
    "(\"2018-02-12\",\"2018-02-12 09:10:23\"),\n",
    "(\"2019-03-11\",\"2019-03-11 11:03:22\"),\n",
    "(\"2022-04-15\",\"2022-04-15 10:11:21\")]\n",
    "data=spark.createDataFrame(date_s,schema=\"DATES STRING,TIMES STRING\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function year in module pyspark.sql.functions:\n",
      "\n",
      "year(col)\n",
      "    Extract the year of a given date as integer.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(year('dt').alias('year')).collect()\n",
      "    [Row(year=2015)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----+-----+----------+---------+---------+----+------+\n",
      "|     DATES|              TIMES|year|month|weekofyear|dayofyear|dayofweek|hour|minute|\n",
      "+----------+-------------------+----+-----+----------+---------+---------+----+------+\n",
      "|2021-01-14|2021-01-14 10:00:21|2021|    1|         2|       14|        5|  10|     0|\n",
      "|2018-02-12|2018-02-12 09:10:23|2018|    2|         7|       43|        2|   9|    10|\n",
      "|2019-03-11|2019-03-11 11:03:22|2019|    3|        11|       70|        2|  11|     3|\n",
      "|2022-04-15|2022-04-15 10:11:21|2022|    4|        15|      105|        6|  10|    11|\n",
      "+----------+-------------------+----+-----+----------+---------+---------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('year',year(col('TIMES'))).\\\n",
    "    withColumn('month',month(col('TIMES'))).\\\n",
    "    withColumn('weekofyear',weekofyear(col('TIMES'))).\\\n",
    "    withColumn('dayofyear',dayofyear(col('TIMES'))).\\\n",
    "    withColumn('dayofweek',dayofweek(col('TIMES'))).\\\n",
    "    withColumn('hour',hour(col('TIMES'))).\\\n",
    "    withColumn('minute',minute(col('TIMES'))).\\\n",
    "show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using to_date and to_timestamp on Spark Data Frames**\n",
    "* to_date(): Is used for converting a string or int or long value to date datatype after sharing what every positional letter means. It convert to yyyy-MM-dd.\n",
    "* to_timestamp(): Is used for converting a string or int or long value to timestamp datatype after sharing what every positional letter means. It convert to yyyy-MM-dd HH:mm:ss SSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     slash|  --format|\n",
      "+----------+----------+\n",
      "|2023-01-22|2023-01-11|\n",
      "|2023-01-22|2023-01-11|\n",
      "|2023-01-22|2023-01-11|\n",
      "|2023-01-22|2023-01-11|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(to_date(lit('2023/01/22'),'yyyy/MM/dd').alias('slash'),to_date(lit('2023--01--11 11:11:11'),'yyyy--MM--dd HH:mm:ss').alias('--format')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|       MMM|      MMMM|\n",
      "+----------+----------+\n",
      "|2023-05-01|2023-03-01|\n",
      "|2023-05-01|2023-03-01|\n",
      "|2023-05-01|2023-03-01|\n",
      "|2023-05-01|2023-03-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(to_date(lit('2023-May-01'),'yyyy-MMM-dd').alias('MMM'),to_date(lit('2023-March-01'),'yyyy-MMMM-dd').alias('MMMM')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|       MMM|               MMMM|\n",
      "+----------+-------------------+\n",
      "|2023-05-01|2023-03-01 07:23:11|\n",
      "|2023-05-01|2023-03-01 07:23:11|\n",
      "|2023-05-01|2023-03-01 07:23:11|\n",
      "|2023-05-01|2023-03-01 07:23:11|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(to_date(lit('May ,2023,01'),'MMMM ,yyyy,dd').alias('MMM'),to_timestamp(lit('2023-March-01 07:23:11'),'yyyy-MMMM-dd HH:mm:ss').alias('MMMM')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select(to_date(lit('2023-May-01'),'yyyy-MMM-dd').alias('MMM'),to_date(lit('2023-March-01'),'yyyy-MMMM-dd').alias('MMMM')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using date_format Function on Spark Data Frames**\n",
    "* When we have timestamp or date or string proper formated data and we want to convert into an other formate to make is useful.\n",
    "* It is converted to string datatype.\n",
    "* yyyy for year.\n",
    "* MM for month.\n",
    "* MMM for alphabetical 3 letter month.\n",
    "* MMMM for alphabetical full name of month.\n",
    "* dd for day.\n",
    "* DD for day in the year.\n",
    "* HH for hour 24 hour.\n",
    "* hh for hour 12 hour.\n",
    "* ss for second.\n",
    "* SSS for millisecond.\n",
    "* EE for getting week day short name.\n",
    "* EEEE for getting week day full name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|     DATES|              TIMES|\n",
      "+----------+-------------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|\n",
      "|2018-02-12|2018-02-12 09:10:23|\n",
      "|2019-03-11|2019-03-11 11:03:22|\n",
      "|2022-04-15|2022-04-15 10:11:21|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year,month,weekofyear,dayofmonth,dayofweek,dayofyear,hour,minute,second\n",
    "date_s=[(\"2021-01-14\",\"2021-01-14 10:00:21\"),\n",
    "(\"2018-02-12\",\"2018-02-12 09:10:23\"),\n",
    "(\"2019-03-11\",\"2019-03-11 11:03:22\"),\n",
    "(\"2022-04-15\",\"2022-04-15 10:11:21\")]\n",
    "data=spark.createDataFrame(date_s,schema=\"DATES STRING,TIMES STRING\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----------------+----------------+------------+--------+----+---+--------+\n",
      "|     DATES|              TIMES|           years|           slash|short_months|   month|hour|Day|Day_full|\n",
      "+----------+-------------------+----------------+----------------+------------+--------+----+---+--------+\n",
      "|2021-01-14|2021-01-14 10:00:21| 2021-January-14| 2021/January/14| 2021-Jan-14| January|  10|Thu|Thursday|\n",
      "|2018-02-12|2018-02-12 09:10:23|2018-February-12|2018/February/12| 2018-Feb-12|February|  09|Mon|  Monday|\n",
      "|2019-03-11|2019-03-11 11:03:22|   2019-March-11|   2019/March/11| 2019-Mar-11|   March|  11|Mon|  Monday|\n",
      "|2022-04-15|2022-04-15 10:11:21|   2022-April-15|   2022/April/15| 2022-Apr-15|   April|  10|Fri|  Friday|\n",
      "+----------+-------------------+----------------+----------------+------------+--------+----+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('years',date_format('DATES','yyyy-MMMM-dd')).\\\n",
    "    withColumn('slash',date_format('DATES','yyyy/MMMM/dd')).\\\n",
    "    withColumn('short_months',date_format('DATES','yyyy-MMM-dd')).\\\n",
    "    withColumn('month',date_format('DATES','MMMM')).\\\n",
    "    withColumn('hour',date_format('TIMES','hh')).\\\n",
    "    withColumn('Day',date_format('TIMES','EE')).\\\n",
    "    withColumn('Day_full',date_format('TIMES','EEEE')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATES: string (nullable = true)\n",
      " |-- TIMES: string (nullable = true)\n",
      " |-- years: string (nullable = true)\n",
      " |-- slash: string (nullable = true)\n",
      " |-- short_months: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('years',date_format('DATES','yyyy-MMMM-dd')).\\\n",
    "    withColumn('slash',date_format('DATES','yyyy/MMMM/dd')).\\\n",
    "    withColumn('short_months',date_format('DATES','yyyy-MMM-dd')).\\\n",
    "    withColumn('month',date_format('DATES','MMMM')).\\\n",
    "    withColumn('hour',date_format('TIMES','hh')).\\\n",
    "    printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dealing with Unix Timestamp in Spark Data Frames**\n",
    "* Unix timestamp is time that has started from 1970 and every second add up a value in it.\n",
    "* If you want to convert normal date and time stamp to unix timestamp than use function as **unix_timestamp(col(),'formate')**.\n",
    "* It is important to share the formate as we need to show th unix_timestamp function that want exactly is the date so it could convert it.\n",
    "* Now if you have unix timestamp value you can convert it into date and time stamp **from_unixtime(col(),'yyyyMMdd')**.\n",
    "* If we dont share the formate in the function then it will convert to timestamp standard as yyyy-MM-dd HH:mm:ss SSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24848f04b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|     DATES|              TIMES|\n",
      "+----------+-------------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|\n",
      "|2018-02-12|2018-02-12 09:10:23|\n",
      "|2019-03-11|2019-03-11 11:03:22|\n",
      "|2022-04-15|2022-04-15 10:11:21|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year,month,weekofyear,dayofmonth,dayofweek,dayofyear,hour,minute,second\n",
    "date_s=[(\"2021-01-14\",\"2021-01-14 10:00:21\"),\n",
    "(\"2018-02-12\",\"2018-02-12 09:10:23\"),\n",
    "(\"2019-03-11\",\"2019-03-11 11:03:22\"),\n",
    "(\"2022-04-15\",\"2022-04-15 10:11:21\")]\n",
    "data=spark.createDataFrame(date_s,schema=\"DATES STRING,TIMES STRING\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----------+----------+\n",
      "|     DATES|              TIMES| unix_date| unix_time|\n",
      "+----------+-------------------+----------+----------+\n",
      "|2021-01-14|2021-01-14 10:00:21|1610564400|1610600421|\n",
      "|2018-02-12|2018-02-12 09:10:23|1518375600|1518408623|\n",
      "|2019-03-11|2019-03-11 11:03:22|1552244400|1552284202|\n",
      "|2022-04-15|2022-04-15 10:11:21|1649962800|1649999481|\n",
      "+----------+-------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('unix_date',unix_timestamp('DATES','yyyy-MM-dd')).\\\n",
    "    withColumn('unix_time',unix_timestamp('TIMES','yyyy-MM-dd HH:mm:ss')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     unixs|\n",
      "+----------+\n",
      "|1678934400|\n",
      "|1681564800|\n",
      "|1684204800|\n",
      "|1686835200|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d=[('1678934400',),\n",
    "('1681564800',),\n",
    "('1684204800',),\n",
    "('1686835200',)]\n",
    "df=spark.createDataFrame(d,schema=\"unixs string\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+\n",
      "|     unixs|     dates|              times|\n",
      "+----------+----------+-------------------+\n",
      "|1678934400|2023-03-16|2023-03-16 07:40:00|\n",
      "|1681564800|2023-04-15|2023-04-15 18:20:00|\n",
      "|1684204800|2023-05-16|2023-05-16 07:40:00|\n",
      "|1686835200|2023-06-15|2023-06-15 18:20:00|\n",
      "+----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('dates',from_unixtime('unixs','yyyy-MM-dd')).withColumn('times',from_unixtime('unixs')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dealing with nulls in Spark Data Frames**\n",
    "* To deal with null we have **coalesce,fill,re**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = [\n",
    "    {\"string_column\": \"value1\", \"float_column\": 1.23, \"int_column\": 10, \"timestamp_column\": \"2023-01-15 00:00:00\"},\n",
    "    {\"string_column\": \"\", \"float_column\": None, \"int_column\": None, \"timestamp_column\": \"2023-02-28 00:00:00\"},\n",
    "    {\"string_column\": \"null\", \"float_column\": 4.56, \"int_column\": 20, \"timestamp_column\": None},\n",
    "    {\"string_column\": None, \"float_column\": 7.89, \"int_column\": 30, \"timestamp_column\": \"2023-03-10 00:00:00\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=spark.createDataFrame(da)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- float_column: double (nullable = true)\n",
      " |-- int_column: long (nullable = true)\n",
      " |-- string_column: string (nullable = true)\n",
      " |-- timestamp_column: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+-------------+-----------+--------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|float_columns|int_columns|string_columns|  timestamp_columns|\n",
      "+------------+----------+-------------+-------------------+-------------+-----------+--------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|         1.23|         10|        value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|            0|          0|              |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|         4.56|         20|          null|                  0|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|         7.89|         30|             0|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+-------------+-----------+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('float_columns',coalesce('float_column',lit('0'))).\\\n",
    "    withColumn('int_columns',coalesce('int_column',lit('0'))).\\\n",
    "    withColumn('string_columns',coalesce('string_column',lit('0'))).\\\n",
    "    withColumn('timestamp_columns',coalesce('timestamp_column',lit('0'))).\\\n",
    "    show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we want to replace 'null' by 0 then we use nullif to make it to null and then after that using nvl it will convert null value to 0.\n",
    "* If we have '' then we can do it by using nested nullif function over nullif function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+-------+\n",
      "|float_column|int_column|string_column|   timestamp_column|strings|\n",
      "+------------+----------+-------------+-------------------+-------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00| value1|\n",
      "|        null|      null|             |2023-02-28 00:00:00|       |\n",
      "|        4.56|        20|         null|               null|      0|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|      0|\n",
      "+------------+----------+-------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('strings',expr(\"nvl(nullif(string_column,'null'),0)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+-------+\n",
      "|float_column|int_column|string_column|   timestamp_column|strings|\n",
      "+------------+----------+-------------+-------------------+-------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00| value1|\n",
      "|        null|      null|             |2023-02-28 00:00:00|      0|\n",
      "|        4.56|        20|         null|               null|      0|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|      0|\n",
      "+------------+----------+-------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('strings',expr(\"nvl(nullif(nullif(string_column,'null'),''),0)\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To deal with **NA,EMPTY values ,and null values** first convert it to int and you will find out that it will convert to null (null is not a value its a property) and then you can use \\\n",
    "    coalesce on top of that result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+-----+----+-----+--------------+---------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|EMPTY| NAs|nulls|coalesce_empty|string_columnss|\n",
      "+------------+----------+-------------+-------------------+-----+----+-----+--------------+---------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00| null|null| null|             1|              1|\n",
      "|        null|      null|             |2023-02-28 00:00:00| null|null| null|             1|              1|\n",
      "|        4.56|        20|       value3|               null| null|null| null|             1|              1|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00| null|null| null|             1|              1|\n",
      "+------------+----------+-------------+-------------------+-----+----+-----+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('EMPTY',lit('').cast('int')).\\\n",
    "    withColumn('NAs',lit('NA').cast('int')).\\\n",
    "    withColumn('nulls',lit('null').cast('int')).\\\n",
    "    withColumn('coalesce_empty',coalesce(lit('').cast('int'),lit(1))).\\\n",
    "    withColumn('string_columnss',coalesce(col('string_column').cast('int'),lit(1))).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----------+------------------+\n",
      "|float_column|nvl(float_column, 0)|int_column|nvl(int_column, 1)|\n",
      "+------------+--------------------+----------+------------------+\n",
      "|        1.23|                1.23|        10|                10|\n",
      "|        null|                 0.0|      null|                 1|\n",
      "|        4.56|                4.56|        20|                20|\n",
      "|        7.89|                7.89|        30|                30|\n",
      "+------------+--------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.selectExpr('float_column',\"nvl(float_column,0)\",'int_column',\"nvl(int_column,1)\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here below the nullif will check is float_column value is equal to '' if yes then it will replace it will null property and then nvl will replace it by default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+----------+---+\n",
      "|float_column| abc|int_column|bcs|\n",
      "+------------+----+----------+---+\n",
      "|        1.23|1.23|        10| 10|\n",
      "|        null| 0.0|      null|  1|\n",
      "|        4.56|4.56|        20| 20|\n",
      "|        7.89|7.89|        30| 30|\n",
      "+------------+----+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.selectExpr('float_column',\"nvl(nullif(float_column,''),0) as abc\",'int_column',\"nvl(nullif(int_column,''),1) as bcs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|             total|\n",
      "+------------+----------+-------------+-------------------+------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|0.6150000095367432|\n",
      "|        null|      null|             |2023-02-28 00:00:00|               0.0|\n",
      "|        4.56|        20|         null|               null|2.2799999713897705|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00| 3.944999933242798|\n",
      "+------------+----------+-------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('total',coalesce(col('float_column').cast('float'),lit(0))/lit(2)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* df.na and df.na.fill and fillna are same.\n",
    "* df.na.drop and df.dropna() as same.\n",
    "* df.na.replace and df.replacena() are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b2580a4b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = [\n",
    "    {\"string_column\": \"value1\", \"float_column\": 1.23, \"int_column\": 10, \"timestamp_column\": \"2023-01-15 00:00:00\"},\n",
    "    {\"string_column\": \"\", \"float_column\": None, \"int_column\": None, \"timestamp_column\": \"2023-02-28 00:00:00\"},\n",
    "    {\"string_column\": \"null\", \"float_column\": 4.56, \"int_column\": 20, \"timestamp_column\": None},\n",
    "    {\"string_column\": None, \"float_column\": 7.89, \"int_column\": 30, \"timestamp_column\": \"2023-03-10 00:00:00\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=spark.createDataFrame(da)\n",
    "data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If I want to fill string_column column with '0' we can specify that as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|            0|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.fillna('0','string_column').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you want to work on whole dataframe you can do it by just using df.fillna('0') as 0 is string type so it will replace the columns data that are string columns.\n",
    "* If we use df.fillna(0) it will replace all int and long columns.\n",
    "* you will see null in string even after using fillna function reason is that null in this case is string not a property that is why we spark cant identify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|         0.0|         0|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.fillna(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|                  0|\n",
      "|        7.89|        30|            0|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.fillna('0').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What if we have list of columns here is the solution for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|         0.0|         0|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.fillna(0,['float_column','int_column']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameNaFunctions in module pyspark.sql.dataframe object:\n",
      "\n",
      "class DataFrameNaFunctions(builtins.object)\n",
      " |  DataFrameNaFunctions(df)\n",
      " |  \n",
      " |  Functionality for working with missing data in :class:`DataFrame`.\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, df)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  drop(self, how='any', thresh=None, subset=None)\n",
      " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      how : str, optional\n",
      " |          'any' or 'all'.\n",
      " |          If 'any', drop a row if it contains any nulls.\n",
      " |          If 'all', drop a row only if all its values are null.\n",
      " |      thresh: int, optional\n",
      " |          default None\n",
      " |          If specified, drop rows that have less than `thresh` non-null values.\n",
      " |          This overwrites the `how` parameter.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.drop().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |  \n",
      " |  fill(self, value, subset=None)\n",
      " |      Replace null values, alias for ``na.fill()``.\n",
      " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : int, float, string, bool or dict\n",
      " |          Value to replace null values with.\n",
      " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      " |          from column name (string) to replacement value. The replacement value must be\n",
      " |          an int, float, boolean, or string.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.fill(50).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      |  5|    50|  Bob|\n",
      " |      | 50|    50|  Tom|\n",
      " |      | 50|    50| null|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df5.na.fill(False).show()\n",
      " |      +----+-------+-----+\n",
      " |      | age|   name|  spy|\n",
      " |      +----+-------+-----+\n",
      " |      |  10|  Alice|false|\n",
      " |      |   5|    Bob|false|\n",
      " |      |null|Mallory| true|\n",
      " |      +----+-------+-----+\n",
      " |      \n",
      " |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      " |      +---+------+-------+\n",
      " |      |age|height|   name|\n",
      " |      +---+------+-------+\n",
      " |      | 10|    80|  Alice|\n",
      " |      |  5|  null|    Bob|\n",
      " |      | 50|  null|    Tom|\n",
      " |      | 50|  null|unknown|\n",
      " |      +---+------+-------+\n",
      " |  \n",
      " |  replace(self, to_replace, value=<no value>, subset=None)\n",
      " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      " |      aliases of each other.\n",
      " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      " |      or strings. Value can have None. When replacing, the new value will be cast\n",
      " |      to the type of the existing column.\n",
      " |      For numeric replacements all values to be replaced should have unique\n",
      " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      " |      and arbitrary replacement will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      to_replace : bool, int, float, string, list or dict\n",
      " |          Value to be replaced.\n",
      " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      " |          must be a mapping between a value and a replacement.\n",
      " |      value : bool, int, float, string or None, optional\n",
      " |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      " |          list, `value` should be of the same length and type as `to_replace`.\n",
      " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      " |          used as a replacement for each item in `to_replace`.\n",
      " |      subset : list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.replace(10, 20).show()\n",
      " |      +----+------+-----+\n",
      " |      | age|height| name|\n",
      " |      +----+------+-----+\n",
      " |      |  20|    80|Alice|\n",
      " |      |   5|  null|  Bob|\n",
      " |      |null|  null|  Tom|\n",
      " |      |null|  null| null|\n",
      " |      +----+------+-----+\n",
      " |      \n",
      " |      >>> df4.na.replace('Alice', None).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace({'Alice': None}).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|   A|\n",
      " |      |   5|  null|   B|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(data.na)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using CASE and WHEN on Spark Data Frames**\n",
    "* If there is null values we can use coalesce now we can use case statement as well.\n",
    "* if we use dataframe expression method we use CASE WHEN THEN ELSE END.\n",
    "* if we use dataframe method we will use when (condition,value_to_fill_if_satified) ,otherwise(elsevalue)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe using list called as persons and categorize them based up on following rules.\n",
    "* 0 to 2 Months than New Born\n",
    "* 2+ Months to 12 Months than Infant\n",
    "* 12+ Months to 48 Months than Toddler\n",
    "* 48+ Months to 144 Months than Kids\n",
    "* 144+ Months than Teenager or Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2| 13|\n",
      "|  3| 18|\n",
      "|  4| 60|\n",
      "|  5|120|\n",
      "|  6|  0|\n",
      "|  7| 12|\n",
      "|  8|160|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons = [\n",
    "    (1, 1),\n",
    "    (2, 13),\n",
    "    (3, 18),\n",
    "    (4, 60),\n",
    "    (5, 120),\n",
    "    (6, 0),\n",
    "    (7, 12),\n",
    "    (8, 160)\n",
    "]\n",
    "personsDF = spark.createDataFrame(persons, schema='id INT, age INT')\n",
    "personsDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+\n",
      "| id|age|categorys|\n",
      "+---+---+---------+\n",
      "|  1|  1| New Born|\n",
      "|  2| 13|  Toddler|\n",
      "|  3| 18|  Toddler|\n",
      "|  4| 60|     Kids|\n",
      "|  5|120|     Kids|\n",
      "|  6|  0| New Born|\n",
      "|  7| 12|   Infant|\n",
      "|  8|160|    ADULT|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.withColumn('categorys',expr(\"\"\"\n",
    "                                     CASE when age >= 0 AND age <= 2 then 'New Born' \n",
    "                                     when age between 2 and 12 then 'Infant'\n",
    "                                     when age between 12 and 48 then 'Toddler'\n",
    "                                     when age between 48 and 144 then 'Kids'\n",
    "                                     when age >144 then 'ADULT' ELSE 'None'                           \n",
    "                                     END\n",
    "                                     \"\"\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+\n",
      "| id|age|category|\n",
      "+---+---+--------+\n",
      "|  1|  1|New Born|\n",
      "|  2| 13| Toddler|\n",
      "|  3| 18| Toddler|\n",
      "|  4| 60|    Kids|\n",
      "|  5|120|    Kids|\n",
      "|  6|  0|New Born|\n",
      "|  7| 12|  Infant|\n",
      "|  8|160|   ADULT|\n",
      "+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.withColumn('category',when(col('age').between(0,2),'New Born').\\\n",
    "                                when((col('age')>=2) & (col('age')<=12),'Infant').\\\n",
    "                                when((col('age')>=12) & (col('age')<=48),'Toddler').\\\n",
    "                                when((col('age')>=48) & (col('age')<=144),'Kids').\\\n",
    "                                when((col('age')>144),'ADULT').\\\n",
    "                                    otherwise('None')).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+\n",
      "| id|age|categorys|\n",
      "+---+---+---------+\n",
      "|  1|  1| New Born|\n",
      "|  2| 13|  Toddler|\n",
      "|  3| 18|  Toddler|\n",
      "|  4| 60|     Kids|\n",
      "|  5|120|     Kids|\n",
      "|  6|  0| New Born|\n",
      "|  7| 12|   Infant|\n",
      "|  8|160|    ADULT|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.withColumn('categorys', expr(\"\"\"\n",
    "                                      CASE when age >= 0 AND age <= 2 then 'New Born' \n",
    "                                      when age between 2 and 12 then 'Infant'\n",
    "                                      when age between 12 and 48 then 'Toddler'\n",
    "                                      when age between 48 and 144 then 'Kids'\n",
    "                                      when age > 144 then 'ADULT' ELSE 'None'                           \n",
    "                                      END\n",
    "                                      \"\"\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of Filter or Where Function on Spark Data Frame**\n",
    "* When we have Dataframe we can user where as well as filter to filter the values based upon the conditions.\n",
    "* While using Dataframe we can use filter in both python style syntax as df.filter (df['column']>1).show() or df.filter(col('column)).show().\n",
    "* We can use SQL type conditions as well as df.filter (\"column > 1\").show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, BooleanType\n",
    "\n",
    "sc=StructType([StructField(\"id\",IntegerType()),\n",
    "               StructField(\"age\",IntegerType()),\n",
    "               StructField(\"status\",BooleanType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "| id|age|status|\n",
      "+---+---+------+\n",
      "|  1|  1|  true|\n",
      "|  2| 13| false|\n",
      "|  3| 18| false|\n",
      "|  4| 60|  true|\n",
      "|  5|120|  true|\n",
      "|  6|  0| false|\n",
      "|  7| 12|  null|\n",
      "|  8|160|  null|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons = [\n",
    "    (1, 1,True),\n",
    "    (2, 13,False),\n",
    "    (3, 18,False),\n",
    "    (4, 60,True),\n",
    "    (5, 120,True),\n",
    "    (6, 0,False),\n",
    "    (7, 12,),\n",
    "    (8, 160,)\n",
    "]\n",
    "df=pd.DataFrame(persons,columns=['id','age','status'])\n",
    "pf=spark.createDataFrame(df)\n",
    "pf.show()\n",
    "# personsDF = spark.createDataFrame(persons, schema=sc)\n",
    "# personsDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "| id|age|status|\n",
      "+---+---+------+\n",
      "|  3| 18| false|\n",
      "|  4| 60|  true|\n",
      "|  5|120|  true|\n",
      "|  8|160| false|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.filter(personsDF['age']>13).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  3| 18|\n",
      "|  4| 60|\n",
      "|  5|120|\n",
      "|  8|160|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.where(col('age')>13).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  4| 60|\n",
      "|  5|120|\n",
      "|  8|160|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.filter(\"age>13\").where (\"age>18\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of Conditions and Operators related to Spark Data Frames**\n",
    "* Now here we have conditional logic that we can used in filter and where function to filter our data.\n",
    "* Equal :: == or =\n",
    "* Not Equal :: !=\n",
    "* Great than :: >\n",
    "* Less than :: <\n",
    "* Great than or Equal to :: >=\n",
    "* Less than or Equal to :: <=\n",
    "* In operator :: isin or IN\n",
    "* Between Operator :: between function or BETWEEN WITH AND\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2| 13|\n",
      "|  3| 18|\n",
      "|  4| 60|\n",
      "|  5|120|\n",
      "|  6|  0|\n",
      "|  7| 12|\n",
      "|  8|160|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.filter(personsDF['age']==1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.filter('age = 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.where('age = 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "| id|age|status|\n",
      "+---+---+------+\n",
      "|  1|  1|  true|\n",
      "|  4| 60|  true|\n",
      "|  5|120|  true|\n",
      "|  7| 12|  true|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.where('status = true').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "| id|age|status|\n",
      "+---+---+------+\n",
      "|  1|  1|  true|\n",
      "|  4| 60|  true|\n",
      "|  5|120|  true|\n",
      "|  7| 12|  true|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.where(expr('status = true')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "| id|age|status|\n",
      "+---+---+------+\n",
      "|  1|  1|  true|\n",
      "|  2| 13| false|\n",
      "|  3| 18| false|\n",
      "|  4| 60|  true|\n",
      "|  5|120|  true|\n",
      "|  6|  0| false|\n",
      "|  7| 12|  null|\n",
      "|  8|160|  null|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "| id|age|status|\n",
      "+---+---+------+\n",
      "|  1|  1|  true|\n",
      "|  2| 13| false|\n",
      "|  3| 18| false|\n",
      "|  4| 60|  true|\n",
      "|  5|120|  true|\n",
      "|  6|  0| false|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.filter(\"isnull(status)==False\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get all the ids that have status as true. Here you will see that value that are null are ignored. we can bring that back as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|status|\n",
      "+---+------+\n",
      "|  1|  true|\n",
      "|  4|  true|\n",
      "|  5|  true|\n",
      "|  7|  null|\n",
      "|  8|  null|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select('id','status').filter(\"status != False OR status is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|status|\n",
      "+---+------+\n",
      "|  2| false|\n",
      "|  3| false|\n",
      "|  6| false|\n",
      "|  7|  null|\n",
      "|  8|  null|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select('id', 'status').filter((col('status') != True) | (col('status').isNull())).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Filter using Between Operator on Spark Data Frames**\n",
    "* If we want to use non SQL type syntax then name the dataframe and call the column name and use .between(a,b).\n",
    "* If we want to use SQL type syntax we will use \"between a AND b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "da = [\n",
    "    {\"string_column\": \"value1\", \"float_column\": 1.23, \"int_column\": 10, \"timestamp_column\": \"2023-01-15 00:00:00\"},\n",
    "    {\"string_column\": \"\", \"float_column\": None, \"int_column\": None, \"timestamp_column\": \"2023-02-28 00:00:00\"},\n",
    "    {\"string_column\": \"null\", \"float_column\": 4.56, \"int_column\": 20, \"timestamp_column\": None},\n",
    "    {\"string_column\": None, \"float_column\": 7.89, \"int_column\": 30, \"timestamp_column\": \"2023-03-10 00:00:00\"}\n",
    "]\n",
    "\n",
    "data=spark.createDataFrame(da)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|string_column|   timestamp_column|\n",
      "+-------------+-------------------+\n",
      "|       value1|2023-01-15 00:00:00|\n",
      "|             |2023-02-28 00:00:00|\n",
      "+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('string_column','timestamp_column').filter(data.timestamp_column.between('2023-01-01','2023-02-28 01:00:11')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|string_column|   timestamp_column|\n",
      "+-------------+-------------------+\n",
      "|       value1|2023-01-15 00:00:00|\n",
      "|             |2023-02-28 00:00:00|\n",
      "+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('string_column','timestamp_column').filter(data['timestamp_column'].between('2023-01-01','2023-02-28 01:00:11')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL Style Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|string_column|   timestamp_column|\n",
      "+-------------+-------------------+\n",
      "|       value1|2023-01-15 00:00:00|\n",
      "|             |2023-02-28 00:00:00|\n",
      "+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('string_column','timestamp_column').filter(\"timestamp_column BETWEEN '2023-01-01' AND '2023-02-29'\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dealing with Null Values while Filtering Data in Spark Data Frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x266e3204b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "da = [\n",
    "    {\"string_column\": \"value1\", \"float_column\": 1.23, \"int_column\": 10, \"timestamp_column\": \"2023-01-15 00:00:00\"},\n",
    "    {\"string_column\": \"\", \"float_column\": None, \"int_column\": None, \"timestamp_column\": \"2023-02-28 00:00:00\"},\n",
    "    {\"string_column\": \"null\", \"float_column\": 4.56, \"int_column\": 20, \"timestamp_column\": None},\n",
    "    {\"string_column\": None, \"float_column\": 7.89, \"int_column\": 30, \"timestamp_column\": \"2023-03-10 00:00:00\"}\n",
    "]\n",
    "\n",
    "data=spark.createDataFrame(da)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|float_column|string_column|\n",
      "+------------+-------------+\n",
      "|        1.23|       value1|\n",
      "|        null|             |\n",
      "|        4.56|         null|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('float_column','string_column').filter(col('string_column').isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|float_column|string_column|\n",
      "+------------+-------------+\n",
      "|        7.89|         null|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('float_column','string_column').filter(col('string_column').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|float_column|string_column|\n",
      "+------------+-------------+\n",
      "|        1.23|       value1|\n",
      "|        null|             |\n",
      "|        4.56|         null|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('float_column','string_column').filter(expr('string_column is not null')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show the ids that have vehical null and empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  1|  1|  true|    car|\n",
      "|  2| 13| false|   buss|\n",
      "|  3| 18| false|   bike|\n",
      "|  4| 60|  true|   null|\n",
      "|  5|120|  true|   null|\n",
      "|  6|  0| false|       |\n",
      "|  7| 12|  null|  cycle|\n",
      "|  8|160|  null|       |\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons = [\n",
    "    (1, 1,True,'car'),\n",
    "    (2, 13,False,'buss'),\n",
    "    (3, 18,False,'bike'),\n",
    "    (4, 60,True,'null'),\n",
    "    (5, 120,True,None),\n",
    "    (6, 0,False,''),\n",
    "    (7, 12,None,'cycle'),\n",
    "    (8, 160,None, '')\n",
    "]\n",
    "df=pd.DataFrame(persons,columns=['id','age','status','vehical'])\n",
    "pf=spark.createDataFrame(df)\n",
    "pf.show()\n",
    "# personsDF = spark.createDataFrame(persons, schema=sc)\n",
    "# personsDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  5|120|  true|   null|\n",
      "|  6|  0| false|       |\n",
      "|  8|160|  null|       |\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select ('*').filter((col('vehical').isNull()) | (col('vehical') == '')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL Type syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  5|120|  true|   null|\n",
      "|  6|  0| false|       |\n",
      "|  8|160|  null|       |\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select('*').filter (\"vehical is null or vehical == ''\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  5|120|  true|   null|\n",
      "|  6|  0| false|       |\n",
      "|  8|160|  null|       |\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select('*').filter (\"vehical is null or vehical == ''\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  1|  1|  true|    car|\n",
      "|  2| 13| false|   buss|\n",
      "|  3| 18| false|   bike|\n",
      "|  4| 60|  true|   null|\n",
      "|  5|120|  true|   null|\n",
      "|  6|  0| false|       |\n",
      "|  7| 12|  null|  cycle|\n",
      "|  8|160|  null|       |\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now show me records that have vehical as car bike and null.\n",
    "* Here you need to use isnull function separately as Null is not a value it is a property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  1|  1|  true|    car|\n",
      "|  3| 18| false|   bike|\n",
      "|  5|120|  true|   null|\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select('*').filter (col('vehical').isin('car','bike')|col('vehical').isNull()).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL style syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  1|  1|  true|    car|\n",
      "|  3| 18| false|   bike|\n",
      "|  5|120|  true|   null|\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select('*').filter(\"vehical IN ('car','bike') or vehical is null\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.filter((col('vehical') == 'car') & (col('status').isNull())).select('*').show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL Style Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|status|\n",
      "+---+------+\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.filter(\"vehical = 'car' and status is null\").select('id','status').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Spark Data Frame for Dropping Columns**\n",
    "* Lets now come up with some smart work.\n",
    "* Create a file sources.ipynb and then place the script of creating session and dataframes and also print out what is created over there.\n",
    "* We can use col('column name') and df['column name'] and string of column names to drop a columns.\n",
    "* The column you want to drop if that does not exist then it will not throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method drop in module pyspark.sql.dataframe:\n",
      "\n",
      "drop(*cols) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` that drops the specified column.\n",
      "    This is a no-op if schema doesn't contain the given column name(s).\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    cols: str or :class:`Column`\n",
      "        a name of the column, or the :class:`Column` to drop\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.drop('age').collect()\n",
      "    [Row(name='Alice'), Row(name='Bob')]\n",
      "    \n",
      "    >>> df.drop(df.age).collect()\n",
      "    [Row(name='Alice'), Row(name='Bob')]\n",
      "    \n",
      "    >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      "    [Row(age=5, height=85, name='Bob')]\n",
      "    \n",
      "    >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      "    [Row(age=5, name='Bob', height=85)]\n",
      "    \n",
      "    >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      "    [Row(name='Bob')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sm.drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm.drop('nationality','bonus').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----+----------------+-----------+\n",
      "|employee_id|salary|bonus|    phone_number|        ssn|\n",
      "+-----------+------+-----+----------------+-----------+\n",
      "|          1|1000.0|   10| +1 123 456 7890|123 45 6789|\n",
      "|          2|1250.0| null|+91 234 567 8901|456 78 9123|\n",
      "|          3| 750.0|     |+44 111 111 1111|222 33 4444|\n",
      "|          4|1500.0|   10|+61 987 654 3210|789 12 6118|\n",
      "+-----------+------+-----+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm.drop('nationality','first_name','last_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----+----------------+-----------+\n",
      "|employee_id|salary|bonus|    phone_number|        ssn|\n",
      "+-----------+------+-----+----------------+-----------+\n",
      "|          1|1000.0|   10| +1 123 456 7890|123 45 6789|\n",
      "|          2|1250.0| null|+91 234 567 8901|456 78 9123|\n",
      "|          3| 750.0|     |+44 111 111 1111|222 33 4444|\n",
      "|          4|1500.0|   10|+61 987 654 3210|789 12 6118|\n",
      "+-----------+------+-----+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm.drop(*['nationality','first_name','last_name']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dropping Duplicate Records from Spark Data Frames**\n",
    "* Distinct , dropDuplicates are doing same job.\n",
    "* If you pass a column name through dropDuplicates(['id']) it will start droping bases of this column.\n",
    "* We can pass list of columns to see for duplicated on the bases of those columns and then remove the duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method distinct in module pyspark.sql.dataframe:\n",
      "\n",
      "distinct() method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.distinct().count()\n",
      "    2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sm.distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dropDuplicates in module pyspark.sql.dataframe:\n",
      "\n",
      "dropDuplicates(subset=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "    optionally only considering certain columns.\n",
      "    \n",
      "    For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      "    :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      "    duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      "    be and system will accordingly limit the state. In addition, too late data older than\n",
      "    watermark will be dropped to avoid any possibility of duplicates.\n",
      "    \n",
      "    :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> df = sc.parallelize([ \\\n",
      "    ...     Row(name='Alice', age=5, height=80), \\\n",
      "    ...     Row(name='Alice', age=5, height=80), \\\n",
      "    ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      "    >>> df.dropDuplicates().show()\n",
      "    +-----+---+------+\n",
      "    | name|age|height|\n",
      "    +-----+---+------+\n",
      "    |Alice|  5|    80|\n",
      "    |Alice| 10|    80|\n",
      "    +-----+---+------+\n",
      "    \n",
      "    >>> df.dropDuplicates(['name', 'height']).show()\n",
      "    +-----+---+------+\n",
      "    | name|age|height|\n",
      "    +-----+---+------+\n",
      "    |Alice|  5|    80|\n",
      "    +-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sm.dropDuplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %run ./sources.ipynb\n",
    "%run ./sources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 34},\n",
    "    {\"ID\": 2, \"Name\": \"Bob\", \"Age\": 30},\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 25},  # Duplicate row\n",
    "    {\"ID\": 3, \"Name\": \"Charlie\", \"Age\": 35},\n",
    "    {\"ID\": 2, \"Name\": \"Bob\", \"Age\": 30},  # Duplicate row and ID column\n",
    "    {\"ID\": 4, \"Name\": \"David\", \"Age\": 40},\n",
    "    {\"ID\": 5, \"Name\": \"Eve\", \"Age\": 28},\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 25}, \n",
    "    {\"ID\": 6, \"Name\": \"Alice\", \"Age\": 34}# Duplicate row\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| ID|   Name|Age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 34|\n",
      "|  2|    Bob| 30|\n",
      "|  1|  Alice| 25|\n",
      "|  3|Charlie| 35|\n",
      "|  2|    Bob| 30|\n",
      "|  4|  David| 40|\n",
      "|  5|    Eve| 28|\n",
      "|  1|  Alice| 25|\n",
      "|  6|  Alice| 34|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame(data)\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# Show the Spark DataFrame\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| ID|   Name|Age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 34|\n",
      "|  2|    Bob| 30|\n",
      "|  1|  Alice| 25|\n",
      "|  3|Charlie| 35|\n",
      "|  4|  David| 40|\n",
      "|  5|    Eve| 28|\n",
      "|  6|  Alice| 34|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.distinct().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets drop the rows based on the duplication of the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| ID|   Name|Age|\n",
      "+---+-------+---+\n",
      "|  5|    Eve| 28|\n",
      "|  4|  David| 40|\n",
      "|  1|  Alice| 34|\n",
      "|  1|  Alice| 25|\n",
      "|  3|Charlie| 35|\n",
      "|  2|    Bob| 30|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.distinct().dropDuplicates(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'nationality'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm['nationality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[first_name: string, last_name: string]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm['first_name','last_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm['first_name','last_name'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------------+-------------+\n",
      "| ID|   Name|Age|col_trimright|        Names|\n",
      "+---+-------+---+-------------+-------------+\n",
      "|  1|  Alice| 34|      ++92321|Companies....|\n",
      "|  2|    Bob| 30|      ++92321|Companies....|\n",
      "|  1|  Alice| 25|      ++92321|Companies....|\n",
      "|  3|Charlie| 35|      ++92321|Companies....|\n",
      "|  2|    Bob| 30|      ++92321|Companies....|\n",
      "|  4|  David| 40|      ++92321|Companies....|\n",
      "|  5|    Eve| 28|      ++92321|Companies....|\n",
      "|  1|  Alice| 25|      ++92321|Companies....|\n",
      "|  6|  Alice| 34|      ++92321|Companies....|\n",
      "+---+-------+---+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs=spark_df.withColumn('col_trimright',lit(\"++92321\")).withColumn('Names',lit('Companies....'))\n",
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+\n",
      "| ID|phone|companies|\n",
      "+---+-----+---------+\n",
      "|  1|92321|Companies|\n",
      "|  2|92321|Companies|\n",
      "|  1|92321|Companies|\n",
      "|  3|92321|Companies|\n",
      "|  2|92321|Companies|\n",
      "|  4|92321|Companies|\n",
      "|  5|92321|Companies|\n",
      "|  1|92321|Companies|\n",
      "|  6|92321|Companies|\n",
      "+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.select ('ID',expr(\"trim(LEADING '+' From col_trimright) as phone\"),expr(\"trim(TRAILING '.' From Names) as companies\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dropping Null based Records from Spark Data Frames**\n",
    "* using dropna() will by default search for NULL columns and then if it finds a row with a single null value it will drop the complete row.\n",
    "* using dropna(how='all') will drop a rows that have full row as null.\n",
    "* using dropna(how='any') will drop a row that have any of the value of the column as null.\n",
    "* using dropna(subset=['c1',c2]) will find out any value is null it will drop all the row.\n",
    "* using dropna(thres=3) will search for columns that have non null values if there are less then 3 it will drop that if its more than then it will not.\n",
    "* using dropna(how='all',subset=['c1','c2']) it will look for all values of these columns if find null then drop the columns.\n",
    "* using dropna(how='any',subset=['c1','c2']) it will look for any value if its null it will drop the full row.\n",
    "* When we use dropna(how='any',thres=3,subset=['Name','Age']) the effect of how='any' get disappeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 34,\"class\":\"mid\"},\n",
    "    {\"ID\": 2, \"Name\": None, \"Age\": 30,\"class\":\"mid\"},\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 25,\"class\":\"high\"},  # Duplicate row\n",
    "    {\"ID\": 2, \"Name\": None, \"Age\": None,\"class\":\"Low\"},\n",
    "    {\"ID\": 2, \"Name\": \"Bob\", \"Age\": 30,\"class\":\"Low\"},  # Duplicate row and ID column\n",
    "    {\"ID\":3, \"Name\": None, \"Age\": None,\"class\":None},\n",
    "    {\"ID\": 5, \"Name\": \"Eve\", \"Age\": 28,\"class\":\"mid\"},\n",
    "    {\"ID\": 1, \"Name\": \"\", \"Age\": 25,\"class\":\"Low\"}, \n",
    "    {\"ID\": 6, \"Name\": None, \"Age\": 34,\"class\":\"Low\"}# Duplicate row\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:: 9\n",
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  2| null|30.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=spark.createDataFrame(pd.DataFrame(datas))\n",
    "# data.show()\n",
    "print(f'count:: {data.count()}')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.dropna(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(how='all',subset=[ 'Name','class']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(how='any',subset=['Name','class']).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we are just looking at three columns so if we get null in any of these column the whole row will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.dropna(how='any',thresh=3,subset=['Name','class','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "| id|visit_date|people|\n",
      "+---+----------+------+\n",
      "|  1|2017-02-01|    10|\n",
      "|  2|2017-01-02|   109|\n",
      "|  3|2017-01-03|   150|\n",
      "|  4|2017-01-04|    99|\n",
      "|  5|2017-01-05|   145|\n",
      "|  6|2017-01-06|  1455|\n",
      "|  7|2017-01-07|   199|\n",
      "|  8|2017-01-09|   188|\n",
      "+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"id\", IntegerType(), True),\n",
    "#     StructField(\"visit_date\", DateType(), True),\n",
    "#     StructField(\"people\", IntegerType(), True)\n",
    "# ])\n",
    "\n",
    "# Define the data\n",
    "data = [\n",
    "    (1, \"2017-02-01\", 10),\n",
    "    (2, \"2017-01-02\", 109),\n",
    "    (3, \"2017-01-03\", 150),\n",
    "    (4, \"2017-01-04\", 99),\n",
    "    (5, \"2017-01-05\", 145),\n",
    "    (6, \"2017-01-06\", 1455),\n",
    "    (7, \"2017-01-07\", 199),\n",
    "    (8, \"2017-01-09\", 188)\n",
    "]\n",
    "pp=pd.DataFrame(data,columns=['id','visit_date','people'])\n",
    "# Create the DataFrame and convert the date strings to DateType\n",
    "df = spark.createDataFrame(pp)\n",
    "df = df.withColumn(\"visit_date\", to_date(df[\"visit_date\"], \"yyyy-MM-dd\"))\n",
    "# pp\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+----------+\n",
      "| id|visit_date|people|difference|\n",
      "+---+----------+------+----------+\n",
      "|  2|2017-01-02|   109|         8|\n",
      "|  3|2017-01-03|   150|         7|\n",
      "|  5|2017-01-05|   145|         5|\n",
      "|  6|2017-01-06|  1455|         4|\n",
      "|  7|2017-01-07|   199|         3|\n",
      "|  8|2017-01-09|   188|         2|\n",
      "+---+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('people > 100').withColumn('difference',expr(\"10-id\")).filter(expr'difference+id').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of Sorting a Spark Data Frame**\n",
    "* sort() we can pass string ,col type and list of columns.\n",
    "* sort() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sort the data on bases of ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 34,\"class\":\"mid\"},\n",
    "    {\"ID\": 2, \"Name\": None, \"Age\": 30,\"class\":\"mid\"},\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 25,\"class\":\"high\"},  # Duplicate row\n",
    "    {\"ID\": 2, \"Name\": None, \"Age\": None,\"class\":\"Low\"},\n",
    "    {\"ID\": 2, \"Name\": \"Bob\", \"Age\": 30,\"class\":\"Low\"},  # Duplicate row and ID column\n",
    "    {\"ID\":3, \"Name\": None, \"Age\": None,\"class\":None},\n",
    "    {\"ID\": 5, \"Name\": \"Eve\", \"Age\": 28,\"class\":\"mid\"},\n",
    "    {\"ID\": 1, \"Name\": \"\", \"Age\": 25,\"class\":\"Low\"}, \n",
    "    {\"ID\": 6, \"Name\": None, \"Age\": 34,\"class\":\"Low\"}# Duplicate row\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:: 9\n",
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  2| null|30.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=spark.createDataFrame(pd.DataFrame(datas))\n",
    "# data.show()\n",
    "print(f'count:: {data.count()}')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|     |25.0|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2| null|30.0|  mid|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort('ID').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|     |25.0|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null|30.0|  mid|\n",
      "|  2| null| NaN|  Low|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(data['ID']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|     |25.0|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null|30.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(data.ID).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|Alice|25.0| high|\n",
      "|  1|     |25.0|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null|30.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(col('ID')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sorting by DESC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n",
      "data count:: 9\n",
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  6| null|34.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2| null|30.0|  mid|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "|  1|Alice|25.0| high|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort('ID',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  6| null|34.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2| null|30.0|  mid|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  1|     |25.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(data['ID'],ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  6| null|34.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  2| null|30.0|  mid|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null| NaN|  Low|\n",
      "|  1|Alice|25.0| high|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(col('ID'),ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  6| null|34.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  2| null|30.0|  mid|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null| NaN|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "|  1|Alice|25.0| high|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(col('ID').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  6| null|34.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null|30.0|  mid|\n",
      "|  2| null| NaN|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  1|     |25.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(data['ID'].desc()).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task**\n",
    "Q: Find the top 3 most purchased items from the input data using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = [(1,'Fruit','Apple'),(2,'Fruit','Orange'),(3,'Fruit','Apple'),(4,'Fruit','Grapes'),(5,'Fruit','Apple'),(6,'Fruit','Apple'),(7,'Fruit','Apple'),(8,'Fruit','Banana'),(9,'Fruit','Orange'),(10,'Fruit','Pineapple'),(11,'Fruit','Apple'),(12,'Fruit','Orange'),(13,'Fruit','Banana'),(14,'Fruit','Apple'),(15,'Fruit','Grapes'),(16,'Fruit','watermelon')]\n",
    "schema = ['orderNum','catrgory','itemName']\n",
    "df = spark.createDataFrame(da,schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|itemName|counts|\n",
      "+--------+------+\n",
      "|   Apple|     7|\n",
      "|  Orange|     3|\n",
      "|  Grapes|     2|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(col('itemName')).agg(expr('count(itemName) as counts')).sort('counts',ascending=False).limit(3).show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dealing with Nulls while sorting Spark Data Frame**\n",
    "* When we have null in the column according to which we want to round-off the data then we can use .asc_nulls_first(),.asc_nulls_last(),desc_nulls_first(),desc_nulls_last().\n",
    "* The data is in asc and nulls are at the top if we write nulls_first or at the last when we write nulls_last()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  3| null| NaN| null|  222|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.orderBy (col('class').asc_nulls_first()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  3| null| NaN| null|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  2| null|30.0|  mid|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2| null| NaN|  Low|\n",
      "|  6| null|34.0|  Low|\n",
      "|  1|     |25.0|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.orderBy (col('class').desc_nulls_first()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  2| null|30.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null| NaN|  Low|\n",
      "|  1|     |25.0|  Low|\n",
      "|  6| null|34.0|  Low|\n",
      "|  3| null| NaN| null|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.orderBy (col('class').desc_nulls_last()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  3| null| NaN| null|\n",
      "|  2| null|30.0|  mid|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null| NaN|  Low|\n",
      "|  1|     |25.0|  Low|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.orderBy (data['class'].desc_nulls_first()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.orderBy (data['class'].desc_nulls_first()).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Composite Sorting of a Data Frame**\n",
    "* sort the data according to id in asc and name in descending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  2| null|30.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(col('marks').desc(),col('id').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  6| null|34.0|  Low|  180|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(data['ID'].desc(),data['marks'].asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  6| null|34.0|  Low|  180|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(desc('ID'),asc('marks')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(['id','marks'],ascending=[1,0]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prioritized Sorting of a Spark Data Frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n",
      "data count:: 9\n",
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "+---+-----+----+-----+-----+\n",
      "\n",
      "+--------+------------+---------+--------+------+\n",
      "|CourseId|  Complexity|     Name|Enrolled| Price|\n",
      "+--------+------------+---------+--------+------+\n",
      "|       1|    Beginner| Course 1|     150| 99.99|\n",
      "|       2|Intermediate| Course 2|     120|129.99|\n",
      "|       3|    Advanced| Course 3|     180|149.99|\n",
      "|       4|    Beginner| Course 4|      90| 79.99|\n",
      "|       5|Intermediate| Course 5|     160|119.99|\n",
      "|       6|    Advanced| Course 6|     190|159.99|\n",
      "|       7|    Beginner| Course 7|     110| 89.99|\n",
      "|       8|Intermediate| Course 8|     140|109.99|\n",
      "|       9|    Advanced| Course 9|     170|139.99|\n",
      "|      10|    Beginner|Course 10|      70| 69.99|\n",
      "|      11|Intermediate|Course 11|     130| 99.99|\n",
      "|      12|    Advanced|Course 12|     200|169.99|\n",
      "|      13|    Beginner|Course 13|      95| 79.99|\n",
      "|      14|Intermediate|Course 14|     175|129.99|\n",
      "|      15|    Advanced|Course 15|     185|149.99|\n",
      "|      16|    Beginner|Course 16|     105| 89.99|\n",
      "|      17|Intermediate|Course 17|     145|119.99|\n",
      "|      18|    Advanced|Course 18|     195|159.99|\n",
      "|      19|    Beginner|Course 19|      75| 69.99|\n",
      "|      20|Intermediate|Course 20|     125| 99.99|\n",
      "+--------+------------+---------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+---------+--------+------+\n",
      "|CourseId|  Complexity|     Name|Enrolled| Price|\n",
      "+--------+------------+---------+--------+------+\n",
      "|       1|    Beginner| Course 1|     150| 99.99|\n",
      "|      16|    Beginner|Course 16|     105| 89.99|\n",
      "|       7|    Beginner| Course 7|     110| 89.99|\n",
      "|      22|    Beginner|Course 22|     100| 79.99|\n",
      "|      28|    Beginner|Course 28|     105| 79.99|\n",
      "|      13|    Beginner|Course 13|      95| 79.99|\n",
      "|       4|    Beginner| Course 4|      90| 79.99|\n",
      "|      19|    Beginner|Course 19|      75| 69.99|\n",
      "|      10|    Beginner|Course 10|      70| 69.99|\n",
      "|      25|    Beginner|Course 25|      85| 69.99|\n",
      "|      14|Intermediate|Course 14|     175|129.99|\n",
      "|      23|Intermediate|Course 23|     155|129.99|\n",
      "|       2|Intermediate| Course 2|     120|129.99|\n",
      "|       5|Intermediate| Course 5|     160|119.99|\n",
      "|      17|Intermediate|Course 17|     145|119.99|\n",
      "|      29|Intermediate|Course 29|     140|119.99|\n",
      "|       8|Intermediate| Course 8|     140|109.99|\n",
      "|      26|Intermediate|Course 26|     115| 99.99|\n",
      "|      11|Intermediate|Course 11|     130| 99.99|\n",
      "|      20|Intermediate|Course 20|     125| 99.99|\n",
      "+--------+------------+---------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_dfs.sort(when(col('Complexity')=='Beginner',0).when(col('Complexity')=='Intermediate',1).otherwise(2),col('Price').desc()).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL Style Syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cas=expr(\"case when Complexity == 'Beginner' then 0 when Complexity == 'Intermediate' then 1 else 2 end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+---------+--------+------+\n",
      "|CourseId|  Complexity|     Name|Enrolled| Price|\n",
      "+--------+------------+---------+--------+------+\n",
      "|       1|    Beginner| Course 1|     150| 99.99|\n",
      "|       7|    Beginner| Course 7|     110| 89.99|\n",
      "|      16|    Beginner|Course 16|     105| 89.99|\n",
      "|      28|    Beginner|Course 28|     105| 79.99|\n",
      "|      22|    Beginner|Course 22|     100| 79.99|\n",
      "|       4|    Beginner| Course 4|      90| 79.99|\n",
      "|      13|    Beginner|Course 13|      95| 79.99|\n",
      "|      10|    Beginner|Course 10|      70| 69.99|\n",
      "|      19|    Beginner|Course 19|      75| 69.99|\n",
      "|      25|    Beginner|Course 25|      85| 69.99|\n",
      "|       2|Intermediate| Course 2|     120|129.99|\n",
      "|      23|Intermediate|Course 23|     155|129.99|\n",
      "|      14|Intermediate|Course 14|     175|129.99|\n",
      "|      17|Intermediate|Course 17|     145|119.99|\n",
      "|       5|Intermediate| Course 5|     160|119.99|\n",
      "|      29|Intermediate|Course 29|     140|119.99|\n",
      "|       8|Intermediate| Course 8|     140|109.99|\n",
      "|      11|Intermediate|Course 11|     130| 99.99|\n",
      "|      26|Intermediate|Course 26|     115| 99.99|\n",
      "|      20|Intermediate|Course 20|     125| 99.99|\n",
      "+--------+------------+---------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_dfs.sort(cas,col('Price').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n",
      "data count:: 9\n",
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "+---+-----+----+-----+-----+\n",
      "\n",
      "+--------+------------+---------+--------+------+\n",
      "|CourseId|  Complexity|     Name|Enrolled| Price|\n",
      "+--------+------------+---------+--------+------+\n",
      "|       1|    Beginner| Course 1|     150| 99.99|\n",
      "|       2|Intermediate| Course 2|     120|129.99|\n",
      "|       3|    Advanced| Course 3|     180|149.99|\n",
      "|       4|    Beginner| Course 4|      90| 79.99|\n",
      "|       5|Intermediate| Course 5|     160|119.99|\n",
      "|       6|    Advanced| Course 6|     190|159.99|\n",
      "|       7|    Beginner| Course 7|     110| 89.99|\n",
      "|       8|Intermediate| Course 8|     140|109.99|\n",
      "|       9|    Advanced| Course 9|     170|139.99|\n",
      "|      10|    Beginner|Course 10|      70| 69.99|\n",
      "|      11|Intermediate|Course 11|     130| 99.99|\n",
      "|      12|    Advanced|Course 12|     200|169.99|\n",
      "|      13|    Beginner|Course 13|      95| 79.99|\n",
      "|      14|Intermediate|Course 14|     175|129.99|\n",
      "|      15|    Advanced|Course 15|     185|149.99|\n",
      "|      16|    Beginner|Course 16|     105| 89.99|\n",
      "|      17|Intermediate|Course 17|     145|119.99|\n",
      "|      18|    Advanced|Course 18|     195|159.99|\n",
      "|      19|    Beginner|Course 19|      75| 69.99|\n",
      "|      20|Intermediate|Course 20|     125| 99.99|\n",
      "+--------+------------+---------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Subset and calculate**\n",
    "After you've extracted values from a list, you can use them to perform additional calculations. Take this example, where the second and fourth element of a list x are extracted. The strings that result are pasted together using the + operator:\n",
    "\n",
    "x = [\"a\", \"b\", \"c\", \"d\"]\\\n",
    "print(x[1] + x[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Aggregations on a Spark Data Frame**\n",
    "1. Get revenue using order_item_subtotal for a given order_item_order_id eg 2\n",
    "2. Get number of items , total quantity as well as revenue for given items order id eg 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|1            |1                  |957                  |299.98                  |1                  |299.98             |\n",
      "|2            |2                  |1073                 |199.99                  |1                  |199.99             |\n",
      "|3            |2                  |502                  |50.0                    |5                  |250.0              |\n",
      "|4            |2                  |403                  |129.99                  |1                  |129.99             |\n",
      "|5            |4                  |897                  |24.99                   |2                  |49.98              |\n",
      "|6            |4                  |365                  |59.99                   |5                  |299.95             |\n",
      "|7            |4                  |502                  |50.0                    |3                  |150.0              |\n",
      "|8            |4                  |1014                 |49.98                   |4                  |199.92             |\n",
      "|9            |5                  |957                  |299.98                  |1                  |299.98             |\n",
      "|10           |5                  |365                  |59.99                   |5                  |299.95             |\n",
      "|11           |5                  |1014                 |49.98                   |2                  |99.96              |\n",
      "|12           |5                  |957                  |299.98                  |1                  |299.98             |\n",
      "|13           |5                  |403                  |129.99                  |1                  |129.99             |\n",
      "|14           |7                  |1073                 |199.99                  |1                  |199.99             |\n",
      "|15           |7                  |957                  |299.98                  |1                  |299.98             |\n",
      "|16           |7                  |926                  |15.99                   |5                  |79.95              |\n",
      "|17           |8                  |365                  |59.99                   |3                  |179.97             |\n",
      "|18           |8                  |365                  |59.99                   |5                  |299.95             |\n",
      "|19           |8                  |1014                 |49.98                   |4                  |199.92             |\n",
      "|20           |8                  |502                  |50.0                    |1                  |50.0               |\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json('../Source_data/retail_db_json/order_items/')\n",
    "df.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_item_id', 'bigint'),\n",
       " ('order_item_order_id', 'bigint'),\n",
       " ('order_item_product_id', 'bigint'),\n",
       " ('order_item_product_price', 'double'),\n",
       " ('order_item_quantity', 'bigint'),\n",
       " ('order_item_subtotal', 'double')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| total|\n",
      "+------+\n",
      "|579.98|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('order_item_order_id = 2').select (sum('order_item_subtotal').alias('total')).show()\n",
    "# select(sum(col('order_item_subtotal'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-------------+\n",
      "|number_of_items|total_quantites|total_revenue|\n",
      "+---------------+---------------+-------------+\n",
      "|              3|              7|       579.98|\n",
      "+---------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('order_item_order_id = 2').select (count('order_item_quantity').alias(\"number_of_items\"),sum('order_item_quantity').alias('total_quantites'),sum('order_item_subtotal').alias('total_revenue')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting Count of a Spark Data Frame**\n",
    "* If we use df.count() it will count the recording in the dataframe.\n",
    "* When we use df.select (count('*')).show() it will count the recording as by using function count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172198"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  172198|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (count('*')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of groupBy on Spark Data Frame**\n",
    "* If i write df.groupBy().min().show() it will apply group by and min on all the numeric columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n",
      "data count:: 9\n",
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "+---+-----+----+-----+-----+\n",
      "\n",
      "+--------+------------+---------+--------+------+\n",
      "|CourseId|  Complexity|     Name|Enrolled| Price|\n",
      "+--------+------------+---------+--------+------+\n",
      "|       1|    Beginner| Course 1|     150| 99.99|\n",
      "|       2|Intermediate| Course 2|     120|129.99|\n",
      "|       3|    Advanced| Course 3|     180|149.99|\n",
      "|       4|    Beginner| Course 4|      90| 79.99|\n",
      "|       5|Intermediate| Course 5|     160|119.99|\n",
      "|       6|    Advanced| Course 6|     190|159.99|\n",
      "|       7|    Beginner| Course 7|     110| 89.99|\n",
      "|       8|Intermediate| Course 8|     140|109.99|\n",
      "|       9|    Advanced| Course 9|     170|139.99|\n",
      "|      10|    Beginner|Course 10|      70| 69.99|\n",
      "|      11|Intermediate|Course 11|     130| 99.99|\n",
      "|      12|    Advanced|Course 12|     200|169.99|\n",
      "|      13|    Beginner|Course 13|      95| 79.99|\n",
      "|      14|Intermediate|Course 14|     175|129.99|\n",
      "|      15|    Advanced|Course 15|     185|149.99|\n",
      "|      16|    Beginner|Course 16|     105| 89.99|\n",
      "|      17|Intermediate|Course 17|     145|119.99|\n",
      "|      18|    Advanced|Course 18|     195|159.99|\n",
      "|      19|    Beginner|Course 19|      75| 69.99|\n",
      "|      20|Intermediate|Course 20|     125| 99.99|\n",
      "+--------+------------+---------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            1|                  1|                  957|                  299.98|                  1|             299.98|\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "|            3|                  2|                  502|                    50.0|                  5|              250.0|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json('../Source_data/retail_db_json/order_items/')\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n",
      "|min(order_item_id)|min(order_item_order_id)|min(order_item_product_id)|min(order_item_product_price)|min(order_item_quantity)|min(order_item_subtotal)|\n",
      "+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n",
      "|                 1|                       1|                        19|                         9.99|                       1|                    9.99|\n",
      "+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy().min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| count|\n",
      "+------+\n",
      "|172198|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy().count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+--------+---------------+\n",
      "|order_customer_id|order_date           |order_id|order_status   |\n",
      "+-----------------+---------------------+--------+---------------+\n",
      "|11599            |2013-07-25 00:00:00.0|1       |CLOSED         |\n",
      "|256              |2013-07-25 00:00:00.0|2       |PENDING_PAYMENT|\n",
      "|12111            |2013-07-25 00:00:00.0|3       |COMPLETE       |\n",
      "+-----------------+---------------------+--------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfo=spark.read.json('../Source_data/retail_db_json/orders/')\n",
    "dfo.show(truncate=0,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_customer_id', 'bigint'),\n",
       " ('order_date', 'string'),\n",
       " ('order_id', 'bigint'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfo.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perform Grouped Aggregations using direct functions on a Spark Data Frame**\n",
    "* When we want to used group aggregation using direct function we can use one aggregation function like max,sum,count etc. When we want to use multiple aggregation we can use\\\n",
    "    function call agg()\n",
    "* When we apply direct function without specifying a column name it will perform aggregation on all the columns.\n",
    "* To perform aggregation on a column we need to write the name in string formate.\n",
    "* once we use df.groupBy() it is changed to grouped Dataframe type and we can use agg function on top to this datatype.\n",
    "* When we apply sum function we cant apply round directly on it we have to use withColumn and then use round aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------------+-------------+\n",
      "|order_customer_id|sum(order_customer_id)|sum(order_id)|\n",
      "+-----------------+----------------------+-------------+\n",
      "|            11938|                 71628|       215124|\n",
      "|             1950|                 17550|       261294|\n",
      "|             2529|                 15174|       269506|\n",
      "+-----------------+----------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfo.groupBy('order_customer_id').sum().show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note::\\\n",
    "> Here you cant see any sort of string column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            1|                  1|                  957|                  299.98|                  1|             299.98|\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "|            3|                  2|                  502|                    50.0|                  5|              250.0|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json('../Source_data/retail_db_json/order_items/')\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.group.GroupedData"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group=df.groupBy('order_item_order_id')\n",
    "type(df_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n",
      "|order_item_order_id|sum(order_item_id)|sum(order_item_order_id)|sum(order_item_product_id)|sum(order_item_product_price)|sum(order_item_quantity)|sum(order_item_subtotal)|\n",
      "+-------------------+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n",
      "|                 29|               425|                     145|                      3897|            909.9300000000001|                       9|                 1109.85|\n",
      "|                474|              5815|                    2370|                      4508|           374.94000000000005|                      13|       774.8199999999999|\n",
      "|                964|              9586|                    3856|                      2964|           499.95000000000005|                      11|       739.8800000000001|\n",
      "|               1677|             20860|                    8385|                      2357|                       277.97|                      14|       649.9200000000001|\n",
      "+-------------------+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## We we see no aggregation on top of string column\n",
    "df_group.sum().show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------------+------------------------+------------------------+\n",
      "|order_item_order_id|sum(order_item_product_price)|sum(order_item_quantity)|sum(order_item_subtotal)|\n",
      "+-------------------+-----------------------------+------------------------+------------------------+\n",
      "|                 29|            909.9300000000001|                       9|                 1109.85|\n",
      "|                474|           374.94000000000005|                      13|       774.8199999999999|\n",
      "|                964|           499.95000000000005|                      11|       739.8800000000001|\n",
      "|               1677|                       277.97|                      14|       649.9200000000001|\n",
      "+-------------------+-----------------------------+------------------------+------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### To be very much specific now we can pass the column names in aggregate function\n",
    "## we cant use round on top of it first we need to change the names and the go for rounding the function.\n",
    "df_group.sum('order_item_product_price','order_item_quantity','order_item_subtotal').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_order_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------------+------------------------+-------------------+-------------------+\n",
      "|                 29|                  909.93|                  9|            1109.85|\n",
      "|                474|                  374.94|                 13|             774.82|\n",
      "|                964|                  499.95|                 11|             739.88|\n",
      "|               1677|                  277.97|                 14|             649.92|\n",
      "+-------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.sum('order_item_product_price','order_item_quantity','order_item_subtotal').select('order_item_order_id','sum(order_item_product_price)','sum(order_item_quantity)','sum(order_item_subtotal)').\\\n",
    "    toDF('order_item_order_id','order_item_product_price','order_item_quantity','order_item_subtotal').\\\n",
    "        withColumn('order_item_product_price',round('order_item_product_price',2)).\\\n",
    "        withColumn('order_item_subtotal',round('order_item_subtotal',2)).\\\n",
    "show(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perform Grouped Aggregations using Agg on a Spark Data Frame**\n",
    "* In group aggregation we have two approaches first is using dict and other is using column style approach.\n",
    "* Dict has its limitations so mostly we use list type syntax.\n",
    "* In case of Dict we cant apply round function on top of the sum function.\n",
    "* If we want to work on same column but use different function we cant use that as in Dictionary we are suppose to use unique keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n",
      "data count:: 9\n",
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "+---+-----+----+-----+-----+\n",
      "\n",
      "+--------+------------+---------+--------+------+\n",
      "|CourseId|  Complexity|     Name|Enrolled| Price|\n",
      "+--------+------------+---------+--------+------+\n",
      "|       1|    Beginner| Course 1|     150| 99.99|\n",
      "|       2|Intermediate| Course 2|     120|129.99|\n",
      "|       3|    Advanced| Course 3|     180|149.99|\n",
      "|       4|    Beginner| Course 4|      90| 79.99|\n",
      "|       5|Intermediate| Course 5|     160|119.99|\n",
      "|       6|    Advanced| Course 6|     190|159.99|\n",
      "|       7|    Beginner| Course 7|     110| 89.99|\n",
      "|       8|Intermediate| Course 8|     140|109.99|\n",
      "|       9|    Advanced| Course 9|     170|139.99|\n",
      "|      10|    Beginner|Course 10|      70| 69.99|\n",
      "|      11|Intermediate|Course 11|     130| 99.99|\n",
      "|      12|    Advanced|Course 12|     200|169.99|\n",
      "|      13|    Beginner|Course 13|      95| 79.99|\n",
      "|      14|Intermediate|Course 14|     175|129.99|\n",
      "|      15|    Advanced|Course 15|     185|149.99|\n",
      "|      16|    Beginner|Course 16|     105| 89.99|\n",
      "|      17|Intermediate|Course 17|     145|119.99|\n",
      "|      18|    Advanced|Course 18|     195|159.99|\n",
      "|      19|    Beginner|Course 19|      75| 69.99|\n",
      "|      20|Intermediate|Course 20|     125| 99.99|\n",
      "+--------+------------+---------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            1|                  1|                  957|                  299.98|                  1|             299.98|\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "|            3|                  2|                  502|                    50.0|                  5|              250.0|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json('../Source_data/retail_db_json/order_items/')\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------------+------------------------+\n",
      "|order_item_order_id|sum(order_item_product_price)|sum(order_item_subtotal)|\n",
      "+-------------------+-----------------------------+------------------------+\n",
      "|                 29|            909.9300000000001|                 1109.85|\n",
      "|                474|           374.94000000000005|       774.8199999999999|\n",
      "|                964|           499.95000000000005|       739.8800000000001|\n",
      "|               1677|                       277.97|       649.9200000000001|\n",
      "+-------------------+-----------------------------+------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group=df.groupBy('order_item_order_id')\n",
    "\n",
    "df_group.agg({'order_item_product_price':'sum','order_item_subtotal':'sum'}).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+-------------------+\n",
      "|order_item_order_id|order_item_product_price|order_item_subtotal|\n",
      "+-------------------+------------------------+-------------------+\n",
      "|                 29|                  909.93|            1109.85|\n",
      "|                474|                  374.94|             774.82|\n",
      "|                964|                  499.95|             739.88|\n",
      "|               1677|                  277.97|             649.92|\n",
      "+-------------------+------------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.agg({'order_item_product_price':'sum','order_item_subtotal':'sum'}).\\\n",
    "    toDF('order_item_order_id','order_item_product_price','order_item_subtotal').\\\n",
    "    withColumn('order_item_product_price',round('order_item_product_price',2)).\\\n",
    "    withColumn('order_item_subtotal',round('order_item_subtotal',2)).\\\n",
    "    show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------------------------+----------------------------------+\n",
      "|order_item_order_id|round(sum(order_item_product_price), 2)|round(sum(order_item_subtotal), 2)|\n",
      "+-------------------+---------------------------------------+----------------------------------+\n",
      "|                 29|                                 909.93|                           1109.85|\n",
      "|                474|                                 374.94|                            774.82|\n",
      "|                964|                                 499.95|                            739.88|\n",
      "|               1677|                                 277.97|                            649.92|\n",
      "+-------------------+---------------------------------------+----------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.agg(round(sum('order_item_product_price'),2),round(sum('order_item_subtotal'),2)).\\\n",
    "    toDF('order_item_order_id','order_item_product_price','order_item_subtotal').\\\n",
    "    show(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: \\\n",
    "If we want to work on same column but use different function we cant use that as in Dictionary we are suppose to use unique keys.\\\n",
    "It was support show 3 columns but its show the min column.\\\n",
    "Lets go for list type syntax this problem is solved over there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+\n",
      "|order_item_order_id|min(order_item_quantity)|\n",
      "+-------------------+------------------------+\n",
      "|                 29|                       1|\n",
      "|                474|                       1|\n",
      "|                964|                       1|\n",
      "|               1677|                       1|\n",
      "+-------------------+------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.agg({'order_item_quantity':'sum','order_item_quantity':'min'}).\\\n",
    "    show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+------------------------+\n",
      "|order_item_order_id|sum(order_item_quantity)|min(order_item_quantity)|\n",
      "+-------------------+------------------------+------------------------+\n",
      "|                 29|                       9|                       1|\n",
      "|                474|                      13|                       1|\n",
      "|                964|                      11|                       1|\n",
      "|               1677|                      14|                       1|\n",
      "+-------------------+------------------------+------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.agg(sum('order_item_quantity'),min('order_item_quantity')).show(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **JOINS In DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "courses_df has\n",
      "\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n",
      "users_df has\n",
      "\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "\n",
      "course_enrolments_df has\n",
      "\n",
      "+-------------------+-------+---------+----------+\n",
      "|course_enrolment_id|user_id|course_id|price_paid|\n",
      "+-------------------+-------+---------+----------+\n",
      "|                  1|     10|        2|      9.99|\n",
      "|                  2|      5|        2|      9.99|\n",
      "|                  3|      7|        5|     10.99|\n",
      "|                  4|      9|        2|      9.99|\n",
      "|                  5|      8|        2|      9.99|\n",
      "|                  6|      5|        5|     10.99|\n",
      "|                  7|      4|        5|     10.99|\n",
      "|                  8|      7|        3|     10.99|\n",
      "|                  9|      8|        5|     10.99|\n",
      "|                 10|      3|        3|     10.99|\n",
      "|                 11|      7|        5|     10.99|\n",
      "|                 12|      3|        2|      9.99|\n",
      "|                 13|      5|        2|      9.99|\n",
      "|                 14|      4|        3|     10.99|\n",
      "|                 15|      8|        2|      9.99|\n",
      "+-------------------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./joins_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method join in module pyspark.sql.dataframe:\n",
      "\n",
      "join(other, on=None, how=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Joins with another :class:`DataFrame`, using the given join expression.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : :class:`DataFrame`\n",
      "        Right side of the join\n",
      "    on : str, list or :class:`Column`, optional\n",
      "        a string for the join column name, a list of column names,\n",
      "        a join expression (Column), or a list of Columns.\n",
      "        If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "        the column(s) must exist on both sides, and this performs an equi-join.\n",
      "    how : str, optional\n",
      "        default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "        ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      "        ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      "        ``anti``, ``leftanti`` and ``left_anti``.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    The following performs a full outer join between ``df1`` and ``df2``.\n",
      "    \n",
      "    >>> from pyspark.sql.functions import desc\n",
      "    >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)                 .sort(desc(\"name\")).collect()\n",
      "    [Row(name='Bob', height=85), Row(name='Alice', height=None), Row(name=None, height=80)]\n",
      "    \n",
      "    >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).collect()\n",
      "    [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      "    \n",
      "    >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      "    >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      "    [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "    \n",
      "    >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      "    [Row(name='Bob', height=85)]\n",
      "    \n",
      "    >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      "    [Row(name='Bob', age=5)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(users_df.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method crossJoin in module pyspark.sql.dataframe:\n",
      "\n",
      "crossJoin(other) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns the cartesian product with another :class:`DataFrame`.\n",
      "    \n",
      "    .. versionadded:: 2.1.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : :class:`DataFrame`\n",
      "        Right side of the cartesian product.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.select(\"age\", \"name\").collect()\n",
      "    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "    >>> df2.select(\"name\", \"height\").collect()\n",
      "    [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n",
      "    >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n",
      "    [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n",
      "     Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(users_df.crossJoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.alias('a').select('a.*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+-------------------+---------+----------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|course_enrolment_id|course_id|price_paid|\n",
      "+-------+---------------+--------------+--------------------+-------------------+---------+----------+\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|                 10|        3|     10.99|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|                 12|        2|      9.99|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|                  7|        5|     10.99|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|                 14|        3|     10.99|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|                  2|        2|      9.99|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|                  6|        5|     10.99|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|                 13|        2|      9.99|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                  3|        5|     10.99|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                  8|        3|     10.99|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                 11|        5|     10.99|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|                  5|        2|      9.99|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|                  9|        5|     10.99|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|                 15|        2|      9.99|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|                  4|        2|      9.99|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|                  1|        2|      9.99|\n",
      "+-------+---------------+--------------+--------------------+-------------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## way1\n",
    "users_df.join(course_enrolments_df,'user_id').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task**\n",
    "* Show user details that have enrolled in courses\n",
    "* Need to display all the fields of users_df and two fields of course_id and course_enrollment_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|course_enrolment_id|user_id|\n",
      "+-------------------+-------+\n",
      "|                 10|      3|\n",
      "|                 12|      3|\n",
      "|                  7|      4|\n",
      "+-------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## way2\n",
    "users_df.alias('c').join(course_enrolments_df.alias('b'),users_df.user_id==course_enrolments_df.user_id).select('b.course_enrolment_id','c.user_id').show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|course_enrolment_id|user_id|course_id|price_paid|\n",
      "+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|                 10|      3|        3|     10.99|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|                 12|      3|        2|      9.99|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|                  7|      4|        5|     10.99|\n",
      "+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## way3\n",
    "c=[users_df.user_id==course_enrolments_df.user_id]\n",
    "users_df.join(course_enrolments_df,c).show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+---------+-------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|course_id|course_enrolment_id|\n",
      "+-------+---------------+--------------+--------------------+---------+-------------------+\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|        3|                 10|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|        2|                 12|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|        5|                  7|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|        3|                 14|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|        2|                  2|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|        5|                  6|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|        2|                 13|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|        5|                  3|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|        3|                  8|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|        5|                 11|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|        2|                  5|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|        5|                  9|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|        2|                 15|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|        2|                  4|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|        2|                  1|\n",
      "+-------+---------------+--------------+--------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.alias ('a').join(course_enrolments_df.alias('b'),users_df.user_id==course_enrolments_df.user_id).select ('a.*','b.course_id','b.course_enrolment_id').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Count the number of courses a users is enrolled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id|count|\n",
      "+-------+-----+\n",
      "|      3|    2|\n",
      "|      4|    2|\n",
      "|      5|    3|\n",
      "|      7|    3|\n",
      "|      8|    3|\n",
      "|      9|    1|\n",
      "|     10|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.alias ('a').join(course_enrolments_df.alias('b'),users_df.user_id==course_enrolments_df.user_id).groupBy('a.user_id').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Details**\n",
    "* Get the users detail and courses enrolled in if there are any.\n",
    "* If the user have no course enrolled we need to get details of the user, course_enrolment_id and course_enrolments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.join(course_enrolments_df,users_df.user_id==course_enrolments_df.user_id,how='left').filter(course_enrolments_df['course_enrolment_id'].isNull()).select (users_df['*']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Details**\n",
    "* Get number of courses enroled by each user.\n",
    "* If there are no enrolments then count should return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id|count|\n",
      "+-------+-----+\n",
      "|      1|    0|\n",
      "|      2|    0|\n",
      "|      3|    2|\n",
      "|      4|    2|\n",
      "|      5|    3|\n",
      "|      6|    0|\n",
      "|      7|    3|\n",
      "|      8|    3|\n",
      "|      9|    1|\n",
      "|     10|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## method1\n",
    "users_df.alias('a').join(course_enrolments_df.alias('b'), users_df.user_id==course_enrolments_df.user_id,how='outer').\\\n",
    "    groupBy('a.user_id').agg (expr('sum(case when (b.course_enrolment_id is Null) then 0 else 1 end )as count')).orderBy('a.user_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id|count|\n",
      "+-------+-----+\n",
      "|      1|    0|\n",
      "|      2|    0|\n",
      "|      3|    2|\n",
      "|      4|    2|\n",
      "|      5|    3|\n",
      "|      6|    0|\n",
      "|      7|    3|\n",
      "|      8|    3|\n",
      "|      9|    1|\n",
      "|     10|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## method2\n",
    "users_df.alias('a').join(course_enrolments_df.alias('b'),users_df.user_id==course_enrolments_df.user_id,how='outer').groupBy('a.user_id').agg(sum(expr('''\n",
    "                                                                                                                                            case when b.course_enrolment_id is Null\n",
    "                                                                                                                                            then 0 else 1 end\n",
    "                                                                                                                                            ''')).alias('count')).orderBy('a.user_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id|count|\n",
      "+-------+-----+\n",
      "|      1|    0|\n",
      "|      2|    0|\n",
      "|      3|    2|\n",
      "|      4|    2|\n",
      "|      5|    3|\n",
      "|      6|    0|\n",
      "|      7|    3|\n",
      "|      8|    3|\n",
      "|      9|    1|\n",
      "|     10|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## method3\n",
    "users_df.alias('a').join(course_enrolments_df.alias('b'),users_df.user_id==course_enrolments_df.user_id,how='outer').\\\n",
    "groupBy('a.user_id').agg(sum(when(col('course_enrolment_id').isNull(),0).otherwise(1)).alias('count')).orderBy('a.user_id').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note::\\\n",
    "> We can replace it with left join and then union with right join and then there will  be duplication so to remove the dups we use .distinct()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Broad Cast Join**\n",
    "* When we have to join data frame and one data frame is small 10MB and other is large it will cause issue so we can broadcast the small table means that the small table will be\\\n",
    "  sent to all executor nodes of the other Big dataframe and after the join happend the data is giving to driver to display.\n",
    "* If you are getting error of broad cast just disable the autoBroadCastJoin property to -1 and then rerun by using more memery for the executors.\n",
    "* you can use autoBroadCastJoin='1500m' to automatically broad cast dataframe less then this value size.\n",
    "* we can manually broadcast dataframe by using BroadCast join function.\n",
    "* If we set autoBroadCast join to 10MB then when we use simple join function on dataset greater then 10MB it will start reduce side join.\n",
    "* By default the autoBroadCastJoin threshold is 10MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10485760b'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.autoBroadcastJoinThreshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1kb'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.autoBroadcastJoinThreshold','1kb')\n",
    "spark.conf.get('spark.sql.autoBroadcastJoinThreshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.autoBroadcastJoinThreshold','10485760b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10485760b'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.autoBroadcastJoinThreshold')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note::\\\n",
    "> Please note the execution timing of both the results with and without broadcasr join even the threshold value is set to 10MB still its taking time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|course_enrolment_id|user_id|course_id|price_paid|\n",
      "+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                  1|     10|        2|      9.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                  2|      5|        2|      9.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                  3|      7|        5|     10.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                  4|      9|        2|      9.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                  5|      8|        2|      9.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                  6|      5|        5|     10.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                  7|      4|        5|     10.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                  8|      7|        3|     10.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                  9|      8|        5|     10.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                 10|      3|        3|     10.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                 11|      7|        5|     10.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                 12|      3|        2|      9.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                 13|      5|        2|      9.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                 14|      4|        3|     10.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                 15|      8|        2|      9.99|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|                  1|     10|        2|      9.99|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|                  2|      5|        2|      9.99|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|                  3|      7|        5|     10.99|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|                  4|      9|        2|      9.99|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|                  5|      8|        2|      9.99|\n",
      "+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(users_df).join(course_enrolments_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|course_enrolment_id|user_id|course_id|price_paid|\n",
      "+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                  1|     10|        2|      9.99|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|                  1|     10|        2|      9.99|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|                  1|     10|        2|      9.99|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|                  1|     10|        2|      9.99|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|                  1|     10|        2|      9.99|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|                  1|     10|        2|      9.99|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                  1|     10|        2|      9.99|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|                  1|     10|        2|      9.99|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|                  1|     10|        2|      9.99|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|                  1|     10|        2|      9.99|\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|                  2|      5|        2|      9.99|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|                  2|      5|        2|      9.99|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|                  2|      5|        2|      9.99|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|                  2|      5|        2|      9.99|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|                  2|      5|        2|      9.99|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|                  2|      5|        2|      9.99|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                  2|      5|        2|      9.99|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|                  2|      5|        2|      9.99|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|                  2|      5|        2|      9.99|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|                  2|      5|        2|      9.99|\n",
      "+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broadcast(users_df).join(course_enrolments_df).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Validate Data Sets for Reading from Files using Spark APIs**\n",
    "* We will import data  into spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "courses_df has\n",
      "\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n",
      "users_df has\n",
      "\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "\n",
      "course_enrolments_df has\n",
      "\n",
      "+-------------------+-------+---------+----------+\n",
      "|course_enrolment_id|user_id|course_id|price_paid|\n",
      "+-------------------+-------+---------+----------+\n",
      "|                  1|     10|        2|      9.99|\n",
      "|                  2|      5|        2|      9.99|\n",
      "|                  3|      7|        5|     10.99|\n",
      "|                  4|      9|        2|      9.99|\n",
      "|                  5|      8|        2|      9.99|\n",
      "|                  6|      5|        5|     10.99|\n",
      "|                  7|      4|        5|     10.99|\n",
      "|                  8|      7|        3|     10.99|\n",
      "|                  9|      8|        5|     10.99|\n",
      "|                 10|      3|        3|     10.99|\n",
      "|                 11|      7|        5|     10.99|\n",
      "|                 12|      3|        2|      9.99|\n",
      "|                 13|      5|        2|      9.99|\n",
      "|                 14|      4|        3|     10.99|\n",
      "|                 15|      8|        2|      9.99|\n",
      "+-------------------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./joins_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = \"\"\"\n",
    "RowID int,\n",
    "OrderID int,\n",
    "OrderDate string,\n",
    "ShipDate string,\n",
    "ShipMode string,\n",
    "CustomerID int,\n",
    "CustomerName string,\n",
    "Segment string,\n",
    "Location string,\n",
    "State string,\n",
    "PostalCode string,\n",
    "Region string,\n",
    "ProductID int,\n",
    "Category string,\n",
    "Sub_Category string,\n",
    "ProductName string,\n",
    "Sales string,\n",
    "Quantity int,\n",
    "Discount int,\n",
    "Profit string\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+----------+------------+----------+-------------+--------+--------------------+--------+-----------+------+---------+---------+------------+--------------------+------+--------+--------+-------+\n",
      "|RowID|OrderID| OrderDate|  ShipDate|    ShipMode|CustomerID| CustomerName| Segment|            Location|   State| PostalCode|Region|ProductID| Category|Sub_Category|         ProductName| Sales|Quantity|Discount| Profit|\n",
      "+-----+-------+----------+----------+------------+----------+-------------+--------+--------------------+--------+-----------+------+---------+---------+------------+--------------------+------+--------+--------+-------+\n",
      "| null|   null|Order Date| Ship Date|   Ship Mode|      null|Customer Name| Segment|            Location|   State|Postal Code|Region|     null| Category|Sub-Category|        Product Name| Sales|    null|    null| Profit|\n",
      "|    1|   null|15/04/2017|1101102016|Second Class|      null|  Claire Gute|Consumer|United States,Hen...|Kentucky|      42420| South|     null|Furniture|   Bookcases|Bush Somerset Col...|261.96|       2|       0|41.9136|\n",
      "|    2|   null|15/04/2018|1101102016|Second Class|      null|  Claire Gute|Consumer|United States,Hen...|Kentucky|      42420| South|     null|Furniture|      Chairs|Hon Deluxe Fabric...|731.94|       3|       0|219.582|\n",
      "+-----+-------+----------+----------+------------+----------+-------------+--------+--------------------+--------+-----------+------+---------+---------+------------+--------------------+------+--------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.schema(s).csv('../Source_data/orders.csv').show(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RowID: integer (nullable = true)\n",
      " |-- OrderID: integer (nullable = true)\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- ShipDate: string (nullable = true)\n",
      " |-- ShipMode: string (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- CustomerName: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- PostalCode: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub_Category: string (nullable = true)\n",
      " |-- ProductName: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Discount: integer (nullable = true)\n",
      " |-- Profit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.schema(s).csv('../Source_data/orders.csv').printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets Import Data of files in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+--------------+-----------+--------------+-----------------+--------------+--------------------+----------------+\n",
      "|customer_city|customer_email|customer_fname|customer_id|customer_lname|customer_password|customer_state|     customer_street|customer_zipcode|\n",
      "+-------------+--------------+--------------+-----------+--------------+-----------------+--------------+--------------------+----------------+\n",
      "|  Brownsville|     XXXXXXXXX|       Richard|          1|     Hernandez|        XXXXXXXXX|            TX|  6303 Heather Plaza|           78521|\n",
      "|    Littleton|     XXXXXXXXX|          Mary|          2|       Barrett|        XXXXXXXXX|            CO|9526 Noble Embers...|           80126|\n",
      "|       Caguas|     XXXXXXXXX|           Ann|          3|         Smith|        XXXXXXXXX|            PR|3422 Blue Pioneer...|           00725|\n",
      "+-------------+--------------+--------------+-----------+--------------+-----------------+--------------+--------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json('../Source_data/retail_db_json/customers/').show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- customer_fname: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_lname: string (nullable = true)\n",
      " |-- customer_password: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- customer_street: string (nullable = true)\n",
      " |-- customer_zipcode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json('../Source_data/retail_db_json/customers/').printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Making Parquet files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=f'../Source_data/parquet/'\n",
    "inputs=f'../Source_data/retail_db_json/customers/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+--------------+-----------+--------------+-----------------+--------------+--------------------+----------------+\n",
      "|customer_city|customer_email|customer_fname|customer_id|customer_lname|customer_password|customer_state|     customer_street|customer_zipcode|\n",
      "+-------------+--------------+--------------+-----------+--------------+-----------------+--------------+--------------------+----------------+\n",
      "|  Brownsville|     XXXXXXXXX|       Richard|          1|     Hernandez|        XXXXXXXXX|            TX|  6303 Heather Plaza|           78521|\n",
      "|    Littleton|     XXXXXXXXX|          Mary|          2|       Barrett|        XXXXXXXXX|            CO|9526 Noble Embers...|           80126|\n",
      "|       Caguas|     XXXXXXXXX|           Ann|          3|         Smith|        XXXXXXXXX|            PR|3422 Blue Pioneer...|           00725|\n",
      "+-------------+--------------+--------------+-----------+--------------+-----------------+--------------+--------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(inputs).coalesce(1).write.parquet(f'{output}/customers',mode='overwrite')\n",
    "spark.read.parquet(f'{output}/customers').show(n=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reading Data in CSV and Writing to | sep**\n",
    "* I will read the data as csv and then write it to output path as pipe separated.\n",
    "* In next step I will read the data as csv it will not read it properly as its now pipe separated.\n",
    "* In next step I will read the data as pipe separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output::\n",
      "../Source_data/parquet/order_tab/\n",
      "inputs::\n",
      "../Source_data/orders.csv\n"
     ]
    }
   ],
   "source": [
    "inputs=f'../Source_data/orders.csv'\n",
    "output=f'../Source_data/parquet/order_tab/'\n",
    "print(f'output::\\n{output}\\ninputs::\\n{inputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = \"\"\"\n",
    "RowID int, OrderID string, OrderDate string, ShipDate string, ShipMode string,\n",
    "CustomerID string, CustomerName string, Segment string, Location string,\n",
    "State string, PostalCode string, Region string,ProductID string,Category string,\n",
    "Sub_Category string,ProductName string,Sales string, Quantity int,Discount int, Profit string\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------+--------+----------+------------+-------+--------+-----+----------+------+---------+--------+------------+-----------+-----+--------+--------+------+\n",
      "|RowID|             OrderID|           OrderDate|ShipDate|ShipMode|CustomerID|CustomerName|Segment|Location|State|PostalCode|Region|ProductID|Category|Sub_Category|ProductName|Sales|Quantity|Discount|Profit|\n",
      "+-----+--------------------+--------------------+--------+--------+----------+------------+-------+--------+-----+----------+------+---------+--------+------------+-----------+-----+--------+--------+------+\n",
      "| null|                null|                null|    null|    null|      null|        null|   null|    null| null|      null|  null|     null|    null|        null|       null| null|    null|    null|  null|\n",
      "| null|Henderson|Kentuck...|                null|    null|    null|      null|        null|   null|    null| null|      null|  null|     null|    null|        null|       null| null|    null|    null|  null|\n",
      "| null|Henderson|Kentuck...| Rounded Back|731...|    null|    null|      null|        null|   null|    null| null|      null|  null|     null|    null|        null|       null| null|    null|    null|  null|\n",
      "+-----+--------------------+--------------------+--------+--------+----------+------------+-------+--------+-----+----------+------+---------+--------+------------+-----------+-----+--------+--------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.schema(s).csv(f'{inputs}').coalesce(1).write.csv(f'{output}',sep='|',mode='overwrite')\n",
    "spark.read.schema(s).csv(f'{output}').show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----------+----------+------------+----------+---------------+---------+--------------------+----------+----------+------+---------------+---------------+------------+--------------------+------+--------+--------+-------+\n",
      "|RowID|       OrderID| OrderDate|  ShipDate|    ShipMode|CustomerID|   CustomerName|  Segment|            Location|     State|PostalCode|Region|      ProductID|       Category|Sub_Category|         ProductName| Sales|Quantity|Discount| Profit|\n",
      "+-----+--------------+----------+----------+------------+----------+---------------+---------+--------------------+----------+----------+------+---------------+---------------+------------+--------------------+------+--------+--------+-------+\n",
      "|    1|CA-2016-152156|15/04/2017|1101102016|Second Class|  CG-12520|    Claire Gute| Consumer|United States,Hen...|  Kentucky|     42420| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|261.96|       2|       0|41.9136|\n",
      "|    2|CA-2016-152156|15/04/2018|1101102016|Second Class|  CG-12520|    Claire Gute| Consumer|United States,Hen...|  Kentucky|     42420| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|731.94|       3|       0|219.582|\n",
      "|    3|CA-2016-138688|15/04/2019| 601602016|Second Class|  DV-13045|Darrin Van Huff|Corporate|United States,Los...|California|     90036|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...| 14.62|       2|       0| 6.8714|\n",
      "+-----+--------------+----------+----------+------------+----------+---------------+---------+--------------------+----------+----------+------+---------------+---------------+------------+--------------------+------+--------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.schema(s).csv(f'{output}',sep='|',header=False)\n",
    "df.filter ('RowID is not null').show(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Source_data/parquet/order_tab/'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note::\\\n",
    "> If you dont have anyone to tell you what is tha fileformate you can read the parquet file as text and then see into it what type of formate will be suitable for this.\\\n",
    "> when we write the parquet data it is by default compressed by snappy algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                             |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Row ID,Order ID,Order Date,Ship Date,Ship Mode,Customer ID,Customer Name,Segment,Location,State,Postal Code,Region,Product ID,Category,Sub-Category,Product Name,Sales,Quantity,Discount,Profit                                                   |\n",
      "|1,CA-2016-152156,15/04/2017,1101102016,Second Class,CG-12520,Claire Gute,Consumer,\"United States,Henderson\",Kentucky,42420,South,FUR-BO-10001798,Furniture,Bookcases,Bush Somerset Collection Bookcase,261.96,2,0,41.9136                         |\n",
      "|2,CA-2016-152156,15/04/2018,1101102016,Second Class,CG-12520,Claire Gute,Consumer,\"United States,Henderson\",Kentucky,42420,South,FUR-CH-10000454,Furniture,Chairs,\"Hon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back\",731.94,3,0,219.582|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text(inputs).show(truncate=0,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                         |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|\"\"|Order ID|Order Date|Ship Date|Ship Mode|Customer ID|Customer Name|Segment|Location|State|Postal Code|Region|Product ID|Category|Sub-Category|Product Name|Sales|\"\"|\"\"|Profit                                                               |\n",
      "|1|CA-2016-152156|15/04/2017|1101102016|Second Class|CG-12520|Claire Gute|Consumer|United States,Henderson|Kentucky|42420|South|FUR-BO-10001798|Furniture|Bookcases|Bush Somerset Collection Bookcase|261.96|2|0|41.9136                       |\n",
      "|2|CA-2016-152156|15/04/2018|1101102016|Second Class|CG-12520|Claire Gute|Consumer|United States,Henderson|Kentucky|42420|South|FUR-CH-10000454|Furniture|Chairs|Hon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back|731.94|3|0|219.582|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text(output).show(truncate=0,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                                       |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"customer_id\":1,\"customer_fname\":\"Richard\",\"customer_lname\":\"Hernandez\",\"customer_email\":\"XXXXXXXXX\",\"customer_password\":\"XXXXXXXXX\",\"customer_street\":\"6303 Heather Plaza\",\"customer_city\":\"Brownsville\",\"customer_state\":\"TX\",\"customer_zipcode\":\"78521\"}|\n",
      "|{\"customer_id\":2,\"customer_fname\":\"Mary\",\"customer_lname\":\"Barrett\",\"customer_email\":\"XXXXXXXXX\",\"customer_password\":\"XXXXXXXXX\",\"customer_street\":\"9526 Noble Embers Ridge\",\"customer_city\":\"Littleton\",\"customer_state\":\"CO\",\"customer_zipcode\":\"80126\"}  |\n",
      "|{\"customer_id\":3,\"customer_fname\":\"Ann\",\"customer_lname\":\"Smith\",\"customer_email\":\"XXXXXXXXX\",\"customer_password\":\"XXXXXXXXX\",\"customer_street\":\"3422 Blue Pioneer Bend\",\"customer_city\":\"Caguas\",\"customer_state\":\"PR\",\"customer_zipcode\":\"00725\"}         |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text('../Source_data/retail_db_json/customers/').show(truncate=0,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+--------------+-----------+--------------+-----------------+--------------+-----------------------+----------------+\n",
      "|customer_city|customer_email|customer_fname|customer_id|customer_lname|customer_password|customer_state|customer_street        |customer_zipcode|\n",
      "+-------------+--------------+--------------+-----------+--------------+-----------------+--------------+-----------------------+----------------+\n",
      "|Brownsville  |XXXXXXXXX     |Richard       |1          |Hernandez     |XXXXXXXXX        |TX            |6303 Heather Plaza     |78521           |\n",
      "|Littleton    |XXXXXXXXX     |Mary          |2          |Barrett       |XXXXXXXXX        |CO            |9526 Noble Embers Ridge|80126           |\n",
      "|Caguas       |XXXXXXXXX     |Ann           |3          |Smith         |XXXXXXXXX        |PR            |3422 Blue Pioneer Bend |00725           |\n",
      "+-------------+--------------+--------------+-----------+--------------+-----------------+--------------+-----------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json('../Source_data/retail_db_json/customers/').show(truncate=0,n=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Dataframe from CSV**\n",
    "* Aproach 1 :: spark.read.csv('path')\n",
    "* Aproach 2 :: spark.read.format('csv')\n",
    "* We can specify schema via String of column and data type or by struct type\n",
    "* Spark read the data via spark dataframe reader which is spark.read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2', '_c3']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.read.format('csv').load('../Source_data/retail_db_json/orders/')\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2', '_c3']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.read.csv('../Source_data/retail_db_json/orders/')\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('_c1', 'string'), ('_c2', 'string'), ('_c3', 'string')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "courses_df has\n",
      "\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n",
      "users_df has\n",
      "\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "\n",
      "course_enrolments_df has\n",
      "\n",
      "+-------------------+-------+---------+----------+\n",
      "|course_enrolment_id|user_id|course_id|price_paid|\n",
      "+-------------------+-------+---------+----------+\n",
      "|                  1|     10|        2|      9.99|\n",
      "|                  2|      5|        2|      9.99|\n",
      "|                  3|      7|        5|     10.99|\n",
      "|                  4|      9|        2|      9.99|\n",
      "|                  5|      8|        2|      9.99|\n",
      "|                  6|      5|        5|     10.99|\n",
      "|                  7|      4|        5|     10.99|\n",
      "|                  8|      7|        3|     10.99|\n",
      "|                  9|      8|        5|     10.99|\n",
      "|                 10|      3|        3|     10.99|\n",
      "|                 11|      7|        5|     10.99|\n",
      "|                 12|      3|        2|      9.99|\n",
      "|                 13|      5|        2|      9.99|\n",
      "|                 14|      4|        3|     10.99|\n",
      "|                 15|      8|        2|      9.99|\n",
      "+-------------------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./joins_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df = spark.createDataFrame([Row(**course) for course in courses])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **USE CASE**\n",
    "> [Elie, Tim, Matt] is string in one column names you need to remove the first letter and make it Array of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+\n",
      "|            names|     name|\n",
      "+-----------------+---------+\n",
      "|[Elie, Tim, Matt]|[E, T, M]|\n",
      "+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=[{'names':['Elie','Tim','Matt']}]\n",
    "us=[Row(**i) for i in df]\n",
    "# us=[Row(*uss.values()) for uss in df]\n",
    "dfs=spark.createDataFrame(us)\n",
    "dfs.withColumn('name',split((concat_ws(',',substring(col('names')[0].cast('string'),0,1),substring(col('names')[1].cast('string'),0,1),substring(col('names')[2].cast('string'),0,1))),',')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    names|\n",
      "+---------+\n",
      "|[E, T, M]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.selectExpr(\n",
    "    '''split(concat(substring(cast(names[0] as string), 1, 1), ',', substring(cast(names[1] as string), 1, 1), ',', substring(cast(names[2] as string), 1, 1)),',') as names'''\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    names|\n",
      "+---------+\n",
      "|[E, T, M]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def splits(names):\n",
    "    return ([ i[0] for i in names])\n",
    "# splits=udf('splits',StringType())\n",
    "\n",
    "spark.udf.register('splits',splits)\n",
    "dfs.createOrReplaceTempView('dfs')\n",
    "spark.sql(\"\"\"select splits(names) as names  from dfs\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **USE CASE 2**\n",
    "* Remove the spaces from both sides of the words in column fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|  fruits|\n",
      "+--------+\n",
      "|  Apple |\n",
      "| Banana |\n",
      "| Cherry |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\" Apple \",), (\" Banana \",), (\" Cherry \",)]\n",
    "df = spark.createDataFrame(data, [\"fruits\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|  fruits|   abc|\n",
      "+--------+------+\n",
      "|  Apple | Apple|\n",
      "| Banana |Banana|\n",
      "| Cherry |Cherry|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('abc',expr('trim(Both ' ' from fruits)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-------------------+--------------+\n",
      "|  fruits|transformed column|lennth_after_transf|size of fruits|\n",
      "+--------+------------------+-------------------+--------------+\n",
      "|  Apple |             Apple|                  5|             7|\n",
      "| Banana |            Banana|                  6|             8|\n",
      "| Cherry |            Cherry|                  6|             8|\n",
      "+--------+------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('transformed column',expr('trim(LEADING ' ' from trim(trailing ' ' from fruits))')).withColumn('lennth_after_transf',length(expr('trim(LEADING ' ' from trim(trailing ' ' from fruits))'))).\\\n",
    "    withColumn('size of fruits',length(expr('fruits'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|  fruits|transformed column|\n",
      "+--------+------------------+\n",
      "|  Apple |             Apple|\n",
      "| Banana |            Banana|\n",
      "| Cherry |            Cherry|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('transformed column',expr(\"trim(LEADING ' ' from trim(trailing ' ' from fruits))\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **USE CASE 3**\n",
    "* Question 12 : Suppose you have a table called log_data with the following columns: log_id, user_id, action, and timestamp. Write a SQL query to calculate the number of actions performed by each user in the last 7 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+\n",
      "|log_id|user_id|action|          timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "|     1|    101| login|2023-09-05 08:30:00|\n",
      "|     2|    102| click|2023-09-06 12:45:00|\n",
      "|     3|    101| click|2023-09-07 14:15:00|\n",
      "|     4|    103| login|2023-09-08 09:00:00|\n",
      "|     5|    102|logout|2023-09-09 17:30:00|\n",
      "|     6|    101| click|2023-09-10 11:20:00|\n",
      "|     7|    103| click|2023-09-11 10:15:00|\n",
      "|     8|    102| click|2023-09-12 13:10:00|\n",
      "+------+-------+------+-------------------+\n",
      "\n",
      "root\n",
      " |-- log_id: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 101, 'login', '2023-09-05 08:30:00'),\n",
    "    (2, 102, 'click', '2023-09-06 12:45:00'),\n",
    "    (3, 101, 'click', '2023-09-07 14:15:00'),\n",
    "    (4, 103, 'login', '2023-09-08 09:00:00'),\n",
    "    (5, 102, 'logout', '2023-09-09 17:30:00'),\n",
    "    (6, 101, 'click', '2023-09-10 11:20:00'),\n",
    "    (7, 103, 'click', '2023-09-11 10:15:00'),\n",
    "    (8, 102, 'click', '2023-09-12 13:10:00')\n",
    "]\n",
    "\n",
    "columns = [\"log_id\", \"user_id\", \"action\", \"timestamp\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|user_id|counts|\n",
      "+-------+------+\n",
      "|    101|     3|\n",
      "|    102|     3|\n",
      "|    103|     2|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('dates', to_date(col('timestamp'))).withColumn('subs',date_sub('dates',7)).filter('dates>=subs').groupBy('user_id').agg(count(col('log_id')).alias('counts')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "courses_df has\n",
      "\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n",
      "users_df has\n",
      "\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "\n",
      "course_enrolments_df has\n",
      "\n",
      "+-------------------+-------+---------+----------+\n",
      "|course_enrolment_id|user_id|course_id|price_paid|\n",
      "+-------------------+-------+---------+----------+\n",
      "|                  1|     10|        2|      9.99|\n",
      "|                  2|      5|        2|      9.99|\n",
      "|                  3|      7|        5|     10.99|\n",
      "|                  4|      9|        2|      9.99|\n",
      "|                  5|      8|        2|      9.99|\n",
      "|                  6|      5|        5|     10.99|\n",
      "|                  7|      4|        5|     10.99|\n",
      "|                  8|      7|        3|     10.99|\n",
      "|                  9|      8|        5|     10.99|\n",
      "|                 10|      3|        3|     10.99|\n",
      "|                 11|      7|        5|     10.99|\n",
      "|                 12|      3|        2|      9.99|\n",
      "|                 13|      5|        2|      9.99|\n",
      "|                 14|      4|        3|     10.99|\n",
      "|                 15|      8|        2|      9.99|\n",
      "+-------------------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./joins_data.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **USE CASE**\n",
    "* You have list in a column and you need to find the even numbers and then make an array of that even numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|               num|\n",
      "+------------------+\n",
      "|[1, 2, 3, 4, 5, 6]|\n",
      "+------------------+\n",
      "\n",
      "root\n",
      " |-- num: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num=[{\"num\":[1,2,3,4,5,6]}]\n",
    "nu=[Row(**i) for i in num]\n",
    "# nu\n",
    "df=spark.createDataFrame(nu)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      num|\n",
      "+---------+\n",
      "|[2, 4, 6]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### method1 using high order functions\n",
    "df.selectExpr ('filter(num ,n -> n % 2 == 0) as num').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      num|\n",
      "+---------+\n",
      "|[2, 4, 6]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### method2:: using dataframe api\n",
    "df.withColumn('num',expr('filter (num , n -> n % 2 == 0)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  num\n",
       "0  [1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### method 3::: by pandas\n",
    "dp=df.toPandas()\n",
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2, 4, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         num\n",
       "0  [2, 4, 6]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filt(num):\n",
    "    return [nu for nu in num if nu%2==0]\n",
    "dp['num']=dp['num'].apply(filt)\n",
    "dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+\n",
      "|user_id|activity|activity_date|\n",
      "+-------+--------+-------------+\n",
      "|      1|   login|   2019-05-01|\n",
      "|      1|homepage|   2019-05-01|\n",
      "|      1|  logout|   2019-05-01|\n",
      "|      2|   login|   2019-06-21|\n",
      "|      2|  logout|   2019-06-21|\n",
      "|      3|   login|   2019-01-01|\n",
      "|      3|    jobs|   2019-01-01|\n",
      "|      3|  logout|   2019-01-01|\n",
      "|      4|   login|   2019-06-21|\n",
      "|      4|  groups|   2019-06-21|\n",
      "|      4|  logout|   2019-06-21|\n",
      "|      5|   login|   2019-03-01|\n",
      "|      5|  logout|   2019-03-01|\n",
      "|      5|   login|   2019-06-21|\n",
      "|      5|  logout|   2019-06-21|\n",
      "+-------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lis=[( 1 , 'login'    , '2019-05-01'),\n",
    "( 1 , 'homepage' , '2019-05-01'),\n",
    "( 1 , 'logout'   , '2019-05-01'),\n",
    "( 2 , 'login'   , '2019-06-21') ,\n",
    "( 2 , 'logout'   , '2019-06-21'),\n",
    "( 3 , 'login'    , '2019-01-01'),\n",
    "( 3 , 'jobs'     , '2019-01-01'),\n",
    "( 3 , 'logout'   , '2019-01-01'),\n",
    "( 4 , 'login'    , '2019-06-21'),\n",
    "( 4 , 'groups'   , '2019-06-21'),\n",
    "( 4 , 'logout'   , '2019-06-21'),\n",
    "( 5 , 'login'    , '2019-03-01'),\n",
    "( 5 , 'logout'   , '2019-03-01'),\n",
    "( 5 , 'login'    , '2019-06-21'),\n",
    "( 5 , 'logout'   , '2019-06-21')]\n",
    "sc='''\n",
    "user_id int , activity string , activity_date string\n",
    "'''\n",
    "df=spark.createDataFrame(lis,sc)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+----------+\n",
      "|user_id|activity|activity_date|      next|\n",
      "+-------+--------+-------------+----------+\n",
      "|      1|   login|   2019-05-01|2019-04-01|\n",
      "|      2|   login|   2019-06-21|2019-04-01|\n",
      "|      4|   login|   2019-06-21|2019-04-01|\n",
      "|      5|   login|   2019-06-21|2019-04-01|\n",
      "+-------+--------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('next',date_sub(lit('2019-06-30'),90)).filter(\"activity =='login' and  activity_date >= next and activity_date <= '2019-06-30'\").show()\n",
    "                                                            # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asfand saeed\n",
      "saeed asfand\n"
     ]
    }
   ],
   "source": [
    "class ES:\n",
    "    def __init__(self,first,last,pay):\n",
    "        self.first=first\n",
    "        self.last=last\n",
    "        self.pay=pay\n",
    "        self.email='{} {}'.format(self.first,self.last)\n",
    "    def fullname(self):\n",
    "        return '{} {}'.format(self.first,self.last)\n",
    "em1=ES('asfand','saeed',33)\n",
    "em2=ES('saeed','asfand',44)\n",
    "print(em1.fullname())\n",
    "print(em2.fullname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "34\n",
      "51\n",
      "1.5\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "class ES:\n",
    "    total_emp=0\n",
    "    inc=1.04\n",
    "    def __init__ (self,first,last,pay):\n",
    "        self.first=first\n",
    "        self.last=last\n",
    "        self.pay=pay\n",
    "        self.email='{} {}'.format(self.first,self.last)\n",
    "        ES.total_emp+=1\n",
    "    def fullname(self):\n",
    "        return '{} {}'.format(self.first,self.last)\n",
    "    def pays(self):\n",
    "        self.pay=int(self.pay * self.inc)\n",
    "em1=ES('asfand','saeed',33)\n",
    "em2=ES('saeed','asfand',44)\n",
    "print(em1.pay)    \n",
    "em1.pays()\n",
    "print(em1.pay) \n",
    "em1.inc=1.5\n",
    "em1.pays()\n",
    "print(em1.pay)\n",
    "ES.inc=1.13\n",
    "print(em1.inc)\n",
    "em1.pays()\n",
    "print(em1.pay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **USE CASE**\n",
    "* create a script that will compare two CSV files to find out what is major changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "courses_df has\n",
      "\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n",
      "users_df has\n",
      "\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "\n",
      "course_enrolments_df has\n",
      "\n",
      "+-------------------+-------+---------+----------+\n",
      "|course_enrolment_id|user_id|course_id|price_paid|\n",
      "+-------------------+-------+---------+----------+\n",
      "|                  1|     10|        2|      9.99|\n",
      "|                  2|      5|        2|      9.99|\n",
      "|                  3|      7|        5|     10.99|\n",
      "|                  4|      9|        2|      9.99|\n",
      "|                  5|      8|        2|      9.99|\n",
      "|                  6|      5|        5|     10.99|\n",
      "|                  7|      4|        5|     10.99|\n",
      "|                  8|      7|        3|     10.99|\n",
      "|                  9|      8|        5|     10.99|\n",
      "|                 10|      3|        3|     10.99|\n",
      "|                 11|      7|        5|     10.99|\n",
      "|                 12|      3|        2|      9.99|\n",
      "|                 13|      5|        2|      9.99|\n",
      "|                 14|      4|        3|     10.99|\n",
      "|                 15|      8|        2|      9.99|\n",
      "+-------------------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./joins_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "path1='../Source_data/use_case_tables_verification/51prod_ld.csv'\n",
    "path2='../Source_data/use_case_tables_verification/5111prod_my_query.csv'\n",
    "\n",
    "path3='../Source_data/use_case_tables_verification/59prod_ld.csv'\n",
    "path4='../Source_data/use_case_tables_verification/59prod_my_query.csv'\n",
    "\n",
    "path5='../Source_data/use_case_tables_verification/42prod_ld.csv'\n",
    "path6='../Source_data/use_case_tables_verification/42prod_my_query.csv'\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "field Operator: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asfandyar\\Desktop\\python_Asfandyar\\40 days\\chilla\\Spark\\Certification Spark\\Pyspark-for-Data-Analyst\\Pyspark\\Code\\Spark_Beginner_to_advance.ipynb Cell 651\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z2151sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m path1\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../Source_data/use_case_tables_verification/Operator ID Mapping.xlsx\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z2151sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dp\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mread_excel(path1)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z2151sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df\u001b[39m=\u001b[39mspark\u001b[39m.\u001b[39;49mcreateDataFrame(dp)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z2151sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:673\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    670\u001b[0m     has_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m    672\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SparkSession, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcreateDataFrame(\n\u001b[0;32m    674\u001b[0m         data, schema, samplingRatio, verifySchema)\n\u001b[0;32m    675\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:340\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    339\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[1;32m--> 340\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:700\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    698\u001b[0m     rdd, schema \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m    699\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 700\u001b[0m     rdd, schema \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[0;32m    701\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n\u001b[0;32m    702\u001b[0m jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsparkSession\u001b[39m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[39m.\u001b[39mrdd(), schema\u001b[39m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:512\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    509\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data)\n\u001b[0;32m    511\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 512\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchemaFromList(data, names\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m    513\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    514\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(converter, data)\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:439\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[1;34m(self, data, names)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[0;32m    438\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcan not infer schema from empty dataset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 439\u001b[0m schema \u001b[39m=\u001b[39m reduce(_merge_type, (_infer_schema(row, names) \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m data))\n\u001b[0;32m    440\u001b[0m \u001b[39mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m    441\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSome of types cannot be determined after inferring\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\types.py:1109\u001b[0m, in \u001b[0;36m_merge_type\u001b[1;34m(a, b, name)\u001b[0m\n\u001b[0;32m   1107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n\u001b[0;32m   1108\u001b[0m     nfs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((f\u001b[39m.\u001b[39mname, f\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m b\u001b[39m.\u001b[39mfields)\n\u001b[1;32m-> 1109\u001b[0m     fields \u001b[39m=\u001b[39m [StructField(f\u001b[39m.\u001b[39mname, _merge_type(f\u001b[39m.\u001b[39mdataType, nfs\u001b[39m.\u001b[39mget(f\u001b[39m.\u001b[39mname, NullType()),\n\u001b[0;32m   1110\u001b[0m                                               name\u001b[39m=\u001b[39mnew_name(f\u001b[39m.\u001b[39mname)))\n\u001b[0;32m   1111\u001b[0m               \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mfields]\n\u001b[0;32m   1112\u001b[0m     names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([f\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fields])\n\u001b[0;32m   1113\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nfs:\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\types.py:1109\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n\u001b[0;32m   1108\u001b[0m     nfs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((f\u001b[39m.\u001b[39mname, f\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m b\u001b[39m.\u001b[39mfields)\n\u001b[1;32m-> 1109\u001b[0m     fields \u001b[39m=\u001b[39m [StructField(f\u001b[39m.\u001b[39mname, _merge_type(f\u001b[39m.\u001b[39;49mdataType, nfs\u001b[39m.\u001b[39;49mget(f\u001b[39m.\u001b[39;49mname, NullType()),\n\u001b[0;32m   1110\u001b[0m                                               name\u001b[39m=\u001b[39;49mnew_name(f\u001b[39m.\u001b[39;49mname)))\n\u001b[0;32m   1111\u001b[0m               \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mfields]\n\u001b[0;32m   1112\u001b[0m     names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([f\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fields])\n\u001b[0;32m   1113\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nfs:\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\types.py:1104\u001b[0m, in \u001b[0;36m_merge_type\u001b[1;34m(a, b, name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[39mreturn\u001b[39;00m a\n\u001b[0;32m   1102\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(a) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mtype\u001b[39m(b):\n\u001b[0;32m   1103\u001b[0m     \u001b[39m# TODO: type cast (such as int -> long)\u001b[39;00m\n\u001b[1;32m-> 1104\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(new_msg(\u001b[39m\"\u001b[39m\u001b[39mCan not merge type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(a), \u001b[39mtype\u001b[39m(b))))\n\u001b[0;32m   1106\u001b[0m \u001b[39m# same type\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n",
      "\u001b[1;31mTypeError\u001b[0m: field Operator: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
     ]
    }
   ],
   "source": [
    "path1='../Source_data/use_case_tables_verification/Operator ID Mapping.xlsx'\n",
    "dp=pd.read_excel(path1)\n",
    "df=spark.createDataFrame(dp)\n",
    "df\n",
    "# df.withColumn('news',expr('trim '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame size: 73153.00 KB\n"
     ]
    }
   ],
   "source": [
    "dp=pd.read_csv(path1,header=1)\n",
    "memory_bytes = dp.memory_usage(deep=True).sum()\n",
    "\n",
    "# Convert bytes to gigabytes\n",
    "memory_gb = memory_bytes\n",
    "\n",
    "# Print the result\n",
    "print(f\"DataFrame size: {memory_gb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------------------------------+--------------+--------+------------------------------------+--------------------------+-------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+---------+------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE                        |PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME                 |COMMERCIAL_OFFER_NAME     |OFFER_GROUP_NAME   |PRODUCT_TYPE|PRODUCT_SUB_TYPE |BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO     |FILE_NAME|source|\n",
      "+--------+----------------+------------------------------------+--------------+--------+------------------------------------+--------------------------+-------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+---------+------+\n",
      "|8324    |8324            |BUNDLEGIFT-WEEKLY EXTREME PLUS-360-7|REMARKS       |0       |BUNDLEGIFT-WEEKLY EXTREME PLUS-360-7|WEEKLY EXTREME PLUS - GIFT|WEEKLY EXTREME PLUS|HYBRID      |HEAVY DATA HYBRID|7              |DISCOUNTED|1.0000  |PRINCIPLE      |0            |NOT APPLICABLE|null     |51    |\n",
      "|8455    |8455            |BUNDLEGIFT-WEEKLY EXTREME PLUS-405-7|REMARKS       |0       |BUNDLEGIFT-WEEKLY EXTREME PLUS-405-7|WEEKLY EXTREME PLUS - GIFT|WEEKLY EXTREME PLUS|HYBRID      |HEAVY DATA HYBRID|7              |DISCOUNTED|1.0000  |PRINCIPLE      |0            |NOT APPLICABLE|null     |51    |\n",
      "|8390    |8390            |BUNDLEGIFT-WEEKLY EXTREME PLUS-320-7|REMARKS       |0       |BUNDLEGIFT-WEEKLY EXTREME PLUS-320-7|WEEKLY EXTREME PLUS - GIFT|WEEKLY EXTREME PLUS|HYBRID      |HEAVY DATA HYBRID|7              |DISCOUNTED|1.0000  |PRINCIPLE      |0            |NOT APPLICABLE|null     |51    |\n",
      "+--------+----------------+------------------------------------+--------------+--------+------------------------------------+--------------------------+-------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+---------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('PROD_KEY', 'string'),\n",
       " ('DURABLE_PROD_KEY', 'string'),\n",
       " ('PRODUCT_CODE', 'string'),\n",
       " ('PRODUCT_FAMILY', 'string'),\n",
       " ('SUB_ACCT', 'string'),\n",
       " ('PROD_TECHNICAL_NAME', 'string'),\n",
       " ('COMMERCIAL_OFFER_NAME', 'string'),\n",
       " ('OFFER_GROUP_NAME', 'string'),\n",
       " ('PRODUCT_TYPE', 'string'),\n",
       " ('PRODUCT_SUB_TYPE', 'string'),\n",
       " ('BUNDLE_VALIDITY', 'string'),\n",
       " ('ADDON_TYPE', 'string'),\n",
       " ('TP_SHARE', 'string'),\n",
       " ('PRINCIPLE_AGENT', 'string'),\n",
       " ('DIGITAL_PRICE', 'string'),\n",
       " ('PORTFOLIO', 'string'),\n",
       " ('FILE_NAME', 'string'),\n",
       " ('source', 'string')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=spark.read.csv(path2,header=True)\n",
    "df2.show(truncate=0,n=3)\n",
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROD_KEY',\n",
       " 'DURABLE_PROD_KEY',\n",
       " 'PRODUCT_CODE',\n",
       " 'PRODUCT_FAMILY',\n",
       " 'SUB_ACCT',\n",
       " 'PROD_TECHNICAL_NAME',\n",
       " 'COMMERCIAL_OFFER_NAME',\n",
       " 'OFFER_GROUP_NAME',\n",
       " 'PRODUCT_TYPE',\n",
       " 'PRODUCT_SUB_TYPE',\n",
       " 'BUNDLE_VALIDITY',\n",
       " 'ADDON_TYPE',\n",
       " 'TP_SHARE',\n",
       " 'PRINCIPLE_AGENT',\n",
       " 'DIGITAL_PRICE',\n",
       " 'PORTFOLIO',\n",
       " 'source',\n",
       " 'FILE_NAME']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols2=['PROD_KEY',\n",
    " 'DURABLE_PROD_KEY',\n",
    " 'PRODUCT_CODE',\n",
    " 'PRODUCT_FAMILY',\n",
    " 'SUB_ACCT',\n",
    " 'PROD_TECHNICAL_NAME',\n",
    " 'COMMERCIAL_OFFER_NAME',\n",
    " 'OFFER_GROUP_NAME',\n",
    " 'PRODUCT_TYPE',\n",
    " 'PRODUCT_SUB_TYPE',\n",
    " 'BUNDLE_VALIDITY',\n",
    " 'ADDON_TYPE',\n",
    " 'TP_SHARE',\n",
    " 'PRINCIPLE_AGENT',\n",
    " 'DIGITAL_PRICE',\n",
    " 'PORTFOLIO',\n",
    " 'source',\n",
    " 'FILE_NAME']\n",
    "cols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------------------+--------------+--------+--------------------+---------------------+-------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+------+---------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|        PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT| PROD_TECHNICAL_NAME|COMMERCIAL_OFFER_NAME|   OFFER_GROUP_NAME|PRODUCT_TYPE| PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|     PORTFOLIO|source|FILE_NAME|\n",
      "+--------+----------------+--------------------+--------------+--------+--------------------+---------------------+-------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+------+---------+\n",
      "|    8324|            8324|BUNDLEGIFT-WEEKLY...|       REMARKS|       0|BUNDLEGIFT-WEEKLY...| WEEKLY EXTREME PL...|WEEKLY EXTREME PLUS|      HYBRID|HEAVY DATA HYBRID|              7|DISCOUNTED|  1.0000|      PRINCIPLE|            0|NOT APPLICABLE|    51|     null|\n",
      "|    8455|            8455|BUNDLEGIFT-WEEKLY...|       REMARKS|       0|BUNDLEGIFT-WEEKLY...| WEEKLY EXTREME PL...|WEEKLY EXTREME PLUS|      HYBRID|HEAVY DATA HYBRID|              7|DISCOUNTED|  1.0000|      PRINCIPLE|            0|NOT APPLICABLE|    51|     null|\n",
      "|    8390|            8390|BUNDLEGIFT-WEEKLY...|       REMARKS|       0|BUNDLEGIFT-WEEKLY...| WEEKLY EXTREME PL...|WEEKLY EXTREME PLUS|      HYBRID|HEAVY DATA HYBRID|              7|DISCOUNTED|  1.0000|      PRINCIPLE|            0|NOT APPLICABLE|    51|     null|\n",
      "+--------+----------------+--------------------+--------------+--------+--------------------+---------------------+-------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df22=df2.select (cols2)\n",
    "df22.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+---------------------------------------+--------------+--------+---------------------------------------+---------------------------+--------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+---------------------+-------------+------------+-----------------+------+--------+-------------+-------------+---------+\n",
      "|prod_key|durable_prod_key|product_code                           |product_family|sub_acct|prod_technical_name                    |commercial_offer_name      |offer_group_name    |product_type|product_sub_type |bundle_validity|addon_type|tp_share|principle_agent|digital_price|portfolio     |start_date_time      |end_date_time|created_date|last_updated_date|source|batch_id|dml_ins_batch|dml_upd_batch|file_name|\n",
      "+--------+----------------+---------------------------------------+--------------+--------+---------------------------------------+---------------------------+--------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+---------------------+-------------+------------+-----------------+------+--------+-------------+-------------+---------+\n",
      "|8482    |8482            |PL_MONTHLY EXTREME PLUS_NWD_1250_10_150|REMARKS       |0       |PL_MONTHLY EXTREME PLUS_NWD_1250_10_150|MONTHLY EXTREME PLUS_PLDISC|MONTHLY EXTREME PLUS|HYBRID      |HEAVY DATA HYBRID|30             |DISCOUNTED|1       |PRINCIPLE      |0            |NOT APPLICABLE|1990-01-01 00:00:00.0|null         |2023-09-14  |null             |51    |1       |1            |1            |null     |\n",
      "|8472    |8472            |PL_EASY CARD MEGA PLUS_NWD_320_10_90   |REMARKS       |0       |PL_EASY CARD MEGA PLUS_NWD_320_10_90   |EASY CARD MEGA PLUS_PLDISC |EASY CARD MEGA PLUS |HYBRID      |HEAVY DATA HYBRID|7              |DISCOUNTED|1       |PRINCIPLE      |0            |NOT APPLICABLE|1990-01-01 00:00:00.0|null         |2023-09-14  |null             |51    |1       |1            |1            |null     |\n",
      "|8391    |8391            |BUNDLEGIFT-WEEKLY SOCIAL PACK-90-7     |REMARKS       |0       |BUNDLEGIFT-WEEKLY SOCIAL PACK-90-7     |WEEKLY SOCIAL PACK - GIFT  |WEEKLY SOCIAL PACK  |HYBRID      |HEAVY DATA HYBRID|7              |DISCOUNTED|1.0000  |PRINCIPLE      |0            |NOT APPLICABLE|1990-01-01 00:00:00.0|null         |2023-09-14  |null             |51    |1       |1            |1            |null     |\n",
      "+--------+----------------+---------------------------------------+--------------+--------+---------------------------------------+---------------------------+--------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+---------------------+-------------+------------+-----------------+------+--------+-------------+-------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('prod_key', 'string'),\n",
       " ('durable_prod_key', 'string'),\n",
       " ('product_code', 'string'),\n",
       " ('product_family', 'string'),\n",
       " ('sub_acct', 'string'),\n",
       " ('prod_technical_name', 'string'),\n",
       " ('commercial_offer_name', 'string'),\n",
       " ('offer_group_name', 'string'),\n",
       " ('product_type', 'string'),\n",
       " ('product_sub_type', 'string'),\n",
       " ('bundle_validity', 'string'),\n",
       " ('addon_type', 'string'),\n",
       " ('tp_share', 'string'),\n",
       " ('principle_agent', 'string'),\n",
       " ('digital_price', 'string'),\n",
       " ('portfolio', 'string'),\n",
       " ('start_date_time', 'string'),\n",
       " ('end_date_time', 'string'),\n",
       " ('created_date', 'string'),\n",
       " ('last_updated_date', 'string'),\n",
       " ('source', 'string'),\n",
       " ('batch_id', 'string'),\n",
       " ('dml_ins_batch', 'string'),\n",
       " ('dml_upd_batch', 'string'),\n",
       " ('file_name', 'string')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=spark.read.csv('../Source_data/use_case_tables_verification/511prod_ld.csv',header=True)\n",
    "df1.show(truncate=0,n=3)\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prod_key',\n",
       " 'durable_prod_key',\n",
       " 'product_code',\n",
       " 'product_family',\n",
       " 'sub_acct',\n",
       " 'prod_technical_name',\n",
       " 'commercial_offer_name',\n",
       " 'offer_group_name',\n",
       " 'product_type',\n",
       " 'product_sub_type',\n",
       " 'bundle_validity',\n",
       " 'addon_type',\n",
       " 'tp_share',\n",
       " 'principle_agent',\n",
       " 'digital_price',\n",
       " 'portfolio',\n",
       " 'source',\n",
       " 'file_name']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols1=['prod_key',\n",
    " 'durable_prod_key',\n",
    " 'product_code',\n",
    " 'product_family',\n",
    " 'sub_acct',\n",
    " 'prod_technical_name',\n",
    " 'commercial_offer_name',\n",
    " 'offer_group_name',\n",
    " 'product_type',\n",
    " 'product_sub_type',\n",
    " 'bundle_validity',\n",
    " 'addon_type',\n",
    " 'tp_share',\n",
    " 'principle_agent',\n",
    " 'digital_price',\n",
    " 'portfolio',\n",
    " 'source',\n",
    " 'file_name']\n",
    "cols1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+---------------------------------------+--------------+--------+---------------------------------------+---------------------------+--------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+------+---------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE                           |PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME                    |COMMERCIAL_OFFER_NAME      |OFFER_GROUP_NAME    |PRODUCT_TYPE|PRODUCT_SUB_TYPE |BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO     |source|FILE_NAME|\n",
      "+--------+----------------+---------------------------------------+--------------+--------+---------------------------------------+---------------------------+--------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+------+---------+\n",
      "|8482    |8482            |PL_MONTHLY EXTREME PLUS_NWD_1250_10_150|REMARKS       |0       |PL_MONTHLY EXTREME PLUS_NWD_1250_10_150|MONTHLY EXTREME PLUS_PLDISC|MONTHLY EXTREME PLUS|HYBRID      |HEAVY DATA HYBRID|30             |DISCOUNTED|1       |PRINCIPLE      |0            |NOT APPLICABLE|51    |null     |\n",
      "|8472    |8472            |PL_EASY CARD MEGA PLUS_NWD_320_10_90   |REMARKS       |0       |PL_EASY CARD MEGA PLUS_NWD_320_10_90   |EASY CARD MEGA PLUS_PLDISC |EASY CARD MEGA PLUS |HYBRID      |HEAVY DATA HYBRID|7              |DISCOUNTED|1       |PRINCIPLE      |0            |NOT APPLICABLE|51    |null     |\n",
      "|8391    |8391            |BUNDLEGIFT-WEEKLY SOCIAL PACK-90-7     |REMARKS       |0       |BUNDLEGIFT-WEEKLY SOCIAL PACK-90-7     |WEEKLY SOCIAL PACK - GIFT  |WEEKLY SOCIAL PACK  |HYBRID      |HEAVY DATA HYBRID|7              |DISCOUNTED|1.0000  |PRINCIPLE      |0            |NOT APPLICABLE|51    |null     |\n",
      "+--------+----------------+---------------------------------------+--------------+--------+---------------------------------------+---------------------------+--------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df11=df1.select (cols1).toDF(*cols2)\n",
    "df11.show(truncate=0,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME|COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO|source|FILE_NAME|\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "|7761    |7761            |PRODUCT_CODE|REMARKS       |0       |PRODUCT_CODE       |COMMERCIAL OFFER NAME|OFFER GROUP NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE      |null         |PORTFOLIO|51    |null     |\n",
      "|7761    |7761            |PRODUCT_CODE|REMARKS       |0       |PRODUCT_CODE       |COMMERCIAL OFFER NAME|OFFER GROUP NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE      |DIGITAL_PRICE|PORTFOLIO|51    |null     |\n",
      "|5425    |5425            |PROD_CODE   |REMARKS       |0       |PROD_CODE          |COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE      |null         |PORTFOLIO|51    |null     |\n",
      "|5425    |5425            |PROD_CODE   |REMARKS       |0       |PROD_CODE          |COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE      |DIGITAL_PRICE|PORTFOLIO|51    |null     |\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dups=df11.union(df22).distinct().groupBy(col('PROD_KEY')).agg(count('*').alias('count')).orderBy(col('PROD_KEY').desc()).filter('count >1').select ('PROD_KEY')\n",
    "# dups.show()\n",
    "dupjoin1=df11.join (dups,'PROD_KEY',how='inner').orderBy(col('PROD_KEY').desc())\n",
    "dupjoin2=df22.join(dups,'PROD_KEY',how='inner').orderBy(col('PROD_KEY').desc())\n",
    "result=dupjoin1.join(dupjoin2,'PROD_KEY',how='inner').orderBy(col('PROD_KEY').desc())\n",
    "dupjoin1.union(dupjoin2).orderBy(col('PROD_KEY').desc()).show(truncate=0)\n",
    "# result.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dups' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asfandyar\\Desktop\\python_Asfandyar\\40 days\\chilla\\Spark\\Certification Spark\\Pyspark-for-Data-Analyst\\Pyspark\\Code\\Spark_Beginner_to_advance.ipynb Cell 634\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z1563sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dups\u001b[39m.\u001b[39mcount()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z1563sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dups\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dups' is not defined"
     ]
    }
   ],
   "source": [
    "dups.count()\n",
    "dups.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asfandyar\\Desktop\\python_Asfandyar\\40 days\\chilla\\Spark\\Certification Spark\\Pyspark-for-Data-Analyst\\Pyspark\\Code\\Spark_Beginner_to_advance.ipynb Cell 632\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z1603sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df11\u001b[39m.\u001b[39mselect(col(\u001b[39m'\u001b[39;49m\u001b[39mDIGITAL_PRICE\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mdistinct())\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "df11.select(col('DIGITAL_PRICE').distinct()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME|COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO|source|FILE_NAME|\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "|7761    |7761            |PRODUCT_CODE|REMARKS       |0       |PRODUCT_CODE       |COMMERCIAL OFFER NAME|OFFER GROUP NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE      |null         |PORTFOLIO|51    |null     |\n",
      "|5425    |5425            |PROD_CODE   |REMARKS       |0       |PROD_CODE          |COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE      |null         |PORTFOLIO|51    |null     |\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## system\n",
    "dupjoin1.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|DIGITAL_PRICE|\n",
      "+-------------+\n",
      "|            0|\n",
      "|         null|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df11.select ('DIGITAL_PRICE').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|DIGITAL_PRICE|\n",
      "+-------------+\n",
      "|            0|\n",
      "|DIGITAL_PRICE|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df22.select ('DIGITAL_PRICE').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME|COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO|source|FILE_NAME|\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "|7761    |7761            |PRODUCT_CODE|REMARKS       |0       |PRODUCT_CODE       |COMMERCIAL OFFER NAME|OFFER GROUP NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE      |DIGITAL_PRICE|PORTFOLIO|51    |null     |\n",
      "|5425    |5425            |PROD_CODE   |REMARKS       |0       |PROD_CODE          |COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE      |DIGITAL_PRICE|PORTFOLIO|51    |null     |\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##my query\n",
    "dupjoin2.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME|COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO|source|FILE_NAME|\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dupjoin2.filter(\"PRODUCT_FAMILY like '%%'\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second file for analysis::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "path3='../Source_data/use_case_tables_verification/59prod_ld.csv'\n",
    "path4='../Source_data/use_case_tables_verification/59prod_my_query.csv'\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+---------+------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME|COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE    |TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO|FILE_NAME|source|\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+---------+------+\n",
      "|7481    |7481            |SERVICE ID  |VAS_SERVICE_ID|0       |SERVICE ID         |COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE    |TP_SHARE|PRINCIPLE      |0            |PORTFOLIO|null     |59    |\n",
      "|8056    |8056            |99161       |VAS_SERVICE_ID|0       |99161              |VIDLY_RENEWAL        |VIDLY           |DIGITAL     |PARTNERSHIP     |1              |NOT APPLICABLE|1       |PRINCIPLE      |0            |CONTENT  |null     |59    |\n",
      "|8059    |8059            |99177       |VAS_SERVICE_ID|0       |99177              |VIDLY                |VIDLY           |DIGITAL     |PARTNERSHIP     |1              |NOT APPLICABLE|1       |PRINCIPLE      |0            |CONTENT  |null     |59    |\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+---------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+------+---------+\n",
      "|prod_key|durable_prod_key|product_code|product_family|sub_acct|prod_technical_name|commercial_offer_name|offer_group_name|product_type|product_sub_type|bundle_validity|    addon_type|tp_share|principle_agent|digital_price|portfolio|source|file_name|\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+------+---------+\n",
      "|    7481|            7481|  SERVICE ID|VAS_SERVICE_ID|       0|         SERVICE ID| COMMERCIAL_OFFER_...|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|    ADDON_TYPE|TP_SHARE|      PRINCIPLE|            0|PORTFOLIO|    59|     null|\n",
      "|    8056|            8056|       99161|VAS_SERVICE_ID|       0|              99161|        VIDLY_RENEWAL|           VIDLY|     DIGITAL|     PARTNERSHIP|              1|NOT APPLICABLE|       1|      PRINCIPLE|            0|  CONTENT|    59|     null|\n",
      "|    8059|            8059|       99177|VAS_SERVICE_ID|       0|              99177|                VIDLY|           VIDLY|     DIGITAL|     PARTNERSHIP|              1|NOT APPLICABLE|       1|      PRINCIPLE|            0|  CONTENT|    59|     null|\n",
      "|    8061|            8061|       99178|VAS_SERVICE_ID|       0|              99178|                VIDLY|           VIDLY|     DIGITAL|     PARTNERSHIP|              7|NOT APPLICABLE|       1|      PRINCIPLE|            0|  CONTENT|    59|     null|\n",
      "|    8062|            8062|       99163|VAS_SERVICE_ID|       0|              99163|        VIDLY_RENEWAL|           VIDLY|     DIGITAL|     PARTNERSHIP|              7|NOT APPLICABLE|       1|      PRINCIPLE|            0|  CONTENT|    59|     null|\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#my\n",
    "df4=spark.read.csv(path4,header=True)\n",
    "df4.show(truncate=0,n=3)\n",
    "df44=df4.select(cols1)\n",
    "df44.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+------+---------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME|COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE    |TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO|source|FILE_NAME|\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+------+---------+\n",
      "|7481    |7481            |SERVICE ID  |VAS_SERVICE_ID|0       |SERVICE ID         |COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE    |TP_SHARE|PRINCIPLE      |0            |PORTFOLIO|59    |null     |\n",
      "|8056    |8056            |99161       |VAS_SERVICE_ID|0       |99161              |VIDLY_RENEWAL        |VIDLY           |DIGITAL     |PARTNERSHIP     |1              |NOT APPLICABLE|1       |PRINCIPLE      |0            |CONTENT  |59    |null     |\n",
      "|8059    |8059            |99177       |VAS_SERVICE_ID|0       |99177              |VIDLY                |VIDLY           |DIGITAL     |PARTNERSHIP     |1              |NOT APPLICABLE|1       |PRINCIPLE      |0            |CONTENT  |59    |null     |\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#systetm\n",
    "df3=spark.read.csv(path3,header=True)\n",
    "# df3.show(truncate=0,n=3)\n",
    "# df3.count()\n",
    "df33=df3.select(cols1).toDF(*cols2)\n",
    "df33.show(truncate=0,n=3)\n",
    "# df3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.columns\n",
    "cols3=['prod_key',\n",
    " 'durable_prod_key',\n",
    " 'product_code',\n",
    " 'product_family',\n",
    " 'sub_acct',\n",
    " 'prod_technical_name',\n",
    " 'commercial_offer_name',\n",
    " 'offer_group_name',\n",
    " 'product_type',\n",
    " 'product_sub_type',\n",
    " 'bundle_validity',\n",
    " 'addon_type',\n",
    " 'tp_share',\n",
    " 'principle_agent',\n",
    " 'digital_price',\n",
    " 'portfolio',\n",
    " 'source',\n",
    " 'file_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+------+---------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME|COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE    |TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO|source|FILE_NAME|\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+------+---------+\n",
      "|7481    |7481            |SERVICE ID  |VAS_SERVICE_ID|0       |SERVICE ID         |COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE    |TP_SHARE|PRINCIPLE      |0            |PORTFOLIO|59    |null     |\n",
      "|8056    |8056            |99161       |VAS_SERVICE_ID|0       |99161              |VIDLY_RENEWAL        |VIDLY           |DIGITAL     |PARTNERSHIP     |1              |NOT APPLICABLE|1       |PRINCIPLE      |0            |CONTENT  |59    |null     |\n",
      "|8059    |8059            |99177       |VAS_SERVICE_ID|0       |99177              |VIDLY                |VIDLY           |DIGITAL     |PARTNERSHIP     |1              |NOT APPLICABLE|1       |PRINCIPLE      |0            |CONTENT  |59    |null     |\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4=spark.read.csv(path4,header=True)\n",
    "# df4.show(truncate=0,n=3)\n",
    "df44=df4.select (cols4)\n",
    "df44.show(truncate=0,n=3)\n",
    "# df4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME|COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO|source|FILE_NAME|\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dups3=df44.union(df33).distinct().groupBy(col('PROD_KEY')).agg(count('*').alias('count')).orderBy(col('PROD_KEY').desc()).filter('count >1').select ('PROD_KEY')\n",
    "# dups.show()\n",
    "dup3join44=df44.join (dups3,'PROD_KEY',how='inner').orderBy(col('PROD_KEY').desc())\n",
    "dup3join33=df33.join(dups3,'PROD_KEY',how='inner').orderBy(col('PROD_KEY').desc())\n",
    "result=dup3join44.join(dup3join33,'PROD_KEY',how='inner').orderBy(col('PROD_KEY').desc())\n",
    "dup3join33.union(dup3join44).orderBy(col('PROD_KEY').desc()).show(truncate=0)\n",
    "# result.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4.columns\n",
    "cols4=['PROD_KEY',\n",
    " 'DURABLE_PROD_KEY',\n",
    " 'PRODUCT_CODE',\n",
    " 'PRODUCT_FAMILY',\n",
    " 'SUB_ACCT',\n",
    " 'PROD_TECHNICAL_NAME',\n",
    " 'COMMERCIAL_OFFER_NAME',\n",
    " 'OFFER_GROUP_NAME',\n",
    " 'PRODUCT_TYPE',\n",
    " 'PRODUCT_SUB_TYPE',\n",
    " 'BUNDLE_VALIDITY',\n",
    " 'ADDON_TYPE',\n",
    " 'TP_SHARE',\n",
    " 'PRINCIPLE_AGENT',\n",
    " 'DIGITAL_PRICE',\n",
    " 'PORTFOLIO',\n",
    " 'source',\n",
    " 'FILE_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+------+---------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME|COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|    ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO|source|FILE_NAME|\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+------+---------+\n",
      "|    8062|            8062|       99163|VAS_SERVICE_ID|       0|              99163|        VIDLY_RENEWAL|           VIDLY|     DIGITAL|     PARTNERSHIP|              7|NOT APPLICABLE|       1|      PRINCIPLE|            0|  CONTENT|    59|     null|\n",
      "|    8062|            8062|       99163|VAS_SERVICE_ID|       0|              99163|        VIDLY_RENEWAL|           VIDLY|     DIGITAL|     PARTNERSHIP|              7|NOT APPLICABLE|       1|      PRINCIPLE|            0|  CONTENT|    59|     null|\n",
      "|    8061|            8061|       99178|VAS_SERVICE_ID|       0|              99178|                VIDLY|           VIDLY|     DIGITAL|     PARTNERSHIP|              7|NOT APPLICABLE|       1|      PRINCIPLE|            0|  CONTENT|    59|     null|\n",
      "|    8061|            8061|       99178|VAS_SERVICE_ID|       0|              99178|                VIDLY|           VIDLY|     DIGITAL|     PARTNERSHIP|              7|NOT APPLICABLE|       1|      PRINCIPLE|            0|  CONTENT|    59|     null|\n",
      "|    8059|            8059|       99177|VAS_SERVICE_ID|       0|              99177|                VIDLY|           VIDLY|     DIGITAL|     PARTNERSHIP|              1|NOT APPLICABLE|       1|      PRINCIPLE|            0|  CONTENT|    59|     null|\n",
      "|    8059|            8059|       99177|VAS_SERVICE_ID|       0|              99177|                VIDLY|           VIDLY|     DIGITAL|     PARTNERSHIP|              1|NOT APPLICABLE|       1|      PRINCIPLE|            0|  CONTENT|    59|     null|\n",
      "|    8056|            8056|       99161|VAS_SERVICE_ID|       0|              99161|        VIDLY_RENEWAL|           VIDLY|     DIGITAL|     PARTNERSHIP|              1|NOT APPLICABLE|       1|      PRINCIPLE|            0|  CONTENT|    59|     null|\n",
      "|    8056|            8056|       99161|VAS_SERVICE_ID|       0|              99161|        VIDLY_RENEWAL|           VIDLY|     DIGITAL|     PARTNERSHIP|              1|NOT APPLICABLE|       1|      PRINCIPLE|            0|  CONTENT|    59|     null|\n",
      "|    7481|            7481|  SERVICE ID|VAS_SERVICE_ID|       0|         SERVICE ID| COMMERCIAL_OFFER_...|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|    ADDON_TYPE|TP_SHARE|      PRINCIPLE|            0|PORTFOLIO|    59|     null|\n",
      "|    7481|            7481|  SERVICE ID|VAS_SERVICE_ID|       0|         SERVICE ID| COMMERCIAL_OFFER_...|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|    ADDON_TYPE|TP_SHARE|      PRINCIPLE|            0|PORTFOLIO|    59|     null|\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+--------------+--------+---------------+-------------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df44.join(df33,'PROD_KEY',how='inner').show()\n",
    "df44.union(df33).orderBy(col('PROD_KEY').desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Source_data/use_case_tables_verification/42prod_ld.csv'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prod_key',\n",
       " 'durable_prod_key',\n",
       " 'product_code',\n",
       " 'product_family',\n",
       " 'sub_acct',\n",
       " 'prod_technical_name',\n",
       " 'commercial_offer_name',\n",
       " 'offer_group_name',\n",
       " 'product_type',\n",
       " 'product_sub_type',\n",
       " 'bundle_validity',\n",
       " 'addon_type',\n",
       " 'tp_share',\n",
       " 'principle_agent',\n",
       " 'digital_price',\n",
       " 'portfolio',\n",
       " 'source',\n",
       " 'file_name']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROD_KEY',\n",
       " 'DURABLE_PROD_KEY',\n",
       " 'PRODUCT_CODE',\n",
       " 'PRODUCT_FAMILY',\n",
       " 'SUB_ACCT',\n",
       " 'PROD_TECHNICAL_NAME',\n",
       " 'COMMERCIAL_OFFER_NAME',\n",
       " 'OFFER_GROUP_NAME',\n",
       " 'PRODUCT_TYPE',\n",
       " 'PRODUCT_SUB_TYPE',\n",
       " 'BUNDLE_VALIDITY',\n",
       " 'ADDON_TYPE',\n",
       " 'TP_SHARE',\n",
       " 'PRINCIPLE_AGENT',\n",
       " 'DIGITAL_PRICE',\n",
       " 'PORTFOLIO',\n",
       " 'source',\n",
       " 'FILE_NAME']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+----------------------+----------------------------------+---------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+------+-------------------------------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME   |COMMERCIAL_OFFER_NAME             |OFFER_GROUP_NAME     |PRODUCT_TYPE|PRODUCT_SUB_TYPE |BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO     |source|FILE_NAME                      |\n",
      "+--------+----------------+------------+--------------+--------+----------------------+----------------------------------+---------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+------+-------------------------------+\n",
      "|167     |167             |500956      |CBS_PRODUCT   |0       |WEEKLY ALL IN ONE PLUS|WEEKLY EASY CARD MEGA - NWD - USSD|WEEKLY EASY CARD MEGA|HYBRID      |HEAVY DATA HYBRID|7              |DISCOUNTED|1\\R\\N   |PRINCIPLE      |0            |NOT APPLICABLE|42    |CBS_PRODUCT_DESC_20230914_1.unl|\n",
      "|595     |595             |502120      |CBS_PRODUCT   |0       |WEEKLY SPORTS BUNDLE  |WEEKLY SPORTS BUNDLE - NWD - MTA  |WEEKLY SPORTS BUNDLE |HYBRID      |GPRS             |7              |DISCOUNTED|1       |PRINCIPLE      |0            |NOT APPLICABLE|42    |CBS_PRODUCT_DESC_20230914_1.unl|\n",
      "|5670    |5670            |502402      |CBS_PRODUCT   |0       |PRE-6                 |PRE-6                             |PRE-6                |GPRS        |GPRS             |180            |DISCOUNTED|1       |PRINCIPLE      |0            |NOT APPLICABLE|42    |CBS_PRODUCT_DESC_20230914_1.unl|\n",
      "+--------+----------------+------------+--------------+--------+----------------------+----------------------------------+---------------------+------------+-----------------+---------------+----------+--------+---------------+-------------+--------------+------+-------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5=spark.read.csv(path5,header=True)\n",
    "# df3.show(truncate=0,n=3)\n",
    "df55=df5.select(cols1).toDF(*cols2)\n",
    "df55.show(truncate=0,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Source_data/use_case_tables_verification/422prod_my_query.csv'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path6='../Source_data/use_case_tables_verification/422prod_my_query.csv'\n",
    "path6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+-------------------------+-------------------------+-------------------------+------------+----------------+---------------+----------+--------+---------------+-------------+--------------+------+-------------------------------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME      |COMMERCIAL_OFFER_NAME    |OFFER_GROUP_NAME         |PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO     |source|FILE_NAME                      |\n",
      "+--------+----------------+------------+--------------+--------+-------------------------+-------------------------+-------------------------+------------+----------------+---------------+----------+--------+---------------+-------------+--------------+------+-------------------------------+\n",
      "|737     |737             |502235      |CBS_PRODUCT   |0       |TIKTOK MTA VARIANT       |DAILY TIKTOK - NWD - MTA |DAILY TIKTOK BUNDLE      |GPRS        |GPRS            |1              |DISCOUNTED|1       |PRINCIPLE      |0            |NOT APPLICABLE|42    |CBS_PRODUCT_DESC_20230914_1.unl|\n",
      "|5409    |5409            |502352      |CBS_PRODUCT   |0       |EC 150 USSD MTA DISCOUNT |EC 150 USSD MTA DISCOUNT |EASY CARD 150            |HYBRID      |PURE HYBRID     |7              |DISCOUNTED|1       |PRINCIPLE      |0            |NOT APPLICABLE|42    |CBS_PRODUCT_DESC_20230914_1.unl|\n",
      "|7218    |7218            |502557      |CBS_PRODUCT   |0       |SUPREME PARTNERSHIP OFFER|SUPREME PARTNERSHIP OFFER|SUPREME PARTNERSHIP OFFER|CALL        |CALL            |1              |FREE      |1       |PRINCIPLE      |0            |NOT APPLICABLE|42    |CBS_PRODUCT_DESC_20230914_1.unl|\n",
      "+--------+----------------+------------+--------------+--------+-------------------------+-------------------------+-------------------------+------------+----------------+---------------+----------+--------+---------------+-------------+--------------+------+-------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6=spark.read.csv(path6,header=True)\n",
    "# df3.show(truncate=0,n=3)\n",
    "df66=df6.select(cols2)\n",
    "df66.show(truncate=0,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT|PROD_TECHNICAL_NAME|COMMERCIAL_OFFER_NAME|OFFER_GROUP_NAME|PRODUCT_TYPE|PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|PORTFOLIO|source|FILE_NAME|\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "+--------+----------------+------------+--------------+--------+-------------------+---------------------+----------------+------------+----------------+---------------+----------+--------+---------------+-------------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dups2=df55.union(df66).distinct().groupBy(col('PROD_KEY')).agg(count('*').alias('count')).orderBy(col('PROD_KEY').desc()).filter('count >1').select ('PROD_KEY')\n",
    "# dups.show()\n",
    "dup2join5=df55.join (dups2,'PROD_KEY',how='inner').orderBy(col('PROD_KEY').desc())\n",
    "dup2join6=df66.join(dups2,'PROD_KEY',how='inner').orderBy(col('PROD_KEY').desc())\n",
    "result=dup2join5.join(dup2join6,'PROD_KEY',how='inner').orderBy(col('PROD_KEY').desc())\n",
    "dup2join5.union(dup2join6).orderBy(col('PROD_KEY').desc()).show(truncate=0)\n",
    "# result.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dups2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+--------------------+---------------------+--------------------+------------+------------------+---------------+----------+--------+---------------+-------------+--------------+------+--------------------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT| PROD_TECHNICAL_NAME|COMMERCIAL_OFFER_NAME|    OFFER_GROUP_NAME|PRODUCT_TYPE|  PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|     PORTFOLIO|source|           FILE_NAME|\n",
      "+--------+----------------+------------+--------------+--------+--------------------+---------------------+--------------------+------------+------------------+---------------+----------+--------+---------------+-------------+--------------+------+--------------------+\n",
      "|     997|             997|      539308|   CBS_PRODUCT|       0|TELENOR BSB - BUS...|          TELENOR BSB|         TELENOR BSB|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|      99|              99|      502221|   CBS_PRODUCT|       0|   FULL DAY OFFER RM|    FULL DAY OFFER RM|      FULL DAY OFFER|      HYBRID|HEAVY VOICE HYBRID|              1|DISCOUNTED|   1\\R\\N|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     984|             984|      501984|   CBS_PRODUCT|       0|       FREE WHATSAPP| MONTHLY WHATSAPP ...|    MONTHLY WHATSAPP|        GPRS|            GPRS_0|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     983|             983|      501014|   CBS_PRODUCT|       0|     DJ_3G_MONTHLY_1| MONTHLY ULTRA PLU...|  MONTHLY ULTRA PLUS|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     982|             982|      502164|   CBS_PRODUCT|       0|MONTHLY ULTRA - U...| MONTHLY ULTRA - U...|       MONTHLY ULTRA|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|      98|              98|      502042|   CBS_PRODUCT|       0|6TO6 INTERNET OFF...| DAILY OFFPEAK - N...|      DAILY OFF-PEAK|        GPRS|            GPRS_0|              1|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     977|             977|      502216|   CBS_PRODUCT|       0|MONTHLY SOCIAL PA...| MONTHLY SOCIAL PA...|MONTHLY SOCIAL PA...|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     976|             976|      502258|   CBS_PRODUCT|       0|MONTHLY SOCIAL PA...| MONTHLY SOCIAL PA...| MONTHLY SOCIAL PACK|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     975|             975|      501911|   CBS_PRODUCT|       0| MONTHLY SOCIAL PACK| MONTHLY SOCIAL PA...| MONTHLY SOCIAL PACK|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     974|             974|      501945|   CBS_PRODUCT|       0|MTA- MONTHLY WHAT...| MONTHLY SOCIAL PA...| MONTHLY SOCIAL PACK|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     971|             971|      502175|   CBS_PRODUCT|       0|  M2M 2 GB - PREPAID|     M2M 2 GB PREPAID|    M2M 2 GB PREPAID|        GPRS|              GPRS|             30|DISCOUNTED|   1\\R\\N|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     970|             970|      502122|   CBS_PRODUCT|       0|  MONTHLY IMO BUNDLE| MONTHLY IMO - NWD...|          IMO BUNDLE|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|      97|              97|      502058|   CBS_PRODUCT|       0|UNLIMITED INTERNE...| DAILY OFFPEAK - N...|      DAILY OFF-PEAK|        GPRS|            GPRS_0|              1|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     969|             969|      502301|   CBS_PRODUCT|       0|       TALEEM BUNDLE|              HEC-900|             HEC-900|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     965|             965|      502222|   CBS_PRODUCT|       0|            AIOU-900|             AIOU-900|            AIOU-900|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     964|             964|      502224|   CBS_PRODUCT|       0|            AIOU-500|             AIOU-500|            AIOU-500|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|      96|              96|      501819|   CBS_PRODUCT|       0|UNLIMITED INTERNE...| DAILY OFFPEAK - N...|      DAILY OFF-PEAK|        GPRS|            GPRS_0|              1|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     921|             921|      502166|   CBS_PRODUCT|       0|   EASYCARD 180 USSD|  WEEKLY EC PLUS USSD|      EASY CARD PLUS|      HYBRID|       PURE HYBRID|              7|DISCOUNTED|   1\\R\\N|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     920|             920|      502155|   CBS_PRODUCT|       0| WEEKLY EC PLUS USSD|  WEEKLY EC PLUS USSD|      EASY CARD PLUS|      HYBRID|       PURE HYBRID|              7|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     919|             919|      502194|   CBS_PRODUCT|       0|  WEEKLY EC PLUS MTA|   WEEKLY EC PLUS MTA|      EASY CARD PLUS|      HYBRID|       PURE HYBRID|              7|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "+--------+----------------+------------+--------------+--------+--------------------+---------------------+--------------------+------------+------------------+---------------+----------+--------+---------------+-------------+--------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#system\n",
    "dup2join5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+--------------+--------+--------------------+---------------------+--------------------+------------+------------------+---------------+----------+--------+---------------+-------------+--------------+------+--------------------+\n",
      "|PROD_KEY|DURABLE_PROD_KEY|PRODUCT_CODE|PRODUCT_FAMILY|SUB_ACCT| PROD_TECHNICAL_NAME|COMMERCIAL_OFFER_NAME|    OFFER_GROUP_NAME|PRODUCT_TYPE|  PRODUCT_SUB_TYPE|BUNDLE_VALIDITY|ADDON_TYPE|TP_SHARE|PRINCIPLE_AGENT|DIGITAL_PRICE|     PORTFOLIO|source|           FILE_NAME|\n",
      "+--------+----------------+------------+--------------+--------+--------------------+---------------------+--------------------+------------+------------------+---------------+----------+--------+---------------+-------------+--------------+------+--------------------+\n",
      "|     997|             997|         997|   CBS_PRODUCT|       0|TELENOR BSB - BUS...|          TELENOR BSB|         TELENOR BSB|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|      99|              99|          99|   CBS_PRODUCT|       0|   FULL DAY OFFER RM|    FULL DAY OFFER RM|      FULL DAY OFFER|      HYBRID|HEAVY VOICE HYBRID|              1|DISCOUNTED|   1\\R\\N|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     984|             984|         984|   CBS_PRODUCT|       0|       FREE WHATSAPP| MONTHLY WHATSAPP ...|    MONTHLY WHATSAPP|        GPRS|            GPRS_0|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     983|             983|         983|   CBS_PRODUCT|       0|     DJ_3G_MONTHLY_1| MONTHLY ULTRA PLU...|  MONTHLY ULTRA PLUS|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     982|             982|         982|   CBS_PRODUCT|       0|MONTHLY ULTRA - U...| MONTHLY ULTRA - U...|       MONTHLY ULTRA|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|      98|              98|          98|   CBS_PRODUCT|       0|6TO6 INTERNET OFF...| DAILY OFFPEAK - N...|      DAILY OFF-PEAK|        GPRS|            GPRS_0|              1|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     977|             977|         977|   CBS_PRODUCT|       0|MONTHLY SOCIAL PA...| MONTHLY SOCIAL PA...|MONTHLY SOCIAL PA...|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     976|             976|         976|   CBS_PRODUCT|       0|MONTHLY SOCIAL PA...| MONTHLY SOCIAL PA...| MONTHLY SOCIAL PACK|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     975|             975|         975|   CBS_PRODUCT|       0| MONTHLY SOCIAL PACK| MONTHLY SOCIAL PA...| MONTHLY SOCIAL PACK|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     974|             974|         974|   CBS_PRODUCT|       0|MTA- MONTHLY WHAT...| MONTHLY SOCIAL PA...| MONTHLY SOCIAL PACK|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     971|             971|         971|   CBS_PRODUCT|       0|  M2M 2 GB - PREPAID|     M2M 2 GB PREPAID|    M2M 2 GB PREPAID|        GPRS|              GPRS|             30|DISCOUNTED|   1\\R\\N|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     970|             970|         970|   CBS_PRODUCT|       0|  MONTHLY IMO BUNDLE| MONTHLY IMO - NWD...|          IMO BUNDLE|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|      97|              97|          97|   CBS_PRODUCT|       0|UNLIMITED INTERNE...| DAILY OFFPEAK - N...|      DAILY OFF-PEAK|        GPRS|            GPRS_0|              1|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     969|             969|         969|   CBS_PRODUCT|       0|       TALEEM BUNDLE|              HEC-900|             HEC-900|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     965|             965|         965|   CBS_PRODUCT|       0|            AIOU-900|             AIOU-900|            AIOU-900|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     964|             964|         964|   CBS_PRODUCT|       0|            AIOU-500|             AIOU-500|            AIOU-500|        GPRS|              GPRS|             30|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|      96|              96|          96|   CBS_PRODUCT|       0|UNLIMITED INTERNE...| DAILY OFFPEAK - N...|      DAILY OFF-PEAK|        GPRS|            GPRS_0|              1|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     921|             921|         921|   CBS_PRODUCT|       0|   EASYCARD 180 USSD|  WEEKLY EC PLUS USSD|      EASY CARD PLUS|      HYBRID|       PURE HYBRID|              7|DISCOUNTED|   1\\R\\N|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     920|             920|         920|   CBS_PRODUCT|       0| WEEKLY EC PLUS USSD|  WEEKLY EC PLUS USSD|      EASY CARD PLUS|      HYBRID|       PURE HYBRID|              7|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "|     919|             919|         919|   CBS_PRODUCT|       0|  WEEKLY EC PLUS MTA|   WEEKLY EC PLUS MTA|      EASY CARD PLUS|      HYBRID|       PURE HYBRID|              7|DISCOUNTED|       1|      PRINCIPLE|            0|NOT APPLICABLE|    42|CBS_PRODUCT_DESC_...|\n",
      "+--------+----------------+------------+--------------+--------+--------------------+---------------------+--------------------+------------+------------------+---------------+----------+--------+---------------+-------------+--------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dup2join6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Source_data/use_case_tables_verification/42prod_my_query.csv'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Source_data/use_case_tables_verification/42prod_my_query.csv'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup2join5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dups2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prod_key',\n",
       " 'durable_prod_key',\n",
       " 'product_code',\n",
       " 'product_family',\n",
       " 'sub_acct',\n",
       " 'prod_technical_name',\n",
       " 'commercial_offer_name',\n",
       " 'offer_group_name',\n",
       " 'product_type',\n",
       " 'product_sub_type',\n",
       " 'bundle_validity',\n",
       " 'addon_type',\n",
       " 'tp_share',\n",
       " 'principle_agent',\n",
       " 'digital_price',\n",
       " 'portfolio',\n",
       " 'source',\n",
       " 'file_name']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROD_KEY',\n",
       " 'DURABLE_PROD_KEY',\n",
       " 'PRODUCT_CODE',\n",
       " 'PRODUCT_FAMILY',\n",
       " 'SUB_ACCT',\n",
       " 'PROD_TECHNICAL_NAME',\n",
       " 'COMMERCIAL_OFFER_NAME',\n",
       " 'OFFER_GROUP_NAME',\n",
       " 'PRODUCT_TYPE',\n",
       " 'PRODUCT_SUB_TYPE',\n",
       " 'BUNDLE_VALIDITY',\n",
       " 'ADDON_TYPE',\n",
       " 'TP_SHARE',\n",
       " 'PRINCIPLE_AGENT',\n",
       " 'DIGITAL_PRICE',\n",
       " 'PORTFOLIO',\n",
       " 'source',\n",
       " 'FILE_NAME']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcount=spark.read.text('../Source_data/use_case_tables_verification/Country_Codes.xlsx')\n",
    "dfcount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "courses_df has\n",
      "\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n",
      "users_df has\n",
      "\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "\n",
      "course_enrolments_df has\n",
      "\n",
      "+-------------------+-------+---------+----------+\n",
      "|course_enrolment_id|user_id|course_id|price_paid|\n",
      "+-------------------+-------+---------+----------+\n",
      "|                  1|     10|        2|      9.99|\n",
      "|                  2|      5|        2|      9.99|\n",
      "|                  3|      7|        5|     10.99|\n",
      "|                  4|      9|        2|      9.99|\n",
      "|                  5|      8|        2|      9.99|\n",
      "|                  6|      5|        5|     10.99|\n",
      "|                  7|      4|        5|     10.99|\n",
      "|                  8|      7|        3|     10.99|\n",
      "|                  9|      8|        5|     10.99|\n",
      "|                 10|      3|        3|     10.99|\n",
      "|                 11|      7|        5|     10.99|\n",
      "|                 12|      3|        2|      9.99|\n",
      "|                 13|      5|        2|      9.99|\n",
      "|                 14|      4|        3|     10.99|\n",
      "|                 15|      8|        2|      9.99|\n",
      "+-------------------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./joins_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "my_list=[i for i in range(1,220)]\n",
    "my_ser=pd.Series(my_list)\n",
    "my_ser.name\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dp=pd.read_excel('../Source_data/use_case_tables_verification/Country_Codes.xlsx',header=0)\n",
    "dp[\"COUNTRYCODE\"].replace(np.NaN,0,inplace=True)\n",
    "dp[\"COUNTRYCODE\"].replace(\" \",0,inplace=True)\n",
    "# spark.createDataFrame(dp).dtypes\n",
    "# sc=\"\"\"\n",
    "# COUNTRYCODE STRING,\n",
    "# COUNTRY STRING\n",
    "# \"\"\"\n",
    "# df=spark.createDataFrame(dp,schema=sc)\n",
    "# df.show()\n",
    "# # df..write.save.format(f, grouping=False, monetary=False)(\"tabele name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTRYCODE</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>921</td>\n",
       "      <td>LEBANON</td>\n",
       "      <td>921+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>921</td>\n",
       "      <td>JORDAN</td>\n",
       "      <td>921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>SYRIA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>IRAQ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>KUWAIT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>383</td>\n",
       "      <td>KOSOVO</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>767</td>\n",
       "      <td>DOMINICA</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>800</td>\n",
       "      <td>UNIVERSAL FREEPHONE NUMBER</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>888</td>\n",
       "      <td>UNITED NATIONS</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>88247</td>\n",
       "      <td>SATELLITE SERVICES TRANSATEL</td>\n",
       "      <td>88247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    COUNTRYCODE                       COUNTRY  count\n",
       "0           921                       LEBANON   921+\n",
       "1           921                        JORDAN    921\n",
       "2             0                         SYRIA      0\n",
       "3             0                          IRAQ      0\n",
       "4             0                        KUWAIT      0\n",
       "..          ...                           ...    ...\n",
       "215         383                        KOSOVO    383\n",
       "216         767                      DOMINICA    767\n",
       "217         800    UNIVERSAL FREEPHONE NUMBER    800\n",
       "218         888                UNITED NATIONS    888\n",
       "219       88247  SATELLITE SERVICES TRANSATEL  88247\n",
       "\n",
       "[220 rows x 3 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dp['COUNTRYCODE']=dp['COUNTRYCODE']\n",
    "# dp.head(n=20)\n",
    "# dp[\"count\"]=dp[\"COUNTRYCODE\"].replace('92++','921')\n",
    "dp[\"COUNTRYCODE\"].replace('921+','921',inplace=True)\n",
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTRYCODE</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>LEBANON</td>\n",
       "      <td>921+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>JORDAN</td>\n",
       "      <td>921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>SYRIA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>IRAQ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>KUWAIT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>SAUDI ARABIA</td>\n",
       "      <td>966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>YEMEN</td>\n",
       "      <td>967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>INMARSAT</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>OMAN</td>\n",
       "      <td>968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>PALESTINE</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>971</td>\n",
       "      <td>UNITED ARAB EMIRATES</td>\n",
       "      <td>971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>972</td>\n",
       "      <td>PALESTINE(ISRAEL)</td>\n",
       "      <td>972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>973</td>\n",
       "      <td>BAHRAIN</td>\n",
       "      <td>973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>974</td>\n",
       "      <td>QATAR</td>\n",
       "      <td>974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>975</td>\n",
       "      <td>BHUTAN</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>976</td>\n",
       "      <td>MONGOLIA</td>\n",
       "      <td>976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>977</td>\n",
       "      <td>NEPAL</td>\n",
       "      <td>977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>992</td>\n",
       "      <td>TAJIKISTAN</td>\n",
       "      <td>992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>993</td>\n",
       "      <td>TURKMENISTAN</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>994</td>\n",
       "      <td>AZERBAIJAN</td>\n",
       "      <td>994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    COUNTRYCODE               COUNTRY count\n",
       "0            11               LEBANON  921+\n",
       "1            11                JORDAN   921\n",
       "2            12                 SYRIA     0\n",
       "3            12                  IRAQ     0\n",
       "4            12                KUWAIT     0\n",
       "5            13          SAUDI ARABIA   966\n",
       "6            14                 YEMEN   967\n",
       "7            15              INMARSAT   870\n",
       "8            16                  OMAN   968\n",
       "9            17             PALESTINE   970\n",
       "10          971  UNITED ARAB EMIRATES   971\n",
       "11          972     PALESTINE(ISRAEL)   972\n",
       "12          973               BAHRAIN   973\n",
       "13          974                 QATAR   974\n",
       "14          975                BHUTAN   975\n",
       "15          976              MONGOLIA   976\n",
       "16          977                 NEPAL   977\n",
       "17          992            TAJIKISTAN   992\n",
       "18          993          TURKMENISTAN   993\n",
       "19          994            AZERBAIJAN   994"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp[\"COUNTRYCODE\"].replace(['921', 0, 966, 967, 870, 968, 970],[11,12,13,14,15,16,17],inplace=True)\n",
    "dp.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['921', 0, 966, 967, 870, 968, 970, 971, 972, 973, 974, 975, 976,\n",
       "       977, 992, 993, 994, 995, 996, 998, 440, 876, 878, 1, 7, 20, 27, 30,\n",
       "       31, 32, 33, 34, 36, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 51, 52,\n",
       "       53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 81, 82, 84, 86,\n",
       "       90, 91, 92, 93, 94, 95, 98, 211, 212, 213, 216, 218, 220, 221, 222,\n",
       "       223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235,\n",
       "       236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248,\n",
       "       249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 262,\n",
       "       263, 264, 265, 266, 267, 268, 269, 290, 291, 297, 298, 299, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 370, 371, 372, 373,\n",
       "       374, 375, 376, 377, 378, 380, 381, 382, 385, 386, 387, 389, 420,\n",
       "       421, 423, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 590,\n",
       "       591, 592, 593, 594, 595, 596, 597, 598, 599, 670, 672, 673, 674,\n",
       "       675, 676, 677, 678, 679, 680, 681, 682, 683, 685, 686, 687, 688,\n",
       "       689, 690, 691, 692, 850, 852, 853, 855, 856, 880, 881, 882, 883,\n",
       "       886, 960, 1284, 1345, 88237, 383, 767, 800, 888, 88247],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp['COUNTRYCODE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp['COUNTRYCODE'].str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: 0       921+\n1       92++\n2          0\n3          0\n4          0\n       ...  \n215      383\n216      767\n217      800\n218      888\n219    88247\nName: COUNTRYCODE, Length: 220, dtype: object of type <class 'pandas.core.series.Series'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asfandyar\\Desktop\\python_Asfandyar\\40 days\\chilla\\Spark\\Certification Spark\\Pyspark-for-Data-Analyst\\Pyspark\\Code\\Spark_Beginner_to_advance.ipynb Cell 668\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z1653sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m split(dp[\u001b[39m'\u001b[39;49m\u001b[39mCOUNTRYCODE\u001b[39;49m\u001b[39m'\u001b[39;49m],\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\functions.py:2981\u001b[0m, in \u001b[0;36msplit\u001b[1;34m(str, pattern, limit)\u001b[0m\n\u001b[0;32m   2948\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2949\u001b[0m \u001b[39mSplits str around matches of the given pattern.\u001b[39;00m\n\u001b[0;32m   2950\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[39m[Row(s=['one', 'two', 'three', ''])]\u001b[39;00m\n\u001b[0;32m   2979\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2980\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n\u001b[1;32m-> 2981\u001b[0m \u001b[39mreturn\u001b[39;00m Column(sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mfunctions\u001b[39m.\u001b[39msplit(_to_java_column(\u001b[39mstr\u001b[39;49m), pattern, limit))\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\column.py:45\u001b[0m, in \u001b[0;36m_to_java_column\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m     43\u001b[0m     jcol \u001b[39m=\u001b[39m _create_column_from_name(col)\n\u001b[0;32m     44\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m     46\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid argument, not a string or column: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m of type \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFor column literals, use \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlit\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39marray\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mstruct\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcreate_map\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfunction.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(col, \u001b[39mtype\u001b[39m(col)))\n\u001b[0;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m jcol\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid argument, not a string or column: 0       921+\n1       92++\n2          0\n3          0\n4          0\n       ...  \n215      383\n216      767\n217      800\n218      888\n219    88247\nName: COUNTRYCODE, Length: 220, dtype: object of type <class 'pandas.core.series.Series'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "split(dp['COUNTRYCODE'],'+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "courses_df has\n",
      "\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n",
      "users_df has\n",
      "\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "\n",
      "course_enrolments_df has\n",
      "\n",
      "+-------------------+-------+---------+----------+\n",
      "|course_enrolment_id|user_id|course_id|price_paid|\n",
      "+-------------------+-------+---------+----------+\n",
      "|                  1|     10|        2|      9.99|\n",
      "|                  2|      5|        2|      9.99|\n",
      "|                  3|      7|        5|     10.99|\n",
      "|                  4|      9|        2|      9.99|\n",
      "|                  5|      8|        2|      9.99|\n",
      "|                  6|      5|        5|     10.99|\n",
      "|                  7|      4|        5|     10.99|\n",
      "|                  8|      7|        3|     10.99|\n",
      "|                  9|      8|        5|     10.99|\n",
      "|                 10|      3|        3|     10.99|\n",
      "|                 11|      7|        5|     10.99|\n",
      "|                 12|      3|        2|      9.99|\n",
      "|                 13|      5|        2|      9.99|\n",
      "|                 14|      4|        3|     10.99|\n",
      "|                 15|      8|        2|      9.99|\n",
      "+-------------------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./joins_data.ipynb\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------------------+\n",
      "|email_id|user_id|        signup_date|\n",
      "+--------+-------+-------------------+\n",
      "|     125|   7771|06/14/2022 00:00:00|\n",
      "|     236|   6950|07/01/2022 00:00:00|\n",
      "|     433|   1052|07/09/2022 00:00:00|\n",
      "|     450|   8963|08/02/2022 00:00:00|\n",
      "|     555|   8963|08/09/2022 00:00:00|\n",
      "|     741|   1235|07/25/2022 00:00:00|\n",
      "+--------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ls=[(125\t\t,7771,\t'06/14/2022 00:00:00'),(236\t\t,6950,\t'07/01/2022 00:00:00'),(433\t\t,1052,\t'07/09/2022 00:00:00'),(450\t\t,8963,\t'08/02/2022 00:00:00'),(555\t\t,8963,\t'08/09/2022 00:00:00'),(741\t\t,1235,\t'07/25/2022 00:00:00')]\n",
    "sc=['email_id','user_id','signup_date']\n",
    "\n",
    "df1=spark.createDataFrame(ls,sc)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|text_id|email_id|signup_action|        action_date|\n",
      "+-------+--------+-------------+-------------------+\n",
      "|   6878|     125|    Confirmed|06/14/2022 00:00:00|\n",
      "|   6997|     433|Not confirmed|07/09/2022 00:00:00|\n",
      "|   7000|     433|    Confirmed|07/10/2022 00:00:00|\n",
      "|   9841|     236|    Confirmed|07/01/2022 00:00:00|\n",
      "|   2800|     555|    Confirmed|08/11/2022 00:00:00|\n",
      "|   1568|     741|    Confirmed|07/26/2022 00:00:00|\n",
      "|   1255|     555|Not confirmed|08/09/2022 00:00:00|\n",
      "|   1522|     741|Not confirmed|07/25/2022 00:00:00|\n",
      "|   6800|     450|Not confirmed|08/02/2022 00:00:00|\n",
      "|   2660|     555|Not confirmed|08/09/2022 00:00:00|\n",
      "+-------+--------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc1=['text_id',\t'email_id',\t'signup_action',\t'action_date']\n",
    "ls1=[(6878,125,\t'Confirmed'\t\t,'06/14/2022 00:00:00'),(6997,433,\t'Not confirmed'\t,'07/09/2022 00:00:00'),(7000,433,\t'Confirmed'\t\t,'07/10/2022 00:00:00'),(9841,236,\t'Confirmed'\t\t,'07/01/2022 00:00:00'),(2800,555,\t'Confirmed'\t\t,'08/11/2022 00:00:00'),(1568,741,\t'Confirmed'\t\t,'07/26/2022 00:00:00'),(1255,555,\t'Not confirmed'\t,'08/09/2022 00:00:00'),(1522,741,\t'Not confirmed'\t,'07/25/2022 00:00:00'),(6800,450,\t'Not confirmed'\t,'08/02/2022 00:00:00'),(2660,555,\t'Not confirmed'\t,'08/09/2022 00:00:00')]\n",
    "df2=spark.createDataFrame(ls1,sc1)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|   1052|\n",
      "|   1235|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,on=(df1['email_id']==df2['email_id']) & (df2['signup_action']=='Confirmed'), how='inner').\\\n",
    "    withColumn('action_date',to_date('action_date','MM/dd/yyy HH:mm:ss')).\\\n",
    "        withColumn('signup_actions',to_date('signup_date','MM/dd/yyy HH:mm:ss')).\\\n",
    "         filter (expr(\"action_date == date_add(signup_actions,1)\")).select ('user_id').show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE CASE 3\n",
    "* Write a Dataframe API Code that will find out days and then filter by sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2023-01-01',)\n",
      "('2023-01-02',)\n",
      "('2023-01-03',)\n",
      "('2023-01-04',)\n",
      "('2023-01-05',)\n",
      "('2023-01-06',)\n",
      "('2023-01-07',)\n",
      "('2023-01-08',)\n",
      "('2023-01-09',)\n",
      "('2023-01-10',)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Specify the start and end dates\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 1, 10)\n",
    "\n",
    "# Define an empty list to store the tuples\n",
    "date_tuples = []\n",
    "\n",
    "# Loop through the dates and create tuples\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    date_tuples.append((current_date.strftime(\"%Y-%m-%d\"),))\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Print the list of date tuples\n",
    "for date_tuple in date_tuples:\n",
    "    print(date_tuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     dates|\n",
      "+----------+\n",
      "|2023-01-01|\n",
      "|2023-01-02|\n",
      "|2023-01-03|\n",
      "|2023-01-04|\n",
      "|2023-01-05|\n",
      "|2023-01-06|\n",
      "|2023-01-07|\n",
      "|2023-01-08|\n",
      "|2023-01-09|\n",
      "|2023-01-10|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_tuple=[('2023-01-01',),\n",
    "('2023-01-02',),\n",
    "('2023-01-03',),\n",
    "('2023-01-04',),\n",
    "('2023-01-05',),\n",
    "('2023-01-06',),\n",
    "('2023-01-07',),\n",
    "('2023-01-08',),\n",
    "('2023-01-09',),\n",
    "('2023-01-10',)]\n",
    "dp=spark.createDataFrame(date_tuple,schema='dates string')\n",
    "dp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|     dates|   new|\n",
      "+----------+------+\n",
      "|2023-01-01|Sunday|\n",
      "|2023-01-08|Sunday|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dp.withColumn('new',date_format('dates','EEEE')).filter(\"new == 'Sunday'\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **USE CASE 4**\n",
    "* In this SQL problem solving session, we have travel details of the user & are finding the last location where the user was spotted on each day using window ranking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "courses_df has\n",
      "\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n",
      "users_df has\n",
      "\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "\n",
      "course_enrolments_df has\n",
      "\n",
      "+-------------------+-------+---------+----------+\n",
      "|course_enrolment_id|user_id|course_id|price_paid|\n",
      "+-------------------+-------+---------+----------+\n",
      "|                  1|     10|        2|      9.99|\n",
      "|                  2|      5|        2|      9.99|\n",
      "|                  3|      7|        5|     10.99|\n",
      "|                  4|      9|        2|      9.99|\n",
      "|                  5|      8|        2|      9.99|\n",
      "|                  6|      5|        5|     10.99|\n",
      "|                  7|      4|        5|     10.99|\n",
      "|                  8|      7|        3|     10.99|\n",
      "|                  9|      8|        5|     10.99|\n",
      "|                 10|      3|        3|     10.99|\n",
      "|                 11|      7|        5|     10.99|\n",
      "|                 12|      3|        2|      9.99|\n",
      "|                 13|      5|        2|      9.99|\n",
      "|                 14|      4|        3|     10.99|\n",
      "|                 15|      8|        2|      9.99|\n",
      "+-------------------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./joins_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------------------+-----------------------+\n",
      "|user_id|user_name      |user_visit_location|user_visit_time        |\n",
      "+-------+---------------+-------------------+-----------------------+\n",
      "|108    |Shivangi Kumari|Bengaluru          |2022-08-01 01:15:00.980|\n",
      "|123    |Yuvraj Sinha   |Jaipur             |2022-08-25 03:55:00.050|\n",
      "|108    |Shivangi Kumari|Pune               |2022-08-06 09:23:00.510|\n",
      "|108    |Shivangi Kumari|Mumbai             |2022-08-06 22:00:05.100|\n",
      "|159    |Priti Dubey    |Lucknow            |2022-08-28 10:20:00.260|\n",
      "|123    |Yuvraj Sinha   |Bengaluru          |2022-08-25 20:13:00.110|\n",
      "|159    |Priti Dubey    |Chennai            |2022-08-20 06:00:00.560|\n",
      "|120    |Jai Dixit      |Delhi              |2022-08-11 08:25:00.430|\n",
      "+-------+---------------+-------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc=['user_id','user_name','user_visit_location','user_visit_time']\n",
    "\n",
    "data=[(108,'Shivangi Kumari','Bengaluru','2022-08-01 01:15:00.980'),\n",
    "(123,'Yuvraj Sinha','Jaipur','2022-08-25 03:55:00.050'),\n",
    "(108,'Shivangi Kumari','Pune','2022-08-06 09:23:00.510'),\n",
    "(108,'Shivangi Kumari','Mumbai','2022-08-06 22:00:05.100'),\n",
    "(159,'Priti Dubey','Lucknow','2022-08-28 10:20:00.260'),\n",
    "(123,'Yuvraj Sinha','Bengaluru','2022-08-25 20:13:00.110'),\n",
    "(159,'Priti Dubey','Chennai','2022-08-20 06:00:00.560'),\n",
    "(120,'Jai Dixit','Delhi','2022-08-11 08:25:00.430')]\n",
    "df=spark.createDataFrame(data,schema=sc)\n",
    "df.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+------------------------+-------------------+-----------------------+------+\n",
      "|user_id|user_name      |to_date(user_visit_time)|user_visit_location|user_visit_time        |rankss|\n",
      "+-------+---------------+------------------------+-------------------+-----------------------+------+\n",
      "|108    |Shivangi Kumari|2022-08-01              |Bengaluru          |2022-08-01 01:15:00.980|1     |\n",
      "|108    |Shivangi Kumari|2022-08-06              |Mumbai             |2022-08-06 22:00:05.100|1     |\n",
      "|120    |Jai Dixit      |2022-08-11              |Delhi              |2022-08-11 08:25:00.430|1     |\n",
      "|123    |Yuvraj Sinha   |2022-08-25              |Bengaluru          |2022-08-25 20:13:00.110|1     |\n",
      "|159    |Priti Dubey    |2022-08-20              |Chennai            |2022-08-20 06:00:00.560|1     |\n",
      "|159    |Priti Dubey    |2022-08-28              |Lucknow            |2022-08-28 10:20:00.260|1     |\n",
      "+-------+---------------+------------------------+-------------------+-----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from (\n",
    "              select user_id, user_name,to_date(user_visit_time), user_visit_location, user_visit_time,rank() over (partition by user_id,to_date(user_visit_time) order by user_visit_time desc) as rankss\n",
    "              from \n",
    "              df ) e where e.rankss=1\n",
    "              \"\"\").show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------------------+-----------------------+-----+\n",
      "|user_id|user_name      |user_visit_location|user_visit_time        |ranks|\n",
      "+-------+---------------+-------------------+-----------------------+-----+\n",
      "|108    |Shivangi Kumari|Bengaluru          |2022-08-01 01:15:00.980|1    |\n",
      "|108    |Shivangi Kumari|Mumbai             |2022-08-06 22:00:05.100|1    |\n",
      "|120    |Jai Dixit      |Delhi              |2022-08-11 08:25:00.430|1    |\n",
      "|123    |Yuvraj Sinha   |Bengaluru          |2022-08-25 20:13:00.110|1    |\n",
      "|159    |Priti Dubey    |Chennai            |2022-08-20 06:00:00.560|1    |\n",
      "|159    |Priti Dubey    |Lucknow            |2022-08-28 10:20:00.260|1    |\n",
      "+-------+---------------+-------------------+-----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Spark Dataframe API\n",
    "from pyspark.sql.window import Window\n",
    "wind= Window.partitionBy(['user_id',to_date('user_visit_time')]).orderBy(col('user_visit_time').desc())\n",
    "df.withColumn(\"ranks\",dense_rank().over(wind)).select('*').filter('ranks == 1').show(truncate=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+\n",
      "|chips                         |Amt     |\n",
      "+------------------------------+--------+\n",
      "|lays1,uncle_chips1,kurkure1   |10,20,30|\n",
      "|wafferrs2                     |40,50   |\n",
      "|potatochips3,hotchips3,balaji3|60,70,80|\n",
      "+------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc=['chips', 'Amt' ]\n",
    "data=[('lays1,uncle_chips1,kurkure1' , '10,20,30'),\n",
    "('wafferrs2' , '40,50'),\n",
    "('potatochips3,hotchips3,balaji3' , '60,70,80')]\n",
    "df=spark.createDataFrame(data,sc)\n",
    "df.show(truncate=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "from spark.sql.function import posexplode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('chipss',split(col('chips'),',')).withColumn('amts',split(col('Amt'),',')).\\\n",
    "    withColumn('exploded_chips',explode('chipss')).withColumn('exploded_amts',explode('amts')).\\\n",
    "        drop(*['chips','Amt','chipss','amts']).createOrReplaceTempView('dff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|exploded_chips|exploded_amts|\n",
      "+--------------+-------------+\n",
      "|         lays1|           10|\n",
      "|         lays1|           20|\n",
      "|         lays1|           30|\n",
      "|  uncle_chips1|           10|\n",
      "|  uncle_chips1|           20|\n",
      "|  uncle_chips1|           30|\n",
      "|      kurkure1|           10|\n",
      "|      kurkure1|           20|\n",
      "|      kurkure1|           30|\n",
      "|     wafferrs2|           40|\n",
      "|     wafferrs2|           50|\n",
      "|  potatochips3|           60|\n",
      "|  potatochips3|           70|\n",
      "|  potatochips3|           80|\n",
      "|     hotchips3|           60|\n",
      "|     hotchips3|           70|\n",
      "|     hotchips3|           80|\n",
      "|       balaji3|           60|\n",
      "|       balaji3|           70|\n",
      "|       balaji3|           80|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dff\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+\n",
      "|chips                         |Amt     |\n",
      "+------------------------------+--------+\n",
      "|lays1,uncle_chips1,kurkure1   |10,20,30|\n",
      "|wafferrs2                     |40,50   |\n",
      "|potatochips3,hotchips3,balaji3|60,70,80|\n",
      "+------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o448.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 100.0 failed 1 times, most recent failure: Lost task 2.0 in stage 100.0 (TID 318) (IT-130-21 executor driver): java.io.FileNotFoundException: C:\\Users\\Asfandyar\\AppData\\Local\\Temp\\blockmgr-0c06a65b-a8bc-4e8a-9b78-362069062d8d\\26\\temp_shuffle_d9ac05b9-d76e-411f-aaac-a97080a9d9b4 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\r\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\Asfandyar\\AppData\\Local\\Temp\\blockmgr-0c06a65b-a8bc-4e8a-9b78-362069062d8d\\26\\temp_shuffle_d9ac05b9-d76e-411f-aaac-a97080a9d9b4 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\r\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asfandyar\\Desktop\\python_Asfandyar\\40 days\\chilla\\Spark\\Certification Spark\\Pyspark-for-Data-Analyst\\Pyspark\\Code\\Spark_Beginner_to_advance.ipynb Cell 721\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z2216sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m spark\u001b[39m.\u001b[39;49msql(\u001b[39m\"\"\"\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z2216sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m          \u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z2216sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m          select * ,row_number() over(partition by exploded_chips order by exploded_amts asc ) as rs\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z2216sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m          from dff \u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z2216sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m          \u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z2216sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m          \u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m)\u001b[39m.\u001b[39;49mshow(truncate\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:502\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    499\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    500\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncate=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m should be either bool or int.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(truncate))\n\u001b[1;32m--> 502\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, int_truncate, vertical))\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o448.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 100.0 failed 1 times, most recent failure: Lost task 2.0 in stage 100.0 (TID 318) (IT-130-21 executor driver): java.io.FileNotFoundException: C:\\Users\\Asfandyar\\AppData\\Local\\Temp\\blockmgr-0c06a65b-a8bc-4e8a-9b78-362069062d8d\\26\\temp_shuffle_d9ac05b9-d76e-411f-aaac-a97080a9d9b4 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\r\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\Asfandyar\\AppData\\Local\\Temp\\blockmgr-0c06a65b-a8bc-4e8a-9b78-362069062d8d\\26\\temp_shuffle_d9ac05b9-d76e-411f-aaac-a97080a9d9b4 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\r\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          \n",
    "          select * ,row_number() over(partition by exploded_chips order by exploded_amts asc ) as rs\n",
    "          from dff \n",
    "          \n",
    "          \"\"\").show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('dff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select e.user_id  from emails as e inner join texts as t on e.email_id\t=t.email_id\tand t.signup_action ='Confirmed'  \n",
    "where date(action_date)-date(signup_date)=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp=pd.read_excel('../Source_data/use_case_tables_verification/Country_Codes.xlsx',header=0,index_col=np.arange(0,220))\n",
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11=df1.select (cols1).toDF(*cols2)\n",
    "df11.show(truncate=0,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M aei sad\n"
     ]
    }
   ],
   "source": [
    "name='My name is Asfand'\n",
    "print(name[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnafsA si eman yM\n"
     ]
    }
   ],
   "source": [
    "name='My name is Asfand'\n",
    "print(name[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya \n"
     ]
    }
   ],
   "source": [
    "name='My name is Asfand'\n",
    "print(name[1:10:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='My name is Asfand'\n",
    "print(name[0:10:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "email='asfandyar@addo.ai'\n",
    "print(email.index('@'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employee name is asfandyar\n"
     ]
    }
   ],
   "source": [
    "print(f\"employee name is {email[:email.index('@')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company email is @addo.ai\n"
     ]
    }
   ],
   "source": [
    "print (f\"company email is {email[email.index('@'):]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of company is addo\n"
     ]
    }
   ],
   "source": [
    "print (f\"Name of company is {email[email.index('@')+1:email.index('.')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis=['asfand','saeed',1,2,3,4]\n",
    "lis.extend([20,30,'Ali'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis.insert(2,[20,30,40, 'saeed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asfand', 'saeed', [20, 30, 40, 'saeed'], 1, 2, 3, 4, 20, 30, 'Ali']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asfand', [20, 30, 40], 1, 2, 3, 4, 20, 30, 'Ali']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis[1].remove('saeed')\n",
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asfand', [20, 30, 40], 1, 2, 3, 4]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## to remove from end \n",
    "lis.pop()\n",
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asfand', [20, 30, 40], 1, 2, 3]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## to remove from end \n",
    "lis.pop()\n",
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis.index('asfand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'list' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asfandyar\\Desktop\\python_Asfandyar\\40 days\\chilla\\Spark\\Certification Spark\\Pyspark-for-Data-Analyst\\Pyspark\\Code\\Spark_Beginner_to_advance.ipynb Cell 691\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z2013sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lis\u001b[39m.\u001b[39;49msort()\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'list' and 'str'"
     ]
    }
   ],
   "source": [
    "lis.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "li=[20,1,43,50,100]\n",
    "li.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 20, 43, 50, 100]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALI', 'asfand', 'saeed']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lii=['saeed','asfand','ALI']\n",
    "lii.sort()\n",
    "lii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saeed', 'asfand', 'ALI']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lii.sort(reverse=True)\n",
    "lii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li=[1,1,1,1]\n",
    "lic=li\n",
    "lic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li=[2,3,4,5]\n",
    "lic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li=[1,1,1,1]\n",
    "lic=li.copy()\n",
    "lic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li=[1,2,4,5,6]\n",
    "lic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Creating Pandas Dataframe from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.DataFrame()\n",
    "car=[\"nesan\",'mehran','japan']\n",
    "typ=['car','car','country']\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      car\n",
       "0   nesan\n",
       "1  mehran\n",
       "2   japan"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['car']=car\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      car     type\n",
       "0   nesan      car\n",
       "1  mehran      car\n",
       "2   japan  country"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['type']=typ\n",
    "df1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">How to create a data frame from a dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      car     type\n",
       "0   nesan      car\n",
       "1  mehran      car\n",
       "2   japan  country"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car=[\"nesan\",'mehran','japan']\n",
    "typ=['car','car','country']\n",
    "d={'car':car,'type':typ}\n",
    "df=pd.DataFrame(d)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Combine two dataframes\n",
    "> 1. concat method\n",
    "> 2. join method\n",
    "> 3. append method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      car     type\n",
       "0   nesan      car\n",
       "1  mehran      car\n",
       "2   japan  country\n",
       "0   nesan      car\n",
       "1  mehran      car\n",
       "2   japan  country"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. concat method\n",
    "pd.concat([df,df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>type</th>\n",
       "      <th>car</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      car     type     car     type\n",
       "0   nesan      car   nesan      car\n",
       "1  mehran      car  mehran      car\n",
       "2   japan  country   japan  country"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. concat method\n",
    "pd.concat([df,df1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asfandyar\\AppData\\Local\\Temp\\ipykernel_25900\\202388049.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df.append(df1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      car     type\n",
       "0   nesan      car\n",
       "1  mehran      car\n",
       "2   japan  country\n",
       "0   nesan      car\n",
       "1  mehran      car\n",
       "2   japan  country"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. append\n",
    "df.append(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      car     type\n",
       "0   nesan      car\n",
       "1  mehran      car\n",
       "2   japan  country\n",
       "3   nesan      car\n",
       "4  mehran      car\n",
       "5   japan  country"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. join\n",
    "dff=pd.concat([df1, df], axis=0, ignore_index=True)\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_x</th>\n",
       "      <th>type</th>\n",
       "      <th>car_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "      <td>nesan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "      <td>mehran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "      <td>nesan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "      <td>mehran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "      <td>japan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    car_x     type   car_y\n",
       "0   nesan      car   nesan\n",
       "1   nesan      car  mehran\n",
       "2  mehran      car   nesan\n",
       "3  mehran      car  mehran\n",
       "4   japan  country   japan"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. join\n",
    "pd.merge(df,df1,left_on='type',right_on='type',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      car     type\n",
       "0   nesan      car\n",
       "1  mehran      car\n",
       "2   japan  country"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      car     type\n",
       "0   nesan      car\n",
       "1  mehran      car\n",
       "2   japan  country"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>type_x</th>\n",
       "      <th>type_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      car   type_x   type_y\n",
       "0   nesan      car      car\n",
       "1  mehran      car      car\n",
       "2   japan  country  country"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df,df1,on='car',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>type_x</th>\n",
       "      <th>type_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      car   type_x   type_y\n",
       "0   nesan      car      car\n",
       "1  mehran      car      car\n",
       "2   japan  country  country"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df,df1,on='car',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>type</th>\n",
       "      <th>row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      car     type  row\n",
       "0   nesan      car    0\n",
       "1  mehran      car    1\n",
       "2   japan  country    2\n",
       "3   nesan      car    3\n",
       "4  mehran      car    4\n",
       "5   japan  country    5"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff['row']=dff.index\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "      <th>car</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row     car     type\n",
       "0    0   nesan      car\n",
       "1    1  mehran      car\n",
       "2    2   japan  country\n",
       "3    3   nesan      car\n",
       "4    4  mehran      car\n",
       "5    5   japan  country"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff[['row','car','type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "name_new={\n",
    "    'row': 'suming_row',\n",
    "    'car': 'car_new',\n",
    "    'type':'type_new'\n",
    "}\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_new</th>\n",
       "      <th>type_new</th>\n",
       "      <th>suming_row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nesan</td>\n",
       "      <td>car</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mehran</td>\n",
       "      <td>car</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>japan</td>\n",
       "      <td>country</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  car_new type_new  suming_row\n",
       "0   nesan      car           0\n",
       "1  mehran      car           1\n",
       "2   japan  country           2\n",
       "3   nesan      car           3\n",
       "4  mehran      car           4\n",
       "5   japan  country           5"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.rename(columns=name_new,inplace=True)\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car_new       nesan\n",
      "type_new        car\n",
      "suming_row        0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dff.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nesan\tcar\t0\n"
     ]
    }
   ],
   "source": [
    "print('\\t'.join(map(str, dff.loc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  car_new type_new  suming_row\n",
      "0   nesan      car           0\n",
      "2   japan  country           2\n",
      "4  mehran      car           4\n"
     ]
    }
   ],
   "source": [
    "print(dff.loc[0:4:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How to fetch a data entry from a pandas dataframe using a given value in index?\n",
    "> 1. a=[10,20,30,40,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bikes</th>\n",
       "      <th>cars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bajaj</td>\n",
       "      <td>lamborghini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tvs</td>\n",
       "      <td>masserati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>herohonda</td>\n",
       "      <td>ferrari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>kawasaki</td>\n",
       "      <td>hyundai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>bmw</td>\n",
       "      <td>ford</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bikes         cars\n",
       "10      bajaj  lamborghini\n",
       "20        tvs    masserati\n",
       "30  herohonda      ferrari\n",
       "40   kawasaki      hyundai\n",
       "50        bmw         ford"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes=[\"bajaj\",\"tvs\",\"herohonda\",\"kawasaki\",\"bmw\"]\n",
    "cars=[\"lamborghini\",\"masserati\",\"ferrari\",\"hyundai\",\"ford\"]\n",
    "a=[10,20,30,40,50]\n",
    "d={'bikes':bikes,'cars': cars}\n",
    "df=pd.DataFrame(d,index=a)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bikes</th>\n",
       "      <th>cars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bajaj</td>\n",
       "      <td>lamborghini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>herohonda</td>\n",
       "      <td>ferrari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>bmw</td>\n",
       "      <td>ford</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bikes         cars\n",
       "10      bajaj  lamborghini\n",
       "30  herohonda      ferrari\n",
       "50        bmw         ford"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:50:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "courses_df has\n",
      "\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n",
      "users_df has\n",
      "\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n",
      "|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n",
      "|      3|         Joanna|      Spennock|jspennock2@redcro...|\n",
      "|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n",
      "|      5|         Loreen|         Malin|lmalin4@independe...|\n",
      "|      6|           Augy|      Christon|  achriston5@mlb.com|\n",
      "|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n",
      "|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|\n",
      "|      9|        Vassily|         Tamas|vtamas8@businessw...|\n",
      "|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n",
      "+-------+---------------+--------------+--------------------+\n",
      "\n",
      "course_enrolments_df has\n",
      "\n",
      "+-------------------+-------+---------+----------+\n",
      "|course_enrolment_id|user_id|course_id|price_paid|\n",
      "+-------------------+-------+---------+----------+\n",
      "|                  1|     10|        2|      9.99|\n",
      "|                  2|      5|        2|      9.99|\n",
      "|                  3|      7|        5|     10.99|\n",
      "|                  4|      9|        2|      9.99|\n",
      "|                  5|      8|        2|      9.99|\n",
      "|                  6|      5|        5|     10.99|\n",
      "|                  7|      4|        5|     10.99|\n",
      "|                  8|      7|        3|     10.99|\n",
      "|                  9|      8|        5|     10.99|\n",
      "|                 10|      3|        3|     10.99|\n",
      "|                 11|      7|        5|     10.99|\n",
      "|                 12|      3|        2|      9.99|\n",
      "|                 13|      5|        2|      9.99|\n",
      "|                 14|      4|        3|     10.99|\n",
      "|                 15|      8|        2|      9.99|\n",
      "+-------------------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./joins_data.ipynb\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **shape**\n",
    "\n",
    "The shape tool gives a tuple of array dimensions and can be used to change the dimensions of an array.\n",
    "\n",
    "(a). Using shape to get array dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "ins=np.array(list(input().split(' ')),dtype=int)\n",
    "def ars (ins):\n",
    "    ass=np.reshape(ins,(3,3))\n",
    "    return ass\n",
    "print(ars(ins))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transpose**\n",
    "\n",
    "We can generate the transposition of an array using the tool numpy.transpose.\\\n",
    "It will not affect the original array, but it will create a new array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '1,2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asfandyar\\Desktop\\python_Asfandyar\\40 days\\chilla\\Spark\\Certification Spark\\Pyspark-for-Data-Analyst\\Pyspark\\Code\\Spark_Beginner_to_advance.ipynb Cell 788\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z2212sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ins\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49marray(\u001b[39mlist\u001b[39;49m(\u001b[39minput\u001b[39;49m()\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)),dtype\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asfandyar/Desktop/python_Asfandyar/40%20days/chilla/Spark/Certification%20Spark/Pyspark-for-Data-Analyst/Pyspark/Code/Spark_Beginner_to_advance.ipynb#Z2212sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(ins)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '1,2'"
     ]
    }
   ],
   "source": [
    "ins=np.array(list(input().split(' ')),dtype=int)\n",
    "print(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 5],\n",
       "       [2, 4, 6]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inss=np.array([[1,2],[3,4],[5,6]])\n",
    "np.transpose(inss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    " import numpy as np\n",
    "\n",
    "arr = np.array([[1,2,3,4],[5,6,7,8]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [5]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[:,[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arr =arr[:,0]\n",
    "print(new_arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+\n",
      "|sale_id|product_name| sale_date|\n",
      "+-------+------------+----------+\n",
      "|      1|     LCPHONE|2000-01-16|\n",
      "|      2|     LCPhone|2000-01-17|\n",
      "|      3|     LcPhOnE|2000-02-18|\n",
      "|      4|  LCKeyCHAiN|2000-02-19|\n",
      "|      5|  LCKeyChain|2000-02-28|\n",
      "|      6|  Matryoshka|2000-03-31|\n",
      "+-------+------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sale_id', 'bigint'), ('product_name', 'string'), ('sale_date', 'string')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data={'sale_id':pd.Series([1,2,3,4,5,6]),\n",
    "'product_name':pd.Series (['LCPHONE','LCPhone','LcPhOnE'   ,'LCKeyCHAiN','LCKeyChain','Matryoshka']),\n",
    "'sale_date':pd.Series(['2000-01-16','2000-01-17','2000-02-18','2000-02-19','2000-02-28','2000-03-31'])}\n",
    "sc=['sale_id','product_name','sale_date']\n",
    "dfs=pd.DataFrame(data)\n",
    "df=spark.createDataFrame(dfs,sc)\n",
    "df.show()\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----+\n",
      "|product_name|sale_date|count|\n",
      "+------------+---------+-----+\n",
      "|  lckeychain|  2000-02|    2|\n",
      "|     lcphone|  2000-01|    2|\n",
      "|     lcphone|  2000-02|    1|\n",
      "|  matryoshka|  2000-03|    1|\n",
      "+------------+---------+-----+\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('product_name',expr(\"trim (Both '' from lower(product_name))\")).withColumn('sale_date',expr(\"date_format(sale_date,'YYYY-MM')\")).groupBy(['sale_date','product_name']).\\\n",
    "    agg(count('product_name').alias('count')).select(['sale_date','count','product_name']).select(['product_name','sale_date','count']).orderBy(col('product_name'),col('sale_date') ).show()\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------------+\n",
      "|product_name|sale_date|count(product_name)|\n",
      "+------------+---------+-------------------+\n",
      "|  lckeychain|  2000-02|                  2|\n",
      "|     lcphone|  2000-01|                  2|\n",
      "|     lcphone|  2000-02|                  1|\n",
      "|  matryoshka|  2000-03|                  1|\n",
      "+------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql('''select product_name,sale_date ,count(product_name) from (select sale_id,lower(trim(product_name)) as product_name,date_format(sale_date,'YYYY-MM') as sale_date\n",
    "          from df) group by 1,2 order by 1,2\n",
    "          ''').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd044dfe803f410830d2077cd20a7505658c0f80bea76037d58d809c64c95f16"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
