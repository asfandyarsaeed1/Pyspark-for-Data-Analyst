{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spark For Data Engineers and Data Scientists**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Frame using Python collection and Pandas Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b86118baf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "! pip install --force-reinstall 'sqlalchemy < 2.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions in c:\\users\\asfandyar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\asfandyar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Asfandyar\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: typing_extensions\n",
      "Version: 4.7.1\n",
      "Summary: Backported and Experimental Type Hints for Python 3.7+\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \"Guido van Rossum, Jukka Lehtosalo, Łukasz Langa, Michael Lee\" <levkivskyi@gmail.com>\n",
      "License: \n",
      "Location: c:\\users\\asfandyar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: black, huggingface-hub, pydantic, SQLAlchemy\n",
      "Files removed: 1\n"
     ]
    }
   ],
   "source": [
    "# ! pip install pandas sqlalchemy mysql-connector-python\n",
    "! pip install --upgrade typing-extensions\n",
    "\n",
    "! pip uninstall typing-extensions\n",
    "! pip install typing-extensions\n",
    "! pip show typing-extensions\n",
    "! pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USER', 'HOST', 'CURRENT_CONNECTIONS', 'TOTAL_CONNECTIONS']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(None, None, 38, 53),\n",
       " ('event_scheduler', 'localhost', 1, 1),\n",
       " ('root', 'localhost', 3, 9)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# establishing the connection\n",
    "conn = mysql.connector.connect(\n",
    "    user='root',password='Huawei@1234qwe', host='localhost', port=3306, database='performance_schema')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('select * from accounts')\n",
    "result = cursor.fetchall()\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Display column names and data\n",
    "print(column_names)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"USER\", StringType(), True),\n",
    "    StructField(\"HOST\", StringType(), True),\n",
    "    StructField(\"CURRENT_CONNECTIONS\", IntegerType(), True),\n",
    "    StructField(\"TOTAL_CONNECTIONS\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=[(None, None, 38, 53),\n",
    " ('event_scheduler', 'localhost', 1, 1),\n",
    " ('root', 'localhost', 3, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              USER       HOST  CURRENT_CONNECTIONS  TOTAL_CONNECTIONS\n",
      "0             None       None                   38                 53\n",
      "1  event_scheduler  localhost                    1                  1\n",
      "2             root  localhost                    3                  8\n"
     ]
    }
   ],
   "source": [
    "pa=pd.DataFrame(nn,columns=column_names)\n",
    "print(pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = spark.createDataFrame(nn,cols)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------------------+-----------------+\n",
      "|           USER|     HOST|CURRENT_CONNECTIONS|TOTAL_CONNECTIONS|\n",
      "+---------------+---------+-------------------+-----------------+\n",
      "|           null|     null|                 38|               53|\n",
      "|event_scheduler|localhost|                  1|                1|\n",
      "|           root|localhost|                  3|                8|\n",
      "+---------------+---------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = spark.createDataFrame(nn,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FacadeDict({})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(mysql+pymysql://root:***@1234qwe@localhost:3306/mydb)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.bind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_list=[1,2,4,5,6,7,8]\n",
    "type(age_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a DataFrame\n",
    "We will use createDataFrame which is availabe on top of spark object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x17f59c77b20>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "    \n",
      "    When ``schema`` is a list of column names, the type of each column\n",
      "    will be inferred from ``data``.\n",
      "    \n",
      "    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "    from ``data``, which should be an RDD of either :class:`Row`,\n",
      "    :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      "    the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      "    Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "    \n",
      "    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      "    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 2.1.0\n",
      "       Added verifySchema.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`RDD` or iterable\n",
      "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      "        :class:`pandas.DataFrame`.\n",
      "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is None.  The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      "        ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
      "        We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
      "    samplingRatio : float, optional\n",
      "        the sample ratio of rows used for inferring\n",
      "    verifySchema : bool, optional\n",
      "        verify data types of every row against schema. Enabled by default.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> l = [('Alice', 1)]\n",
      "    >>> spark.createDataFrame(l).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).collect()\n",
      "    [Row(age=1, name='Alice')]\n",
      "    \n",
      "    >>> rdd = sc.parallelize(l)\n",
      "    >>> spark.createDataFrame(rdd).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      "    >>> df.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> person = rdd.map(lambda r: Person(*r))\n",
      "    >>> df2 = spark.createDataFrame(person)\n",
      "    >>> df2.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> df3 = spark.createDataFrame(rdd, schema)\n",
      "    >>> df3.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "    [Row(name='Alice', age=1)]\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    [Row(0=1, 1=2)]\n",
      "    \n",
      "    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "    [Row(a='Alice', b=1)]\n",
      "    >>> rdd = rdd.map(lambda row: row[1])\n",
      "    >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      "    [Row(value=1)]\n",
      "    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    Py4JJavaError: ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help (spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    4|\n",
      "|    5|\n",
      "|    6|\n",
      "|    7|\n",
      "|    8|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(age_list,'int').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    4|\n",
      "|    5|\n",
      "|    6|\n",
      "|    7|\n",
      "|    8|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "spark.createDataFrame(age_list,IntegerType()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = ['asfand','saeed','ali','salman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=spark.createDataFrame(names_list,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|asfand|\n",
      "| saeed|\n",
      "|   ali|\n",
      "|salman|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating Multi column using Python List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating single column out of list of tuples\n",
    "\n",
    "\n",
    "user_lists= [(2,),(1,),(3,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list=[(120,),(122,),(123,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (user_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark will throw an error if I pass just int and dont specify the name of the column\n",
    "sp=spark.createDataFrame(user_list,'id int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|120|\n",
      "|122|\n",
      "|123|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating multi column dataframe from list of tuples\n",
    "users_list=[(1,'asfand'),(2,'saeed'),(3,'ali')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|id int|name string|\n",
      "+------+-----------+\n",
      "|    11|     asfand|\n",
      "|    22|      saeed|\n",
      "|    33|       khan|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user=[(11,'asfand'),(22,'saeed'),(33,'khan')]\n",
    "spark.createDataFrame(user,['id int','name string']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp=spark.createDataFrame(users_list,['id int','name string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|id int|name string|\n",
      "+------+-----------+\n",
      "|     1|     asfand|\n",
      "|     2|      saeed|\n",
      "|     3|        ali|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id int: long (nullable = true)\n",
      " |-- name string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp=spark.createDataFrame(users_list,'id int,name string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Row Detail**\n",
    "* Spark data frame is nothing its collection of row objects. To see in detail I have used collect function that will convert the dataframe into python list of rows.\n",
    "* Row is constructed under SQL module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='asfand'), Row(id=2, name='saeed'), Row(id=3, name='ali')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql    import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row(*args, **kwargs)\n",
    "* First is list of arguments\n",
    "* Second is list of key , values arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23c6b497b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=[Row(id=1,name='asfand')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saeed'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows1=Row(2,'saeed')\n",
    "rows1[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(rows).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asfand'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asfand'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Converting List of List into Data Frame using ROW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Row(1, 'asfand')>, <Row(2, 'saeed')>, <Row(3, 'khan')>]\n",
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nas=[[1,'asfand'],[2,'saeed'],[3,'khan']]\n",
    "### convert into row and then to dataframe\n",
    "nass=[ Row(*i)  for i in nas]\n",
    "print (nass)\n",
    "spark.createDataFrame(nass,'id int,name string').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Converting List of Tuples into Data Frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_list=[(1,'asfand'),(2,'saeed'),(3,'ali')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Converting the list of tuples into dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23c6b497b20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|id int|names string|\n",
      "+------+------------+\n",
      "|     1|      asfand|\n",
      "|     2|       saeed|\n",
      "|     3|         ali|\n",
      "+------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'asfand'), (2, 'saeed'), (3, 'ali')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(users_list,['id int','names string']).show()\n",
    "users_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp=spark.createDataFrame(users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| _1|    _2|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Converting List of Tuples into List of Rows and Converting into DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumy (*list):\n",
    "    print(list)\n",
    "    print (len(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 'asfand'),)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# if we dont put * at the start it will make it single unit as an out put.\n",
    "users=(1,'asfand')\n",
    "dumy (users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'asfand')\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# by having the number of argument 2 we can make it two columns with one row \n",
    "dumy (*users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do the row comprehansions\n",
    "user= [Row(*us) for us in users_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Row(1, 'asfand')>, <Row(2, 'saeed')>, <Row(3, 'ali')>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we go with the list of Rows\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp=spark.createDataFrame(user,'id int , names string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| names|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- names: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert List of Dicts ito Spark DataFrame Using Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dicts=[\n",
    "    {'id' : 1 , 'name' : 'asfand'}\n",
    "    ,{'id' : 2 , 'name' :'saeed'}\n",
    "    ,{'id' : 3 , 'name' : 'ali'}\n",
    "    ,{'id' : 4 , 'name' :'khan'}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(user_dicts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Converting the list of dicts via \\*args and \\*\\*args**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([2, 'saeed'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To understand take one element and convert it into Row then we will use for loop for whole list of Dicts\n",
    "usr=user_dicts[1].values()\n",
    "usr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(*list):\n",
    "    print(list)\n",
    "    print(len(list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(dict_values([2, 'saeed']),)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "dump(usr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'saeed')\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "dump(*usr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr1=[Row(*us.values()) for us in user_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Row(1, 'asfand')>, <Row(2, 'saeed')>, <Row(3, 'ali')>, <Row(4, 'khan')>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp=spark.createDataFrame(usr1, 'id int ,user_name string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|user_name|\n",
      "+---+---------+\n",
      "|  1|   asfand|\n",
      "|  2|    saeed|\n",
      "|  3|      ali|\n",
      "|  4|     khan|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2, 'name': 'saeed'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_dicts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr11= user_dicts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumps(**ars):\n",
    "    print (ars)\n",
    "    print(len(ars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 2, 'name': 'saeed'}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# this has considered id and name as 2 different arguments\n",
    "dumps(**user_dicts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all the dataset we have below things.\n",
    "dp=[Row (**us) for us in user_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='asfand'),\n",
       " Row(id=2, name='saeed'),\n",
       " Row(id=3, name='ali'),\n",
       " Row(id=4, name='khan')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp1=spark.createDataFrame(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dp1.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **List of Dic to Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1=[\n",
    "    {\"id\":1,\n",
    "     \"name\": 'asfand',\n",
    "     \"email\" : 'asfand@addo.ai',\n",
    "     \"is_customer\": True,\n",
    "     \"amount\":1000.32,\n",
    "     'customr_from':datetime.datetime(2021,1,15),\n",
    "     'last_update': datetime.datetime(2021,2,10,1,15,0)       \n",
    "    },\n",
    " {   \"id\":2,\n",
    "     \"name\": 'saeed',\n",
    "     \"email\" : 'saeed@addo.ai',\n",
    "     \"is_customer\": True,\n",
    "     \"amount\":900.32,\n",
    "     'customr_from':datetime.datetime(2021,2,25),\n",
    "     'last_update': datetime.datetime(2021,3,11,2,10,0)\n",
    "},\n",
    "  {   \"id\":3,\n",
    "     \"name\": 'ali',\n",
    "     \"email\" : 'ali@addo.ai',\n",
    "     \"is_customer\": False,\n",
    "     \"amount\":None,\n",
    "     'customr_from':None,\n",
    "     'last_update': datetime.datetime(2021,3,11,2,10,0)\n",
    "},\n",
    "    {   \"id\":4,\n",
    "     \"name\": 'khan',\n",
    "     \"email\" : 'khan@addo.ai',\n",
    "     \"is_customer\": False,\n",
    "     \"amount\":None,\n",
    "     'customr_from':None,\n",
    "     'last_update': datetime.datetime(2021,3,12,4,10,0)\n",
    "},\n",
    "        {   \"id\":5,\n",
    "     \"name\": 'He',\n",
    "     \"email\" : 'he@addo.ai',\n",
    "     \"is_customer\": False,\n",
    "     \"amount\":None,\n",
    "     'customr_from':None,\n",
    "     'last_update': datetime.datetime(2022,4,12,4,10,0)\n",
    "}   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros=[Row (**us) for us in user1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='asfand', email='asfand@addo.ai', is_customer=True, amount=1000.32, customr_from=datetime.datetime(2021, 1, 15, 0, 0), last_update=datetime.datetime(2021, 2, 10, 1, 15)),\n",
       " Row(id=2, name='saeed', email='saeed@addo.ai', is_customer=True, amount=900.32, customr_from=datetime.datetime(2021, 2, 25, 0, 0), last_update=datetime.datetime(2021, 3, 11, 2, 10)),\n",
       " Row(id=3, name='ali', email='ali@addo.ai', is_customer=False, amount=None, customr_from=None, last_update=datetime.datetime(2021, 3, 11, 2, 10)),\n",
       " Row(id=4, name='khan', email='khan@addo.ai', is_customer=False, amount=None, customr_from=None, last_update=datetime.datetime(2021, 3, 12, 4, 10)),\n",
       " Row(id=5, name='He', email='he@addo.ai', is_customer=False, amount=None, customr_from=None, last_update=datetime.datetime(2022, 4, 12, 4, 10))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------+---+-----------+-------------------+------+\n",
      "| amount|       customr_from|         email| id|is_customer|        last_update|  name|\n",
      "+-------+-------------------+--------------+---+-----------+-------------------+------+\n",
      "|1000.32|2021-01-15 00:00:00|asfand@addo.ai|  1|       true|2021-02-10 01:15:00|asfand|\n",
      "| 900.32|2021-02-25 00:00:00| saeed@addo.ai|  2|       true|2021-03-11 02:10:00| saeed|\n",
      "|   null|               null|   ali@addo.ai|  3|      false|2021-03-11 02:10:00|   ali|\n",
      "|   null|               null|  khan@addo.ai|  4|      false|2021-03-12 04:10:00|  khan|\n",
      "|   null|               null|    he@addo.ai|  5|      false|2022-04-12 04:10:00|    He|\n",
      "+-------+-------------------+--------------+---+-----------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(user1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr12=[Row(**us) for us in user1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='asfand', email='asfand@addo.ai', is_customer=True, amount=1000.32, customr_from=datetime.datetime(2021, 1, 15, 0, 0), last_update=datetime.datetime(2021, 2, 10, 1, 15)),\n",
       " Row(id=2, name='saeed', email='saeed@addo.ai', is_customer=True, amount=900.32, customr_from=datetime.datetime(2021, 2, 25, 0, 0), last_update=datetime.datetime(2021, 3, 11, 2, 10)),\n",
       " Row(id=3, name='ali', email='ali@addo.ai', is_customer=False, amount=None, customr_from=None, last_update=datetime.datetime(2021, 3, 11, 2, 10)),\n",
       " Row(id=4, name='khan', email='khan@addo.ai', is_customer=False, amount=None, customr_from=None, last_update=datetime.datetime(2021, 3, 12, 4, 10)),\n",
       " Row(id=5, name='He', email='he@addo.ai', is_customer=False, amount=None, customr_from=None, last_update=datetime.datetime(2022, 4, 12, 4, 10))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usr12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp = spark.createDataFrame(usr12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------+-----------+-------+-------------------+-------------------+\n",
      "| id|  name|         email|is_customer| amount|       customr_from|        last_update|\n",
      "+---+------+--------------+-----------+-------+-------------------+-------------------+\n",
      "|  1|asfand|asfand@addo.ai|       true|1000.32|2021-01-15 00:00:00|2021-02-10 01:15:00|\n",
      "|  2| saeed| saeed@addo.ai|       true| 900.32|2021-02-25 00:00:00|2021-03-11 02:10:00|\n",
      "|  3|   ali|   ali@addo.ai|      false|   null|               null|2021-03-11 02:10:00|\n",
      "|  4|  khan|  khan@addo.ai|      false|   null|               null|2021-03-12 04:10:00|\n",
      "|  5|    He|    he@addo.ai|      false|   null|               null|2022-04-12 04:10:00|\n",
      "+---+------+--------------+-----------+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'email', 'is_customer', 'amount', 'customr_from', 'last_update']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'),\n",
       " ('name', 'string'),\n",
       " ('email', 'string'),\n",
       " ('is_customer', 'boolean'),\n",
       " ('amount', 'double'),\n",
       " ('customr_from', 'timestamp'),\n",
       " ('last_update', 'timestamp')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssp.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Specify Schema As String**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that all the values on there position must have same datatype\n",
    "\n",
    "usr_string=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     1000.2,\n",
    "     datetime.datetime(2022,4,12,4,10,0)\n",
    "        ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     922.2,\n",
    "     datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     111.2,\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "     112.2,\n",
    "     datetime.datetime(2022,4,21,5,10,0))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schema as string\n",
    "usrs='''\n",
    "id int,\n",
    "name string,\n",
    "is_customer boolean,\n",
    "amount float,\n",
    "updated_instance timestamp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1=spark.createDataFrame(usr_string,schema=usrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+------+-------------------+\n",
      "| id|  name|is_customer|amount|   updated_instance|\n",
      "+---+------+-----------+------+-------------------+\n",
      "|  1|asfand|       true|1000.2|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true| 922.2|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false| 111.2|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false| 112.2|2022-04-21 05:10:00|\n",
      "+---+------+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Specify Schema for Spark Dataframe Using List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that all the values on there position must have same datatype\n",
    "\n",
    "usr_schema_list=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     1000.2,\n",
    "     datetime.datetime(2022,4,12,4,10,0)\n",
    "        ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     922.2,\n",
    "     datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     111.2,\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "     112.2,\n",
    "     datetime.datetime(2022,4,21,5,10,0))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schema via list of columns when such things happens we just have to pass list of column name and datatype will be infered\n",
    "user_schema=[\n",
    "    'id',\n",
    "    'name',\n",
    "    'is_customer',\n",
    "    'amount',\n",
    "    'update_record' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2=spark.createDataFrame(usr_schema_list,schema=user_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+------+-------------------+\n",
      "| id|  name|is_customer|amount|      update_record|\n",
      "+---+------+-----------+------+-------------------+\n",
      "|  1|asfand|       true|1000.2|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true| 922.2|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false| 111.2|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false| 112.2|2022-04-21 05:10:00|\n",
      "+---+------+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- update_record: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp2.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note::\\\n",
    "> The only difference between list of columns and string of columns with data type is that we cant specify any data type while we are using List of columns strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Specify Schema Using Spark Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that all the values on there position must have same datatype\n",
    "\n",
    "usr_schema_type=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     1000.2,\n",
    "     datetime.datetime(2022,4,12,4,10,0)\n",
    "        ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     922.2,\n",
    "     datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     111.2,\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "     112.2,\n",
    "     datetime.datetime(2022,4,21,5,10,0))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "she=StructType([\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('name', StringType()),\n",
    "    StructField('is_customer', BooleanType()),\n",
    "    StructField('amount', FloatType()),\n",
    "    StructField('update', TimestampType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp3=spark.createDataFrame(usr_schema_type,she)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+------+-------------------+\n",
      "| id|  name|is_customer|amount|             update|\n",
      "+---+------+-----------+------+-------------------+\n",
      "|  1|asfand|       true|1000.2|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true| 922.2|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false| 111.2|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false| 112.2|2022-04-21 05:10:00|\n",
      "+---+------+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2a8726c7b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Spark DataFrame Using Pandas Data Frame**\n",
    "* when there is any missing row in the dict,tuple and list then you will not be able to make spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_panda=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'amount': 1000.2,\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'amount':922.2,\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'amount':111.2,\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'amount':112.2}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert it into collection of ROW list and than convert to spark dataframe\n",
    "user_row=[Row(**us)  for us in usr_panda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='asfand', is_customer=True, amount=1000.2, update=datetime.datetime(2022, 4, 12, 4, 10)),\n",
       " Row(id=2, name='saeed', is_customer=True, amount=922.2, update=datetime.datetime(2021, 4, 11, 4, 10)),\n",
       " Row(id=3, name='ali', is_customer=False, amount=111.2, update=datetime.datetime(2021, 4, 22, 12, 10)),\n",
       " Row(id=4, name='khan', is_customer=False, amount=112.2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert to sparkdataframe you will find error as there is one row missing\n",
    "sp4=spark.createDataFrame(user_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to tickle this issue first of all go for pandas as the missing rows will be replaced by NaN\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>is_customer</th>\n",
       "      <th>amount</th>\n",
       "      <th>update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>asfand</td>\n",
       "      <td>True</td>\n",
       "      <td>1000.2</td>\n",
       "      <td>2022-04-12 04:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>saeed</td>\n",
       "      <td>True</td>\n",
       "      <td>922.2</td>\n",
       "      <td>2021-04-11 04:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ali</td>\n",
       "      <td>False</td>\n",
       "      <td>111.2</td>\n",
       "      <td>2021-04-22 12:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>khan</td>\n",
       "      <td>False</td>\n",
       "      <td>112.2</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    name  is_customer  amount              update\n",
       "0   1  asfand         True  1000.2 2022-04-12 04:10:00\n",
       "1   2   saeed         True   922.2 2021-04-11 04:10:00\n",
       "2   3     ali        False   111.2 2021-04-22 12:10:00\n",
       "3   4    khan        False   112.2                 NaT"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(usr_panda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp5=spark.createDataFrame(pd.DataFrame(usr_panda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+------+-------------------+\n",
      "| id|  name|is_customer|amount|             update|\n",
      "+---+------+-----------+------+-------------------+\n",
      "|  1|asfand|       true|1000.2|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true| 922.2|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false| 111.2|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false| 112.2|               null|\n",
      "+---+------+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd=spark.read.option('multiline','true').json('iris.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-------+\n",
      "|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "|        1.4|       0.2|        5.1|       3.5| setosa|\n",
      "|        1.4|       0.2|        4.9|       3.0| setosa|\n",
      "|        1.3|       0.2|        4.7|       3.2| setosa|\n",
      "|        1.5|       0.2|        4.6|       3.1| setosa|\n",
      "|        1.4|       0.2|        5.0|       3.6| setosa|\n",
      "|        1.7|       0.4|        5.4|       3.9| setosa|\n",
      "|        1.4|       0.3|        4.6|       3.4| setosa|\n",
      "|        1.5|       0.2|        5.0|       3.4| setosa|\n",
      "|        1.4|       0.2|        4.4|       2.9| setosa|\n",
      "|        1.5|       0.1|        4.9|       3.1| setosa|\n",
      "|        1.5|       0.2|        5.4|       3.7| setosa|\n",
      "|        1.6|       0.2|        4.8|       3.4| setosa|\n",
      "|        1.4|       0.1|        4.8|       3.0| setosa|\n",
      "|        1.1|       0.1|        4.3|       3.0| setosa|\n",
      "|        1.2|       0.2|        5.8|       4.0| setosa|\n",
      "|        1.5|       0.4|        5.7|       4.4| setosa|\n",
      "|        1.3|       0.4|        5.4|       3.9| setosa|\n",
      "|        1.4|       0.3|        5.1|       3.5| setosa|\n",
      "|        1.7|       0.3|        5.7|       3.8| setosa|\n",
      "|        1.5|       0.3|        5.1|       3.8| setosa|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Arry Type Columns in Spark DataFrame**\n",
    "* Here we use Explode the data and Explode_outer the data.\n",
    "* Explode have the requirement of removing the null value row from the output where as Explode_out will include the null values rows as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Arry=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': ['+92329677783','+9234245672'],\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': ['+92329672283','+9234236672'],\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': ['+92329634283','+9234242222'],\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': None,#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "use=[Row(**us) for us in usr_Arry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='asfand', is_customer=True, phone=['+92329677783', '+9234245672'], update=datetime.datetime(2022, 4, 12, 4, 10)),\n",
       " Row(id=2, name='saeed', is_customer=True, phone=['+92329672283', '+9234236672'], update=datetime.datetime(2021, 4, 11, 4, 10)),\n",
       " Row(id=3, name='ali', is_customer=False, phone=['+92329634283', '+9234242222'], update=datetime.datetime(2021, 4, 22, 12, 10)),\n",
       " Row(id=4, name='khan', is_customer=False, phone=None, update=None)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp1=spark.createDataFrame(use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+---------------------------+-------------------+\n",
      "|id |name  |is_customer|phone                      |update             |\n",
      "+---+------+-----------+---------------------------+-------------------+\n",
      "|1  |asfand|true       |[+92329677783, +9234245672]|2022-04-12 04:10:00|\n",
      "|2  |saeed |true       |[+92329672283, +9234236672]|2021-04-11 04:10:00|\n",
      "|3  |ali   |false      |[+92329634283, +9234242222]|2021-04-22 12:10:00|\n",
      "|4  |khan  |false      |null                       |null               |\n",
      "+---+------+-----------+---------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|               phone|\n",
      "+---+--------------------+\n",
      "|  1|[+92329677783, +9...|\n",
      "|  2|[+92329672283, +9...|\n",
      "|  3|[+92329634283, +9...|\n",
      "|  4|                null|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp1.select('id','phone').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'is_customer', 'phone', 'update']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data first import the col and then separate the first and second number from eachother\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+-------------+\n",
      "| id|               phone| firs_number|second_number|\n",
      "+---+--------------------+------------+-------------+\n",
      "|  1|[+92329677783, +9...|+92329677783|  +9234245672|\n",
      "|  2|[+92329672283, +9...|+92329672283|  +9234236672|\n",
      "|  3|[+92329634283, +9...|+92329634283|  +9234242222|\n",
      "|  4|                null|        null|         null|\n",
      "+---+--------------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp1.select('id','phone',col('phone')[0].alias('firs_number'),col('phone')[1].alias('second_number')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+\n",
      "| id|first_number| 2nd_number|\n",
      "+---+------------+-----------+\n",
      "|  1|+92329677783|+9234245672|\n",
      "|  2|+92329672283|+9234236672|\n",
      "|  3|+92329634283|+9234242222|\n",
      "|  4|        null|       null|\n",
      "+---+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp1.select('id',col('phone')[0].alias('first_number'),col('phone')[1].alias('2nd_number')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-------------------+------------+\n",
      "| id|  name|is_customer|             update|   new_phone|\n",
      "+---+------+-----------+-------------------+------------+\n",
      "|  1|asfand|       true|2022-04-12 04:10:00|+92329677783|\n",
      "|  1|asfand|       true|2022-04-12 04:10:00| +9234245672|\n",
      "|  2| saeed|       true|2021-04-11 04:10:00|+92329672283|\n",
      "|  2| saeed|       true|2021-04-11 04:10:00| +9234236672|\n",
      "|  3|   ali|      false|2021-04-22 12:10:00|+92329634283|\n",
      "|  3|   ali|      false|2021-04-22 12:10:00| +9234242222|\n",
      "+---+------+-----------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now apply explode function\n",
    "spp1.withColumn('new_phone',explode('phone')).drop('phone').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-------------------+------------+\n",
      "| id|  name|is_customer|             update|   new_phone|\n",
      "+---+------+-----------+-------------------+------------+\n",
      "|  1|asfand|       true|2022-04-12 04:10:00|+92329677783|\n",
      "|  1|asfand|       true|2022-04-12 04:10:00| +9234245672|\n",
      "|  2| saeed|       true|2021-04-11 04:10:00|+92329672283|\n",
      "|  2| saeed|       true|2021-04-11 04:10:00| +9234236672|\n",
      "|  3|   ali|      false|2021-04-22 12:10:00|+92329634283|\n",
      "|  3|   ali|      false|2021-04-22 12:10:00| +9234242222|\n",
      "|  4|  khan|      false|               null|        null|\n",
      "+---+------+-----------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp1.withColumn('new_phone',explode_outer('phone')).drop('phone').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **List of Tuple to Pandas and then to Spark DataFrame**\n",
    "* First We will add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Map=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     {\"+92329677783\",\"+92342454672\"},\n",
    "      datetime.datetime(2022,4,12,4,10,0)\n",
    "    ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     {\"+92329622283\", \"+92342442312\"},\n",
    "      datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     {\"+92329644483\", \"+92342445552\"},\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "      None,#['+92329633456','+9234247654'],\n",
    "     None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType,BooleanType,TimestampType,ArrayType\n",
    "\n",
    "scha=StructType([StructField('id', IntegerType()),\n",
    "    StructField('name', StringType()),\n",
    "    StructField('is_customer', BooleanType()),\n",
    "    StructField('phone', StringType()),\n",
    "    StructField('update', TimestampType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>is_customer</th>\n",
       "      <th>phone</th>\n",
       "      <th>update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>asfand</td>\n",
       "      <td>True</td>\n",
       "      <td>{+92329677783, +92342454672}</td>\n",
       "      <td>2022-04-12 04:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>saeed</td>\n",
       "      <td>True</td>\n",
       "      <td>{+92342442312, +92329622283}</td>\n",
       "      <td>2021-04-11 04:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ali</td>\n",
       "      <td>False</td>\n",
       "      <td>{+92329644483, +92342445552}</td>\n",
       "      <td>2021-04-22 12:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>khan</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    name  is_customer                         phone              update\n",
       "0   1  asfand         True  {+92329677783, +92342454672} 2022-04-12 04:10:00\n",
       "1   2   saeed         True  {+92342442312, +92329622283} 2021-04-11 04:10:00\n",
       "2   3     ali        False  {+92329644483, +92342445552} 2021-04-22 12:10:00\n",
       "3   4    khan        False                          None                 NaT"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### now creating pandas Dataframe\n",
    "df=pd.DataFrame(usr_Map,columns=['id','name','is_customer','phone','update'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|[+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|[+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|[+92342445552, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|                null|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp=spark.createDataFrame(df,scha)\n",
    "sp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Map Type Columns in Spark DataFrame**\n",
    "* When we have Key value nest it is called Map\n",
    "\n",
    "> Note ::\\\n",
    ">  we can use explode function just with map and array type column data not struct ie. row('name'='asfand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Map=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': {\"first\":\"+92329677783\", \"Second\":\"+92342454672\"},\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': {\"first\":\"+92329622283\", \"Second\":\"+92342442312\"},\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': {\"first\":\"+92329644483\", \"Second\":\"+92342445552\"},\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': None,#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import Row\n",
    "# Now convert to Row list\n",
    "uses=[Row(**us) for us in usr_Map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_Maps=spark.createDataFrame(uses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-----------------------------------------------+-------------------+\n",
      "|id |name  |is_customer|phone                                          |update             |\n",
      "+---+------+-----------+-----------------------------------------------+-------------------+\n",
      "|1  |asfand|true       |{Second -> +92342454672, first -> +92329677783}|2022-04-12 04:10:00|\n",
      "|2  |saeed |true       |{Second -> +92342442312, first -> +92329622283}|2021-04-11 04:10:00|\n",
      "|3  |ali   |false      |{Second -> +92342445552, first -> +92329644483}|2021-04-22 12:10:00|\n",
      "|4  |khan  |false      |null                                           |null               |\n",
      "+---+------+-----------+-----------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+-------------+\n",
      "| id|  name|is_customer|               phone|             update|phone[Second]|\n",
      "+---+------+-----------+--------------------+-------------------+-------------+\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00| +92342454672|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00| +92342442312|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00| +92342445552|\n",
      "|  4|  khan|      false|                null|               null|         null|\n",
      "+---+------+-----------+--------------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select (\"*\",col('phone')['Second']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "| id|  name|is_customer|               phone|             update|      Second|\n",
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00|+92342454672|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00|+92342442312|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00|+92342445552|\n",
      "|  4|  khan|      false|                null|               null|        null|\n",
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select(\"*\",col('phone.Second')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+-------------+\n",
      "| id|  name|is_customer|               phone|             update|phone[Second]|\n",
      "+---+------+-----------+--------------------+-------------------+-------------+\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00| +92342454672|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00| +92342442312|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00| +92342445552|\n",
      "|  4|  khan|      false|                null|               null|         null|\n",
      "+---+------+-----------+--------------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select(\"*\",use_Maps['phone']['Second']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-----------------------------------------------+-------------------+\n",
      "|id |name  |is_customer|phone                                          |update             |\n",
      "+---+------+-----------+-----------------------------------------------+-------------------+\n",
      "|1  |asfand|true       |{Second -> +92342454672, first -> +92329677783}|2022-04-12 04:10:00|\n",
      "|2  |saeed |true       |{Second -> +92342442312, first -> +92329622283}|2021-04-11 04:10:00|\n",
      "|3  |ali   |false      |{Second -> +92342445552, first -> +92329644483}|2021-04-22 12:10:00|\n",
      "|4  |khan  |false      |null                                           |null               |\n",
      "+---+------+-----------+-----------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select (\"*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------------+\n",
      "| id|  name| types|      phones|\n",
      "+---+------+------+------------+\n",
      "|  1|asfand|Second|+92342454672|\n",
      "|  1|asfand| first|+92329677783|\n",
      "|  2| saeed|Second|+92342442312|\n",
      "|  2| saeed| first|+92329622283|\n",
      "|  3|   ali|Second|+92342445552|\n",
      "|  3|   ali| first|+92329644483|\n",
      "+---+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select('id','name',explode(col('phone'))).withColumnRenamed('key','types').withColumnRenamed('value','phones').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'phone[Second]'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_Maps['phone']['Second']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+------------+\n",
      "| id|  name|phone[Second]|       first|\n",
      "+---+------+-------------+------------+\n",
      "|  1|asfand| +92342454672|+92329677783|\n",
      "|  2| saeed| +92342442312|+92329622283|\n",
      "|  3|   ali| +92342445552|+92329644483|\n",
      "|  4|  khan|         null|        null|\n",
      "+---+------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select('id','name',use_Maps['phone']['Second'],col('phone.first')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-------------------+------------+------------+\n",
      "| id|  name|is_customer|             update|     first_n|      Second|\n",
      "+---+------+-----------+-------------------+------------+------------+\n",
      "|  1|asfand|       true|2022-04-12 04:10:00|+92329677783|+92342454672|\n",
      "|  2| saeed|       true|2021-04-11 04:10:00|+92329622283|+92342442312|\n",
      "|  3|   ali|      false|2021-04-22 12:10:00|+92329644483|+92342445552|\n",
      "|  4|  khan|      false|               null|        null|        null|\n",
      "+---+------+-----------+-------------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select (\"*\").withColumn('first_n',col('phone.first')).withColumn('Second',col('phone.Second')).drop('phone').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "| id|  name|is_customer|               phone|             update|     first_n|\n",
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00|+92329677783|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00|+92329622283|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00|+92329644483|\n",
      "|  4|  khan|      false|                null|               null|        null|\n",
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select(\"*\").withColumn('first_n', col('phone')['first']).show()\n",
    "# .withColumn('Second',col(['phone'])['Second']).drop('phone').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+----------+------------+\n",
      "| id|  name|is_customer|               phone|             update|Phone_Type|      Number|\n",
      "+---+------+-----------+--------------------+-------------------+----------+------------+\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00|    Second|+92342454672|\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00|     first|+92329677783|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00|    Second|+92342442312|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00|     first|+92329622283|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00|    Second|+92342445552|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00|     first|+92329644483|\n",
      "+---+------+-----------+--------------------+-------------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using Explode function we get the column of key and values\n",
    "use_Maps.select(\"*\",explode(\"phone\")).\\\n",
    "    withColumnRenamed(\"key\",\"Phone_Type\").\\\n",
    "    withColumnRenamed(\"value\",\"Number\").\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+----------+------------+\n",
      "| id|  name|is_customer|               phone|             update|Phone_Type|      Number|\n",
      "+---+------+-----------+--------------------+-------------------+----------+------------+\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00|    Second|+92342454672|\n",
      "|  1|asfand|       true|{Second -> +92342...|2022-04-12 04:10:00|     first|+92329677783|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00|    Second|+92342442312|\n",
      "|  2| saeed|       true|{Second -> +92342...|2021-04-11 04:10:00|     first|+92329622283|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00|    Second|+92342445552|\n",
      "|  3|   ali|      false|{Second -> +92342...|2021-04-22 12:10:00|     first|+92329644483|\n",
      "+---+------+-----------+--------------------+-------------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_Maps.select(\"*\",explode(\"phone\")).\\\n",
    "    withColumnRenamed(\"key\",\"Phone_Type\").\\\n",
    "    withColumnRenamed(\"value\",\"Number\").\\\n",
    "    show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Spark Data Frame to Select and Rename Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=None), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_struct= [Row(**us) for us in usr_Struct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp2=spark.createDataFrame(user_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: struct (nullable = true)\n",
      " |    |-- first: string (nullable = true)\n",
      " |    |-- Second: string (nullable = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+\n",
      "| id|       first|          bb|\n",
      "+---+------------+------------+\n",
      "|  1|+92329677783|+92329677783|\n",
      "|  2|+92329622283|+92329622283|\n",
      "|  3|+92329644483|+92329644483|\n",
      "|  4|        null|        null|\n",
      "+---+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.select('id',col('phone.first'),col('phone')['first'].alias('bb')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+\n",
      "| id|       first|      Second|\n",
      "+---+------------+------------+\n",
      "|  1|+92329677783|+92342454672|\n",
      "|  2|+92329622283|+92342442312|\n",
      "|  3|+92329644483|+92342445552|\n",
      "|  4|        null|        null|\n",
      "+---+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.select('id','phone.first','phone.Second').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+----------------------------+-------------------+\n",
      "|id |name  |is_customer|phone                       |update             |\n",
      "+---+------+-----------+----------------------------+-------------------+\n",
      "|1  |asfand|true       |{+92329677783, +92342454672}|2022-04-12 04:10:00|\n",
      "|2  |saeed |true       |{+92329622283, +92342442312}|2021-04-11 04:10:00|\n",
      "|3  |ali   |false      |{+92329644483, +92342445552}|2021-04-22 12:10:00|\n",
      "|4  |khan  |false      |{null, null}                |null               |\n",
      "+---+------+-----------+----------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.select('*').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+\n",
      "| id|       first|      Second|\n",
      "+---+------------+------------+\n",
      "|  1|+92329677783|+92342454672|\n",
      "|  2|+92329622283|+92342442312|\n",
      "|  3|+92329644483|+92342445552|\n",
      "|  4|        null|        null|\n",
      "+---+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.select ('id',col('phone.*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+\n",
      "| id|first_contact_number|      Second|\n",
      "+---+--------------------+------------+\n",
      "|  1|        +92329677783|+92342454672|\n",
      "|  2|        +92329622283|+92342442312|\n",
      "|  3|        +92329644483|+92342445552|\n",
      "|  4|                null|        null|\n",
      "+---+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spp2.select ('id',col('phone.*')).withColumnRenamed('first','first_contact_number').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Renaming Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2399eb19e40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  id|\n",
      "+--------------------+\n",
      "|5.000,1.0,0,,IDD_...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usr_Struct=[\n",
    "    {'id':'5.000,1.0,0,,IDD_SMS_KSA_IC,16384,null,0,586,NewBasic,NewBasic,null;1.00,1.0,0,,IDD_SMS_KSA_IC,128,null,1,20005252,null,null,null;-5.000,1.0,0,,IDD_SMS_KSA_IC,128,null,1,586,null,null,null'}]\n",
    "import pandas as pd\n",
    "df=spark.createDataFrame(pd.DataFrame(usr_Struct))\n",
    "df.show()\n",
    "df.createOrReplaceTempView('users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id                                                                                                                                                                                          |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|5.000,1.0,0,,IDD_SMS_KSA_IC,16384,null,0,586,NewBasic,NewBasic,null;1.00,1.0,0,,IDD_SMS_KSA_IC,128,null,1,20005252,null,null,null;-5.000,1.0,0,,IDD_SMS_KSA_IC,128,null,1,586,null,null,null|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from users \"\"\").show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|num_semicolons|\n",
      "+--------------+\n",
      "|             2|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_semicolons = spark.sql(\"SELECT (LENGTH(id) - LENGTH(REPLACE(id, ';', ''))) AS num_semicolons FROM users\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "num_semicolons.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>is_customer</th>\n",
       "      <th>phone</th>\n",
       "      <th>update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>asfand</td>\n",
       "      <td>True</td>\n",
       "      <td>(+92329677783, +92342454672)</td>\n",
       "      <td>2022-04-12 04:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>saeed</td>\n",
       "      <td>True</td>\n",
       "      <td>(+92329622283, +92342442312)</td>\n",
       "      <td>2021-04-11 04:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ali</td>\n",
       "      <td>False</td>\n",
       "      <td>(+92329644483, +92342445552)</td>\n",
       "      <td>2021-04-22 12:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>khan</td>\n",
       "      <td>False</td>\n",
       "      <td>(None, None)</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    name  is_customer                         phone              update\n",
       "0   1  asfand         True  (+92329677783, +92342454672) 2022-04-12 04:10:00\n",
       "1   2   saeed         True  (+92329622283, +92342442312) 2021-04-11 04:10:00\n",
       "2   3     ali        False  (+92329644483, +92342445552) 2021-04-22 12:10:00\n",
       "3   4    khan        False                  (None, None)                 NaT"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(usr_Struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=spark.createDataFrame(pd.DataFrame(usr_Struct))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('u').select ('u.*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias ('s').select (['s.id','s.name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------------------+\n",
      "|id |name  |phone                       |\n",
      "+---+------+----------------------------+\n",
      "|1  |asfand|{+92329677783, +92342454672}|\n",
      "|2  |saeed |{+92329622283, +92342442312}|\n",
      "|3  |ali   |{+92329644483, +92342445552}|\n",
      "|4  |khan  |{null, null}                |\n",
      "+---+------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('u').select(['u.id','u.name','u.phone']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+\n",
      "| id|  name|               phone|\n",
      "+---+------+--------------------+\n",
      "|  1|asfand|{+92329677783, +9...|\n",
      "|  2| saeed|{+92329622283, +9...|\n",
      "|  3|   ali|{+92329644483, +9...|\n",
      "|  4|  khan|        {null, null}|\n",
      "+---+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import concat,lit\n",
    "df.select(col('id'),col('name'),col('phone')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+\n",
      "| id|  name|user_name|\n",
      "+---+------+---------+\n",
      "|  1|asfand| asfand_1|\n",
      "|  2| saeed|  saeed_2|\n",
      "|  3|   ali|    ali_3|\n",
      "|  4|  khan|   khan_4|\n",
      "+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (col('id'),col('name'),concat(df['name'],lit('_'),col('id')).alias('user_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id| name_id|\n",
      "+---+--------+\n",
      "|  1|asfand_1|\n",
      "|  2| saeed_2|\n",
      "|  3|   ali_3|\n",
      "|  4|  khan_4|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('id'),concat(col('name'),lit(\"_\"),col('id')).alias('name_id')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note::\\\n",
    "> We will use col() function allot when it comes to applying functions on top of columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SelectExpr**\n",
    "* It takes strings not col type object like df['id'] and col('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method selectExpr in module pyspark.sql.dataframe:\n",
      "\n",
      "selectExpr(*expr) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      "    \n",
      "    This is a variant of :func:`select` that accepts SQL expressions.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      "    [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.selectExpr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can only use spark SQL function while using selectEXPR not pyspark.sql.functions. When we are using this we don't need to import the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1dab5864b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=spark.createDataFrame(pd.DataFrame(usr_Struct))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select ('id','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (col('id'),col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['id'],df['name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (['id','name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+\n",
      "| id|length(name)| name_id|\n",
      "+---+------------+--------+\n",
      "|  1|           6|asfand_1|\n",
      "|  2|           5| saeed_2|\n",
      "|  3|           3|   ali_3|\n",
      "|  4|           4|  khan_4|\n",
      "+---+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id',length(col('name')),concat(col('name'),lit('_'),col('id')).alias('name_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+\n",
      "|(id * 2)|length(name)| name_id|\n",
      "+--------+------------+--------+\n",
      "|       2|           6|asfand_1|\n",
      "|       4|           5| saeed_2|\n",
      "|       6|           3|   ali_3|\n",
      "|       8|           4|  khan_4|\n",
      "+--------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('id*2',\"length(name)\",\"concat(name,'_',id) as name_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|(id * 2)|  name|\n",
      "+--------+------+\n",
      "|       2|asfand|\n",
      "|       4| saeed|\n",
      "|       6|   ali|\n",
      "|       8|  khan|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('id*2','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|     ids|\n",
      "+--------+\n",
      "|1_asfand|\n",
      "| 2_saeed|\n",
      "|   3_ali|\n",
      "|  4_khan|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"concat(id,'_',name) as ids\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+--------+\n",
      "| id|  name|               phone|     ids|\n",
      "+---+------+--------------------+--------+\n",
      "|  1|asfand|{+92329677783, +9...|asfand_1|\n",
      "|  2| saeed|{+92329622283, +9...| saeed_2|\n",
      "|  3|   ali|{+92329644483, +9...|   ali_3|\n",
      "|  4|  khan|        {null, null}|  khan_4|\n",
      "+---+------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('id','name','phone',\"concat(name,'_',id) as ids\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+\n",
      "| id|  name|     ids|\n",
      "+---+------+--------+\n",
      "|  1|asfand|asfand_1|\n",
      "|  2| saeed| saeed_2|\n",
      "|  3|   ali|   ali_3|\n",
      "|  4|  khan|  khan_4|\n",
      "+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('users')\n",
    "spark.sql('''select id,name,concat(name,'_',id) as ids from users''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|     ids|\n",
      "+---+--------+\n",
      "|  1|asfand_1|\n",
      "|  2| saeed_2|\n",
      "|  3|   ali_3|\n",
      "|  4|  khan_4|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_name=concat(col('name'),lit('_'),col('id')).alias('ids')\n",
    "df.select('id',full_name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding col function in Spark**\n",
    "* Select works on column type object that is col('column name') and df['column name'] and string as column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27289184b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=spark.createDataFrame(pd.DataFrame(usr_Struct))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'name'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'name'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'), ('order_date', 'int')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "dates=date_format('update','yyyyMMdd').cast('int').alias('order_date')\n",
    "df.select('id',dates).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'is_customer', 'phone', 'update']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "colum=['id', 'name', 'is_customer', 'phone', 'update']\n",
    "dff=df.select (*colum).withColumn('date_froms',to_date(col('update')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: struct (nullable = true)\n",
      " |    |-- first: string (nullable = true)\n",
      " |    |-- Second: string (nullable = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      " |-- date_froms: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data_format will get date,timestamp and string as input and as output it will how string what so ever formate you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+----------+-------------+\n",
      "| id|  name|is_customer|               phone|             update|date_froms|date_from_int|\n",
      "+---+------+-----------+--------------------+-------------------+----------+-------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|2022-04-12|     20220412|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|2021-04-11|     20210411|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|2021-04-22|     20210422|\n",
      "|  4|  khan|      false|        {null, null}|               null|      null|         null|\n",
      "+---+------+-----------+--------------------+-------------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.select (\"*\").withColumn('date_from_int',date_format(col('date_froms'),'yyyyMMdd')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: struct (nullable = true)\n",
      " |    |-- first: string (nullable = true)\n",
      " |    |-- Second: string (nullable = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      " |-- date_froms: date (nullable = true)\n",
      " |-- date_from_int: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.select (\"*\").withColumn('date_from_int',date_format(col('date_froms'),'yyyyMMdd')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: struct (nullable = true)\n",
      " |    |-- first: string (nullable = true)\n",
      " |    |-- Second: string (nullable = true)\n",
      " |-- update: timestamp (nullable = true)\n",
      " |-- date_froms: date (nullable = true)\n",
      " |-- date_from_int: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.select (\"*\").withColumn('date_from_int',date_format(col('date_froms'),'yyyyMMdd').cast('integer')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   from_dates\n",
      "0  2023-09-09\n",
      "1  2023-09-01\n",
      "2  2023-09-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asfandyar\\AppData\\Local\\Temp\\ipykernel_27504\\274203270.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_test=df_test.append({'from_dates':'2023-09-03'},ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df_test=pd.DataFrame(columns=['from_dates'])\n",
    "df_test['from_dates']=['2023-09-09','2023-09-01']\n",
    "df_test=df_test.append({'from_dates':'2023-09-03'},ignore_index=True)\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from_dates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_spark=spark.createDataFrame(df_test)\n",
    "df_test_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|from_dates|     dates|\n",
      "+----------+----------+\n",
      "|2023-09-09|2023/09/09|\n",
      "|2023-09-01|2023/09/01|\n",
      "|2023-09-03|2023/09/03|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_spark.select ('*').withColumn('dates',date_format(col('from_dates'),'yyyy/MM/dd')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'date_format(2023-09-09, yyyy/MM/dd)'>\n"
     ]
    }
   ],
   "source": [
    "print(date_format('2023-09-09','yyyy/MM/dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_format in module pyspark.sql.functions:\n",
      "\n",
      "date_format(date, format)\n",
      "    Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "    format given by the second argument.\n",
      "    \n",
      "    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "    pattern letters of `datetime pattern`_. can be used.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Whenever possible, use specialized functions like `year`.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "    [Row(date='04/08/2015')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- order_date: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id',dates).printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Understanding lit() in Spark**\n",
    "* If we want to add ,subtract or do any sort of operation in spark we can do that by lit as it returns column type object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d0e0a127d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql  import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "     \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=spark.createDataFrame(pd.DataFrame(usr_Struct))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|asfand|\n",
      "|  2| saeed|\n",
      "|  3|   ali|\n",
      "|  4|  khan|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('id','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|sum_20|\n",
      "+---+------+------+\n",
      "|  1|asfand|    21|\n",
      "|  2| saeed|    22|\n",
      "|  3|   ali|    23|\n",
      "|  4|  khan|    24|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (col('id'),col('name'),(col('id')+lit(20)).alias ('sum_20')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Renaming Spark DataFrame Columns or Expression**\n",
    "* If we want to add new column we will use withColumn function.\n",
    "* WithColumn function can help in renaming of the existing columns as well but it will add up new column which we need to drop in future where as withColumnRenamed will just rename the selected column.\n",
    "* withColumnRenamed function will help in renaming the columns.\n",
    "* Alias is also used to rename the column.\n",
    "* to_DF is use to rename bunch of columns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **WithColumn and WithColumnRenamed**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task is to use withColumn and withColumnRenamed to show difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method withColumn in module pyspark.sql.dataframe:\n",
      "\n",
      "withColumn(colName, col) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      "    existing column that has the same name.\n",
      "    \n",
      "    The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      "    a column from some other :class:`DataFrame` will raise an error.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    colName : str\n",
      "        string, name of the new column.\n",
      "    col : :class:`Column`\n",
      "        a :class:`Column` expression for the new column.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This method introduces a projection internally. Therefore, calling it multiple\n",
      "    times, for instance, via loops in order to add multiple columns can generate big\n",
      "    plans which can cause performance issues and even `StackOverflowException`.\n",
      "    To avoid this, use :func:`select` with the multiple columns at once.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.withColumn('age2', df.age + 2).collect()\n",
      "    [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.withColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0),\n",
    "     'course':[1,2]\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0),\n",
    "     'course':[1,3,2]},\n",
    "    \n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0),\n",
    "     'course':[1,2,4,5]\n",
    "     \n",
    "     },\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None, #datetime.datetime(2022,5,1,10,10,0)}\n",
    "     'course':[]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>is_customer</th>\n",
       "      <th>phone</th>\n",
       "      <th>update</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>asfand</td>\n",
       "      <td>True</td>\n",
       "      <td>(+92329677783, +92342454672)</td>\n",
       "      <td>2022-04-12 04:10:00</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>saeed</td>\n",
       "      <td>True</td>\n",
       "      <td>(+92329622283, +92342442312)</td>\n",
       "      <td>2021-04-11 04:10:00</td>\n",
       "      <td>[1, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ali</td>\n",
       "      <td>False</td>\n",
       "      <td>(+92329644483, +92342445552)</td>\n",
       "      <td>2021-04-22 12:10:00</td>\n",
       "      <td>[1, 2, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>khan</td>\n",
       "      <td>False</td>\n",
       "      <td>(None, None)</td>\n",
       "      <td>NaT</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    name  is_customer                         phone              update  \\\n",
       "0   1  asfand         True  (+92329677783, +92342454672) 2022-04-12 04:10:00   \n",
       "1   2   saeed         True  (+92329622283, +92342442312) 2021-04-11 04:10:00   \n",
       "2   3     ali        False  (+92329644483, +92342445552) 2021-04-22 12:10:00   \n",
       "3   4    khan        False                  (None, None)                 NaT   \n",
       "\n",
       "         course  \n",
       "0        [1, 2]  \n",
       "1     [1, 3, 2]  \n",
       "2  [1, 2, 4, 5]  \n",
       "3            []  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa=pd.DataFrame(usr_Struct)\n",
    "pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n",
      "| id|  name|courses|\n",
      "+---+------+-------+\n",
      "|  1|asfand|      2|\n",
      "|  2| saeed|      3|\n",
      "|  3|   ali|      4|\n",
      "|  4|  khan|      0|\n",
      "+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select ('id','name','course').withColumn('courses',size(col('course'))).drop(col('course')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+\n",
      "| id|  name| dep_courses|\n",
      "+---+------+------------+\n",
      "|  1|asfand|      [1, 2]|\n",
      "|  2| saeed|   [1, 3, 2]|\n",
      "|  3|   ali|[1, 2, 4, 5]|\n",
      "|  4|  khan|          []|\n",
      "+---+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Lets rename the columns\n",
    "df.select('id','name','course').withColumnRenamed('course','dep_courses').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "df.select('id','name','phone').withColumn('cou',size('phone')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+\n",
      "|student_id|student_name|students|\n",
      "+----------+------------+--------+\n",
      "|         1|      asfand|asfand_1|\n",
      "|         2|       saeed| saeed_2|\n",
      "|         3|         ali|   ali_3|\n",
      "|         4|        khan|  khan_4|\n",
      "+----------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (col('id').alias('student_id'),col('name').alias('student_name'),concat(col('name'),lit('_'),col('id')).alias('students')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+\n",
      "|student_id|student_name|students|\n",
      "+----------+------------+--------+\n",
      "|         1|      asfand|asfand_1|\n",
      "|         2|       saeed| saeed_2|\n",
      "|         3|         ali|   ali_3|\n",
      "|         4|        khan|  khan_4|\n",
      "+----------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (df['id'].alias('student_id'),df['name'].alias('student_name'),concat(df['name'],lit('_'),df['id']).alias('students')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **using toDF**\n",
    "* toDF is use to convert list of tuple to spark dataframe and rdd to spark dataframe.\n",
    "* Here we will use toDF for bulk renaming of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[('Alice', 1), ('Bob', 2), ('Carol', 3)]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2), (\"Carol\", 3)])\n",
    "print(type(rdd.collect()))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('name', 'string'), ('id', 'bigint')]\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(rdd.toDF(['name','id']).dtypes)\n",
    "print (type(rdd.toDF(['name','id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "     \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=spark.createDataFrame(pd.DataFrame(usr_Struct))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "original=['id','name','is_customer','phone','update']\n",
    "new=['user_id','user_name','user_is_customer','user_phone','user_update']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------------+----------------------------+-------------------+\n",
      "|user_id|user_name|user_is_customer|user_phone                  |user_update        |\n",
      "+-------+---------+----------------+----------------------------+-------------------+\n",
      "|1      |asfand   |true            |{+92329677783, +92342454672}|2022-04-12 04:10:00|\n",
      "|2      |saeed    |true            |{+92329622283, +92342442312}|2021-04-11 04:10:00|\n",
      "|3      |ali      |false           |{+92329644483, +92342445552}|2021-04-22 12:10:00|\n",
      "|4      |khan     |false           |{null, null}                |null               |\n",
      "+-------+---------+----------------+----------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(original).toDF('user_id','user_name','user_is_customer','user_phone','user_update').show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------------+----------------------------+-------------------+\n",
      "|user_id|user_name|user_is_customer|user_phone                  |user_update        |\n",
      "+-------+---------+----------------+----------------------------+-------------------+\n",
      "|1      |asfand   |true            |{+92329677783, +92342454672}|2022-04-12 04:10:00|\n",
      "|2      |saeed    |true            |{+92329622283, +92342442312}|2021-04-11 04:10:00|\n",
      "|3      |ali      |false           |{+92329644483, +92342445552}|2021-04-22 12:10:00|\n",
      "|4      |khan     |false           |{null, null}                |null               |\n",
      "+-------+---------+----------------+----------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(original).toDF(*new).show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sour=['id','name','is_customer','phone','update']\n",
    "target=['s_id','s_name','s_is_customer','s_phone','s_update']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------------+--------------------+-------------------+\n",
      "|s_id|s_name|s_is_customer|             s_phone|           s_update|\n",
      "+----+------+-------------+--------------------+-------------------+\n",
      "|   1|asfand|         true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|   2| saeed|         true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|   3|   ali|        false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|   4|  khan|        false|        {null, null}|               null|\n",
      "+----+------+-------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (sour).toDF(*target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('toDF' in dir(pd.DataFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', '_AXIS_LEN', '_AXIS_NAMES', '_AXIS_NUMBERS', '_AXIS_ORDERS', '_AXIS_TO_AXIS_NUMBER', '_HANDLED_TYPES', '__abs__', '__add__', '__and__', '__annotations__', '__array__', '__array_priority__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__divmod__', '__doc__', '__eq__', '__finalize__', '__floordiv__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__imod__', '__imul__', '__init__', '__init_subclass__', '__invert__', '__ior__', '__ipow__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '__xor__', '_accessors', '_accum_func', '_add_numeric_operations', '_agg_by_level', '_agg_examples_doc', '_agg_summary_and_see_also_doc', '_align_frame', '_align_series', '_append', '_arith_method', '_as_manager', '_box_col_values', '_can_fast_transpose', '_check_inplace_and_allows_duplicate_labels', '_check_inplace_setting', '_check_is_chained_assignment_possible', '_check_label_or_level_ambiguity', '_check_setitem_copy', '_clear_item_cache', '_clip_with_one_bound', '_clip_with_scalar', '_cmp_method', '_combine_frame', '_consolidate', '_consolidate_inplace', '_construct_axes_dict', '_construct_axes_from_arguments', '_construct_result', '_constructor', '_constructor_sliced', '_convert', '_count_level', '_data', '_dir_additions', '_dir_deletions', '_dispatch_frame_op', '_drop_axis', '_drop_labels_or_levels', '_ensure_valid_index', '_find_valid_index', '_from_arrays', '_from_mgr', '_get_agg_axis', '_get_axis', '_get_axis_name', '_get_axis_number', '_get_axis_resolvers', '_get_block_manager_axis', '_get_bool_data', '_get_cleaned_column_resolvers', '_get_column_array', '_get_index_resolvers', '_get_item_cache', '_get_label_or_level_values', '_get_numeric_data', '_get_value', '_getitem_bool_array', '_getitem_multilevel', '_gotitem', '_hidden_attrs', '_indexed_same', '_info_axis', '_info_axis_name', '_info_axis_number', '_info_repr', '_init_mgr', '_inplace_method', '_internal_names', '_internal_names_set', '_is_copy', '_is_homogeneous_type', '_is_label_or_level_reference', '_is_label_reference', '_is_level_reference', '_is_mixed_type', '_is_view', '_iset_item', '_iset_item_mgr', '_iset_not_inplace', '_iter_column_arrays', '_ixs', '_join_compat', '_logical_func', '_logical_method', '_maybe_cache_changed', '_maybe_update_cacher', '_metadata', '_min_count_stat_function', '_needs_reindex_multi', '_protect_consolidate', '_reduce', '_reduce_axis1', '_reindex_axes', '_reindex_columns', '_reindex_index', '_reindex_multi', '_reindex_with_indexers', '_rename', '_replace_columnwise', '_repr_data_resource_', '_repr_fits_horizontal_', '_repr_fits_vertical_', '_repr_html_', '_repr_latex_', '_reset_cache', '_reset_cacher', '_sanitize_column', '_series', '_set_axis', '_set_axis_name', '_set_axis_nocheck', '_set_is_copy', '_set_item', '_set_item_frame_value', '_set_item_mgr', '_set_value', '_setitem_array', '_setitem_frame', '_setitem_slice', '_slice', '_stat_axis', '_stat_axis_name', '_stat_axis_number', '_stat_function', '_stat_function_ddof', '_take_with_is_copy', '_to_dict_of_blocks', '_typ', '_update_inplace', '_validate_dtype', '_values', '_where', 'abs', 'add', 'add_prefix', 'add_suffix', 'agg', 'aggregate', 'align', 'all', 'any', 'append', 'apply', 'applymap', 'asfreq', 'asof', 'assign', 'astype', 'at', 'at_time', 'attrs', 'axes', 'backfill', 'between_time', 'bfill', 'bool', 'boxplot', 'clip', 'columns', 'combine', 'combine_first', 'compare', 'convert_dtypes', 'copy', 'corr', 'corrwith', 'count', 'cov', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'div', 'divide', 'dot', 'drop', 'drop_duplicates', 'droplevel', 'dropna', 'dtypes', 'duplicated', 'empty', 'eq', 'equals', 'eval', 'ewm', 'expanding', 'explode', 'ffill', 'fillna', 'filter', 'first', 'first_valid_index', 'flags', 'floordiv', 'from_dict', 'from_records', 'ge', 'get', 'groupby', 'gt', 'head', 'hist', 'iat', 'idxmax', 'idxmin', 'iloc', 'index', 'infer_objects', 'info', 'insert', 'interpolate', 'isin', 'isna', 'isnull', 'items', 'iteritems', 'iterrows', 'itertuples', 'join', 'keys', 'kurt', 'kurtosis', 'last', 'last_valid_index', 'le', 'loc', 'lookup', 'lt', 'mad', 'mask', 'max', 'mean', 'median', 'melt', 'memory_usage', 'merge', 'min', 'mod', 'mode', 'mul', 'multiply', 'ndim', 'ne', 'nlargest', 'notna', 'notnull', 'nsmallest', 'nunique', 'pad', 'pct_change', 'pipe', 'pivot', 'pivot_table', 'plot', 'pop', 'pow', 'prod', 'product', 'quantile', 'query', 'radd', 'rank', 'rdiv', 'reindex', 'reindex_like', 'rename', 'rename_axis', 'reorder_levels', 'replace', 'resample', 'reset_index', 'rfloordiv', 'rmod', 'rmul', 'rolling', 'round', 'rpow', 'rsub', 'rtruediv', 'sample', 'select_dtypes', 'sem', 'set_axis', 'set_flags', 'set_index', 'shape', 'shift', 'size', 'skew', 'slice_shift', 'sort_index', 'sort_values', 'sparse', 'squeeze', 'stack', 'std', 'style', 'sub', 'subtract', 'sum', 'swapaxes', 'swaplevel', 'tail', 'take', 'to_clipboard', 'to_csv', 'to_dict', 'to_excel', 'to_feather', 'to_gbq', 'to_hdf', 'to_html', 'to_json', 'to_latex', 'to_markdown', 'to_numpy', 'to_parquet', 'to_period', 'to_pickle', 'to_records', 'to_sql', 'to_stata', 'to_string', 'to_timestamp', 'to_xarray', 'to_xml', 'transform', 'transpose', 'truediv', 'truncate', 'tshift', 'tz_convert', 'tz_localize', 'unstack', 'update', 'value_counts', 'values', 'var', 'where', 'xs']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pd.DataFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.DataFrame.toDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(sour).toDf(*target).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Manipulating Columns in Spark Data Frames**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25d0b264b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('orders.csv',header=True)\n",
    "# ,sc?hema='Row_ID int,Order_ID int,Order_Date string,Ship_Date string,Ship_Mode string,Customer_ID int,Customer_Name string,Segment string,Location string,State string,Postal_Code int,Region string,Product_ID string,Category string,`Sub-Category` string,Product_Name string,Sales  decimal(18,4),Quantity int,Discount  decimal(18,4),Profit decimal(18,4)',header=False).\\\n",
    "    # show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------+--------------+-----------+------------------+-----------+--------------------+--------------+-----------+-------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|Row ID|      Order ID|Order Date| Ship Date|     Ship Mode|Customer ID|     Customer Name|    Segment|            Location|         State|Postal Code| Region|     Product ID|       Category|Sub-Category|        Product Name|   Sales|Quantity|Discount|  Profit|\n",
      "+------+--------------+----------+----------+--------------+-----------+------------------+-----------+--------------------+--------------+-----------+-------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|     1|CA-2016-152156|15/04/2017|1101102016|  Second Class|   CG-12520|       Claire Gute|   Consumer|United States,Hen...|      Kentucky|      42420|  South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|  261.96|       2|       0| 41.9136|\n",
      "|     2|CA-2016-152156|15/04/2018|1101102016|  Second Class|   CG-12520|       Claire Gute|   Consumer|United States,Hen...|      Kentucky|      42420|  South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|  731.94|       3|       0| 219.582|\n",
      "|     3|CA-2016-138688|15/04/2019| 601602016|  Second Class|   DV-13045|   Darrin Van Huff|  Corporate|United States,Los...|    California|      90036|   West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|   14.62|       2|       0|  6.8714|\n",
      "|     4|US-2015-108966|15/04/2020|1001802015|Standard Class|   SO-20335|    Sean O'Donnell|   Consumer|United States,For...|       Florida|      33311|  South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|957.5775|       5|    0.45|-383.031|\n",
      "|     5|US-2015-108966|15/04/2021|1001802015|Standard Class|   SO-20335|    Sean O'Donnell|   Consumer|United States,For...|       Florida|      33311|  South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|  22.368|       2|     0.2|  2.5164|\n",
      "|     6|CA-2014-115812|15/04/2022| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|FUR-FU-10001487|      Furniture| Furnishings|Eldon Expressions...|   48.86|       7|       0| 14.1694|\n",
      "|     7|CA-2014-115812|15/04/2023| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|OFF-AR-10002833|Office Supplies|         Art|          Newell 322|    7.28|       4|       0|  1.9656|\n",
      "|     8|CA-2014-115812|15/04/2024| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|TEC-PH-10002275|     Technology|      Phones|Mitel 5320 IP Pho...| 907.152|       6|     0.2| 90.7152|\n",
      "|     9|CA-2014-115812|15/04/2025| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|OFF-BI-10003910|Office Supplies|     Binders|DXL Angle-View Bi...|  18.504|       3|     0.2|  5.7825|\n",
      "|    10|CA-2014-115812|15/04/2026| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|OFF-AP-10002892|Office Supplies|  Appliances|Belkin F5C206VTEL...|   114.9|       5|       0|   34.47|\n",
      "|    11|CA-2014-115812|15/04/2027| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|FUR-TA-10001539|      Furniture|      Tables|Chromcraft Rectan...|1706.184|       9|     0.2| 85.3092|\n",
      "|    12|CA-2014-115812|15/04/2028| 601402014|Standard Class|   BH-11710|   Brosina Hoffman|   Consumer|United States,Los...|    California|      90032|   West|TEC-PH-10002033|     Technology|      Phones|Konftel 250 Confe...| 911.424|       4|     0.2| 68.3568|\n",
      "|    13|CA-2017-114412|15/04/2017| 402002017|Standard Class|   AA-10480|      Andrew Allen|   Consumer|United States,Con...|North Carolina|      28027|  South|OFF-PA-10002365|Office Supplies|       Paper|          Xerox 1967|  15.552|       3|     0.2|  5.4432|\n",
      "|    14|CA-2016-161389|15/04/2017|1201002016|Standard Class|   IM-15070|      Irene Maddox|   Consumer|United States,Sea...|    Washington|      98103|   West|OFF-BI-10003656|Office Supplies|     Binders|Fellowes PB200 Pl...| 407.976|       3|     0.2|132.5922|\n",
      "|    15|US-2015-118983|22/11/2015|1102602015|Standard Class|   HP-14815|     Harold Pawlan|Home Office|United States,For...|         Texas|      76106|Central|OFF-AP-10002311|Office Supplies|  Appliances|Holmes Replacemen...|   68.81|       5|     0.8|-123.858|\n",
      "|    16|US-2015-118983|22/11/2015|1102602015|Standard Class|   HP-14815|     Harold Pawlan|Home Office|United States,For...|         Texas|      76106|Central|OFF-BI-10000756|Office Supplies|     Binders|Storex DuraTech R...|   2.544|       3|     0.8|  -3.816|\n",
      "|    17|CA-2014-105893|15/04/2017|1101802014|Standard Class|   PK-19075|         Pete Kriz|   Consumer|United States,Mad...|     Wisconsin|      53711|Central|OFF-ST-10004186|Office Supplies|     Storage|\"Stur-D-Stor Shel...|  665.88|       6|       0| 13.3176|\n",
      "|    18|CA-2014-167164|13/05/2014| 501502014|  Second Class|   AG-10270|   Alejandro Grove|   Consumer|United States,Wes...|          Utah|      84084|   West|OFF-ST-10000107|Office Supplies|     Storage|Fellowes Super St...|    55.5|       2|       0|    9.99|\n",
      "|    19|CA-2014-143336|27/08/2014|  90102014|  Second Class|   ZD-21925|Zuschuss Donatelli|   Consumer|United States,San...|    California|      94109|   West|OFF-AR-10003056|Office Supplies|         Art|          Newell 341|    8.56|       2|       0|  2.4824|\n",
      "|    20|CA-2014-143336|27/08/2014|  90102014|  Second Class|   ZD-21925|Zuschuss Donatelli|   Consumer|United States,San...|    California|      94109|   West|TEC-PH-10001949|     Technology|      Phones|Cisco SPA 501G IP...|  213.48|       3|     0.2|  16.011|\n",
      "+------+--------------+----------+----------+--------------+-----------+------------------+-----------+--------------------+--------------+-----------+-------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: string (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Ship Mode: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Customer Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Postal Code: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Product Name: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Discount: string (nullable = true)\n",
      " |-- Profit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.select('Row ID','Order ID','Ship Date','Order Date').withColumn('order_month',to_date(col('Order Date'),'dd/MM/yyyy').alias('order_month')).dropna(subset=['order_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------+-----------+----------+\n",
      "|Row ID|      Order ID| Ship Date|Order Date|order_month|month_data|\n",
      "+------+--------------+----------+----------+-----------+----------+\n",
      "|     1|CA-2016-152156|1101102016|15/04/2017| 2017-04-15|    201704|\n",
      "|     2|CA-2016-152156|1101102016|15/04/2018| 2018-04-15|    201804|\n",
      "|     3|CA-2016-138688| 601602016|15/04/2019| 2019-04-15|    201904|\n",
      "|     4|US-2015-108966|1001802015|15/04/2020| 2020-04-15|    202004|\n",
      "|     5|US-2015-108966|1001802015|15/04/2021| 2021-04-15|    202104|\n",
      "|     6|CA-2014-115812| 601402014|15/04/2022| 2022-04-15|    202204|\n",
      "|     7|CA-2014-115812| 601402014|15/04/2023| 2023-04-15|    202304|\n",
      "|     8|CA-2014-115812| 601402014|15/04/2024| 2024-04-15|    202404|\n",
      "|     9|CA-2014-115812| 601402014|15/04/2025| 2025-04-15|    202504|\n",
      "|    10|CA-2014-115812| 601402014|15/04/2026| 2026-04-15|    202604|\n",
      "|    11|CA-2014-115812| 601402014|15/04/2027| 2027-04-15|    202704|\n",
      "|    12|CA-2014-115812| 601402014|15/04/2028| 2028-04-15|    202804|\n",
      "|    13|CA-2017-114412| 402002017|15/04/2017| 2017-04-15|    201704|\n",
      "|    14|CA-2016-161389|1201002016|15/04/2017| 2017-04-15|    201704|\n",
      "|    15|US-2015-118983|1102602015|22/11/2015| 2015-11-22|    201511|\n",
      "|    16|US-2015-118983|1102602015|22/11/2015| 2015-11-22|    201511|\n",
      "|    17|CA-2014-105893|1101802014|15/04/2017| 2017-04-15|    201704|\n",
      "|    18|CA-2014-167164| 501502014|13/05/2014| 2014-05-13|    201405|\n",
      "|    19|CA-2014-143336|  90102014|27/08/2014| 2014-08-27|    201408|\n",
      "|    20|CA-2014-143336|  90102014|27/08/2014| 2014-08-27|    201408|\n",
      "+------+--------------+----------+----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df1.select('Row ID','Order ID','Ship Date','Order Date','order_month').withColumn('month_data',date_format(col('order_month'),'yyyyMM'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|order_month|count|\n",
      "+-----------+-----+\n",
      "| 2014-05-21|   10|\n",
      "| 2014-05-30|    8|\n",
      "| 2014-05-26|    8|\n",
      "| 2014-05-23|    7|\n",
      "| 2014-05-20|    7|\n",
      "| 2014-05-13|    6|\n",
      "| 2014-05-27|    5|\n",
      "| 2014-05-18|    4|\n",
      "| 2014-05-25|    3|\n",
      "| 2014-05-28|    3|\n",
      "| 2014-05-31|    2|\n",
      "| 2014-05-16|    2|\n",
      "| 2014-05-22|    2|\n",
      "| 2014-05-19|    2|\n",
      "| 2014-05-14|    1|\n",
      "| 2014-05-24|    1|\n",
      "| 2014-05-17|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(col('month_data')=='201405').groupBy(col('order_month')).count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **lower Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------------------------+--------------+-----------+-----------------------------+-----------------------------+\n",
      "|Row ID|Order ID      |Location                     |State         |Postal Code|loctions                     |Locations                    |\n",
      "+------+--------------+-----------------------------+--------------+-----------+-----------------------------+-----------------------------+\n",
      "|1     |CA-2016-152156|United States,Henderson      |Kentucky      |42420      |united states,henderson      |United States,henderson      |\n",
      "|2     |CA-2016-152156|United States,Henderson      |Kentucky      |42420      |united states,henderson      |United States,henderson      |\n",
      "|3     |CA-2016-138688|United States,Los Angeles    |California    |90036      |united states,los angeles    |United States,los Angeles    |\n",
      "|4     |US-2015-108966|United States,Fort Lauderdale|Florida       |33311      |united states,fort lauderdale|United States,fort Lauderdale|\n",
      "|5     |US-2015-108966|United States,Fort Lauderdale|Florida       |33311      |united states,fort lauderdale|United States,fort Lauderdale|\n",
      "|6     |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|7     |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|8     |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|9     |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|10    |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|11    |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|12    |CA-2014-115812|United States,Los Angeles    |California    |90032      |united states,los angeles    |United States,los Angeles    |\n",
      "|13    |CA-2017-114412|United States,Concord        |North Carolina|28027      |united states,concord        |United States,concord        |\n",
      "|14    |CA-2016-161389|United States,Seattle        |Washington    |98103      |united states,seattle        |United States,seattle        |\n",
      "|15    |US-2015-118983|United States,Fort Worth     |Texas         |76106      |united states,fort worth     |United States,fort Worth     |\n",
      "|16    |US-2015-118983|United States,Fort Worth     |Texas         |76106      |united states,fort worth     |United States,fort Worth     |\n",
      "|17    |CA-2014-105893|United States,Madison        |Wisconsin     |53711      |united states,madison        |United States,madison        |\n",
      "|18    |CA-2014-167164|United States,West Jordan    |Utah          |84084      |united states,west jordan    |United States,west Jordan    |\n",
      "|19    |CA-2014-143336|United States,San Francisco  |California    |94109      |united states,san francisco  |United States,san Francisco  |\n",
      "|20    |CA-2014-143336|United States,San Francisco  |California    |94109      |united states,san francisco  |United States,san Francisco  |\n",
      "+------+--------------+-----------------------------+--------------+-----------+-----------------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Row ID','Order ID','Location','State','Postal Code').withColumn('loctions',lower(col('Location'))).withColumn('Locations',initcap(col('loctions'))).show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concat_ws Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------------------------+--------------+-----------------------------+--------------+-----------+--------------------------------------+\n",
      "|Row ID|Order ID      |Location                     |State         |Location                     |State         |Postal Code|combine                               |\n",
      "+------+--------------+-----------------------------+--------------+-----------------------------+--------------+-----------+--------------------------------------+\n",
      "|1     |CA-2016-152156|United States,Henderson      |Kentucky      |United States,Henderson      |Kentucky      |42420      |United States,Henderson_Kentucky      |\n",
      "|2     |CA-2016-152156|United States,Henderson      |Kentucky      |United States,Henderson      |Kentucky      |42420      |United States,Henderson_Kentucky      |\n",
      "|3     |CA-2016-138688|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90036      |United States,Los Angeles_California  |\n",
      "|4     |US-2015-108966|United States,Fort Lauderdale|Florida       |United States,Fort Lauderdale|Florida       |33311      |United States,Fort Lauderdale_Florida |\n",
      "|5     |US-2015-108966|United States,Fort Lauderdale|Florida       |United States,Fort Lauderdale|Florida       |33311      |United States,Fort Lauderdale_Florida |\n",
      "|6     |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|7     |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|8     |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|9     |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|10    |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|11    |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|12    |CA-2014-115812|United States,Los Angeles    |California    |United States,Los Angeles    |California    |90032      |United States,Los Angeles_California  |\n",
      "|13    |CA-2017-114412|United States,Concord        |North Carolina|United States,Concord        |North Carolina|28027      |United States,Concord_North Carolina  |\n",
      "|14    |CA-2016-161389|United States,Seattle        |Washington    |United States,Seattle        |Washington    |98103      |United States,Seattle_Washington      |\n",
      "|15    |US-2015-118983|United States,Fort Worth     |Texas         |United States,Fort Worth     |Texas         |76106      |United States,Fort Worth_Texas        |\n",
      "|16    |US-2015-118983|United States,Fort Worth     |Texas         |United States,Fort Worth     |Texas         |76106      |United States,Fort Worth_Texas        |\n",
      "|17    |CA-2014-105893|United States,Madison        |Wisconsin     |United States,Madison        |Wisconsin     |53711      |United States,Madison_Wisconsin       |\n",
      "|18    |CA-2014-167164|United States,West Jordan    |Utah          |United States,West Jordan    |Utah          |84084      |United States,West Jordan_Utah        |\n",
      "|19    |CA-2014-143336|United States,San Francisco  |California    |United States,San Francisco  |California    |94109      |United States,San Francisco_California|\n",
      "|20    |CA-2014-143336|United States,San Francisco  |California    |United States,San Francisco  |California    |94109      |United States,San Francisco_California|\n",
      "+------+--------------+-----------------------------+--------------+-----------------------------+--------------+-----------+--------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select ('Row ID','Order ID','Location','State','Location','State','Postal Code').withColumn('combine',concat_ws('_','Location','State')).show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "| id|  name|is_customer|               phone|             update|   dates|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|20220412|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|20210411|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|20210422|\n",
      "|  4|  khan|      false|        {null, null}|               null|    null|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "df.withColumn('dates',date_format('update','yyyyMMdd')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    year|count|\n",
      "+--------+-----+\n",
      "|20220412|    1|\n",
      "|20210411|    1|\n",
      "|20210422|    1|\n",
      "|    null|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Group by\n",
    "df.groupBy(date_format('update','yyyyMMdd').alias('year')).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(date_format('update','yyyyMMdd').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col('name').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df['name'].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|             update|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|\n",
      "|  4|  khan|      false|        {null, null}|               null|\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+-----+\n",
      "| id|  name|is_customer|               phone|             update|names|\n",
      "+---+------+-----------+--------------------+-------------------+-----+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00| null|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00| null|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00| null|\n",
      "|  4|  khan|      false|        {null, null}|               null| null|\n",
      "+---+------+-----------+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('names','id'+lit(5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+-----+\n",
      "| id|  name|is_customer|               phone|             update|names|\n",
      "+---+------+-----------+--------------------+-------------------+-----+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|    6|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|    7|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|    8|\n",
      "|  4|  khan|      false|        {null, null}|               null|    9|\n",
      "+---+------+-----------+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('names',col('id')+lit(5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "| id|  name|is_customer|               phone|             update|  new_id|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|asfand_1|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00| saeed_2|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|   ali_3|\n",
      "|  4|  khan|      false|        {null, null}|               null|  khan_4|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_id',concat(df.name,lit('_'),df.id)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "| id|  name|is_customer|               phone|             update| new_ids|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|asfand_1|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00| saeed_2|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|   ali_3|\n",
      "|  4|  khan|      false|        {null, null}|               null|  khan_4|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_ids',concat(\"name\",lit('_'),\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "| id|  name|is_customer|               phone|             update| new_ids|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|asfand_1|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00| saeed_2|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|   ali_3|\n",
      "|  4|  khan|      false|        {null, null}|               null|  khan_4|\n",
      "+---+------+-----------+--------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_ids',concat_ws(\"_\",\"name\",\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "| id|  name|is_customer|               phone|             update|   new_phone|\n",
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|+92329677783|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|+92329622283|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|+92329644483|\n",
      "|  4|  khan|      false|        {null, null}|               null|        null|\n",
      "+---+------+-----------+--------------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take the first element from struct type column\n",
    "df.withColumn('new_phone',df.phone.first).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Substrings**\n",
    "* If we are working on fixed length record and we want to extract information from that we can use substring.\n",
    "* Columns like product name ,ID , Status etc are best examples of using substring on top of substring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23529a74b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task is to get the first country code of phone numbers and customer number should be in different column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "     \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=pd.DataFrame(usr_Struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+----------------------------+-------------------+\n",
      "|id |name  |is_customer|phone                       |update             |\n",
      "+---+------+-----------+----------------------------+-------------------+\n",
      "|1  |asfand|true       |{+92329677783, +92342454672}|2022-04-12 04:10:00|\n",
      "|2  |saeed |true       |{+92329622283, +92342442312}|2021-04-11 04:10:00|\n",
      "|3  |ali   |false      |{+92329644483, +92342445552}|2021-04-22 12:10:00|\n",
      "|4  |khan  |false      |{null, null}                |null               |\n",
      "+---+------+-----------+----------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Taking position in +ve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+--------------+----+\n",
      "| id|  name|is_customer|               phone|             update|phone_customer|code|\n",
      "+---+------+-----------+--------------------+-------------------+--------------+----+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|     329677783| +92|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|     329622283| +92|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|     329644483| +92|\n",
      "|  4|  khan|      false|        {null, null}|               null|          null|null|\n",
      "+---+------+-----------+--------------------+-------------------+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').withColumn('phone_customer',substring(col('phone')['first'],4,10)).withColumn('code',substring(col('phone.first'),1,3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+--------------+----+\n",
      "| id|  name|is_customer|               phone|             update|phone_customer|code|\n",
      "+---+------+-----------+--------------------+-------------------+--------------+----+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|     329677783| +92|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|     329622283| +92|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|     329644483| +92|\n",
      "|  4|  khan|      false|        {null, null}|               null|          null|null|\n",
      "+---+------+-----------+--------------------+-------------------+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').withColumn('phone_customer',substring(col('phone')['first'],-9,10)).withColumn('code',substring(col('phone.first'),-12,3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+-------+\n",
      "| id|  name|is_customer|               phone|             update|country|\n",
      "+---+------+-----------+--------------------+-------------------+-------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|    +92|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|    +92|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|    +92|\n",
      "|  4|  khan|      false|        {null, null}|               null|   null|\n",
      "+---+------+-----------+--------------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('country',substring(df.phone.first,1,3)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extracting Strings Using Split from Spark Data Frame**\n",
    "* If we are working with variable length column then its best way to first split the data by delimiter and then apply operations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example 1::**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    {'id':1,\n",
    "     'name':\"asfand\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329677783\", Second=\"+92342454672\"),\n",
    "     'update': datetime.datetime(2022,4,12,4,10,0)\n",
    "    },\n",
    "    {'id':2,\n",
    "     'name':\"saeed\",\n",
    "     'is_customer':True,\n",
    "     'phone': Row(first=\"+92329622283\", Second=\"+92342442312\"),\n",
    "     'update': datetime.datetime(2021,4,11,4,10,0)},\n",
    "    {'id':3,\n",
    "     'name':\"ali\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=\"+92329644483\", Second=\"+92342445552\"), ## Predefined structure as a Row\n",
    "     'update': datetime.datetime(2021,4,22,12,10,0)},\n",
    "    {'id':4,\n",
    "     'name':\"khan\",\n",
    "     'is_customer':False,\n",
    "     'phone': Row(first=None,Second=None),#['+92329633456','+9234247654'],\n",
    "     'update': None #datetime.datetime(2022,5,1,10,10,0)}\n",
    "     \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+-----------+\n",
      "| id|  name|is_customer|               phone|             update|  new_phone|\n",
      "+---+------+-----------+--------------------+-------------------+-----------+\n",
      "|  1|asfand|       true|{+92329677783, +9...|2022-04-12 04:10:00|00232967778|\n",
      "|  2| saeed|       true|{+92329622283, +9...|2021-04-11 04:10:00|00232962228|\n",
      "|  3|   ali|      false|{+92329644483, +9...|2021-04-22 12:10:00|00232964448|\n",
      "|  4|  khan|      false|        {null, null}|               null|       null|\n",
      "+---+------+-----------+--------------------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_phone',lpad(substring(df.phone.first,3,9),11,'0')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example 2::**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     \"+92329677783,+92342454672\",\n",
    "     datetime.datetime(2022,4,12,4,10,0)\n",
    "    ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     \"+92329622283,+92342442312\",\n",
    "     datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     \"+92329644483,+92342445552\", ## Predefined structure as a Row\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "     \",\",\n",
    "     None)#['+92329633456','+9234247654'],\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-------------------------+-------------------+\n",
      "|id |name  |is_customer|phone                    |dates              |\n",
      "+---+------+-----------+-------------------------+-------------------+\n",
      "|1  |asfand|true       |+92329677783,+92342454672|2022-04-12 04:10:00|\n",
      "|2  |saeed |true       |+92329622283,+92342442312|2021-04-11 04:10:00|\n",
      "|3  |ali   |false      |+92329644483,+92342445552|2021-04-22 12:10:00|\n",
      "|4  |khan  |false      |,                        |null               |\n",
      "+---+------+-----------+-------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sec=\"\"\"\n",
    "id int,\n",
    "name string,\n",
    "is_customer boolean,\n",
    "phone string,\n",
    "dates timestamp\n",
    "\n",
    "\"\"\"\n",
    "df=spark.createDataFrame(usr_Struct,schema=sec)\n",
    "df.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- dates: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(usr_Struct,schema=sec).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-------------------------+-------------------+------------+\n",
      "|id |name  |is_customer|phone                    |dates              |phones      |\n",
      "+---+------+-----------+-------------------------+-------------------+------------+\n",
      "|1  |asfand|true       |+92329677783,+92342454672|2022-04-12 04:10:00|+92329677783|\n",
      "|1  |asfand|true       |+92329677783,+92342454672|2022-04-12 04:10:00|+92342454672|\n",
      "|2  |saeed |true       |+92329622283,+92342442312|2021-04-11 04:10:00|+92329622283|\n",
      "|2  |saeed |true       |+92329622283,+92342442312|2021-04-11 04:10:00|+92342442312|\n",
      "|3  |ali   |false      |+92329644483,+92342445552|2021-04-22 12:10:00|+92329644483|\n",
      "|3  |ali   |false      |+92329644483,+92342445552|2021-04-22 12:10:00|+92342445552|\n",
      "|4  |khan  |false      |,                        |null               |            |\n",
      "|4  |khan  |false      |,                        |null               |            |\n",
      "+---+------+-----------+-------------------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (\"*\").withColumn('phones',explode(split(col('phone'),','))).show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now lets count the number of phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|id |count|\n",
      "+---+-----+\n",
      "|1  |2    |\n",
      "|2  |2    |\n",
      "|3  |2    |\n",
      "|4  |2    |\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\").withColumn('phone',explode(split(col('phone'),','))).groupBy(col('id')).count().show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Padding Characters around Strings in Spark DataFrame Columns**\n",
    "* When we want to make fix length columns we use padding.\n",
    "* It is good for main frame systems.\n",
    "\n",
    "### Task ::\n",
    "* Id should be padded with zeros if the length is not 3.\n",
    "* The name should be right padded ie '-' on right side.\n",
    "* The phone num should me right paddded if its less then 9 with '-'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     \"+92329677783,+92342454672\",\n",
    "     datetime.datetime(2022,4,12,4,10,0)\n",
    "    ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     \"+92329622283,+92342442312\",\n",
    "     datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     \"+92329644483,+92342445552\", ## Predefined structure as a Row\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "     \",\",\n",
    "     None)#['+92329633456','+9234247654'],\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------------------------+-------------------+\n",
      "|_1 |_2    |_3   |_4                       |_5                 |\n",
      "+---+------+-----+-------------------------+-------------------+\n",
      "|1  |asfand|true |+92329677783,+92342454672|2022-04-12 04:10:00|\n",
      "|2  |saeed |true |+92329622283,+92342442312|2021-04-11 04:10:00|\n",
      "|3  |ali   |false|+92329644483,+92342445552|2021-04-22 12:10:00|\n",
      "|4  |khan  |false|,                        |null               |\n",
      "+---+------+-----+-------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(usr_Struct).show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(usr_Struct).toDF(\"id\",\"name\",\"is_customer\",\"phone\",\"dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+-------------+\n",
      "|id |name  |phone       |phones       |\n",
      "+---+------+------------+-------------+\n",
      "|001|asfand|+92329677783|++92329677783|\n",
      "|001|asfand|+92342454672|++92342454672|\n",
      "|002|saeed-|+92329622283|++92329622283|\n",
      "|002|saeed-|+92342442312|++92342442312|\n",
      "|003|ali---|+92329644483|++92329644483|\n",
      "|003|ali---|+92342445552|++92342445552|\n",
      "|004|khan--|            |+++++++++++++|\n",
      "|004|khan--|            |+++++++++++++|\n",
      "+---+------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (lpad(col('id'),3,'0').alias('id'),rpad(col('name'),6,'-').alias(\"name\"),explode(split(col('phone'),',')).alias('phone')).withColumn('phones',lpad('phone',13,'+')).show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Trimming Characters from strings in Spark Data Frame Columns**\n",
    "* We use rtrim ,ltrim and trim to remove the spaces on right ,left and on both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function rtrim in module pyspark.sql.functions:\n",
      "\n",
      "rtrim(col)\n",
      "    Trim the spaces from right end for the specified string value.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(rtrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ltrim in module pyspark.sql.functions:\n",
      "\n",
      "ltrim(col)\n",
      "    Trim the spaces from left end for the specified string value.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ltrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function trim in module pyspark.sql.functions:\n",
      "\n",
      "trim(col)\n",
      "    Trim the spaces from both ends for the specified string column.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Function: trim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.StringTrim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Usage: \\n    trim(str) - Removes the leading and trailing space characters from `str`.\\n\\n    trim(BOTH FROM str) - Removes the leading and trailing space characters from `str`.\\n\\n    trim(LEADING FROM str) - Removes the leading space characters from `str`.\\n\\n    trim(TRAILING FROM str) - Removes the trailing space characters from `str`.\\n\\n    trim(trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\\n\\n    trim(BOTH trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\\n\\n    trim(LEADING trimStr FROM str) - Remove the leading `trimStr` characters from `str`.\\n\\n    trim(TRAILING trimStr FROM str) - Remove the trailing `trimStr` characters from `str`.\\n  |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE FUNCTION trim').show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To remove special character on right side we can use trim(LEADING '.' From col('name')).\n",
    "* To remove special character on left side we can use trim(TRAILING '' From col('name')).\n",
    "* To remove special character on both side we can use trim(BOTH '' From col('name'))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_Struct=[\n",
    "    (1,\n",
    "     \"asfand\",\n",
    "     True,\n",
    "     \"+92329677783,+92342454672\",\n",
    "     datetime.datetime(2022,4,12,4,10,0)\n",
    "    ),\n",
    "    (2,\n",
    "     \"saeed\",\n",
    "     True,\n",
    "     \"+92329622283,+92342442312\",\n",
    "     datetime.datetime(2021,4,11,4,10,0)),\n",
    "    (3,\n",
    "     \"ali\",\n",
    "     False,\n",
    "     \"+92329644483,+92342445552\", ## Predefined structure as a Row\n",
    "     datetime.datetime(2021,4,22,12,10,0)),\n",
    "    (4,\n",
    "     \"khan\",\n",
    "     False,\n",
    "     \",\",\n",
    "     None)#['+92329633456','+9234247654'],\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------------+-------------------+\n",
      "| id|  name|is_customer|               phone|              dates|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "|  1|asfand|       true|+92329677783,+923...|2022-04-12 04:10:00|\n",
      "|  2| saeed|       true|+92329622283,+923...|2021-04-11 04:10:00|\n",
      "|  3|   ali|      false|+92329644483,+923...|2021-04-22 12:10:00|\n",
      "|  4|  khan|      false|                   ,|               null|\n",
      "+---+------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(usr_Struct).toDF(\"id\",\"name\",\"is_customer\",\"phone\",\"dates\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+-------------+\n",
      "| id|  name|       phone|       phones|\n",
      "+---+------+------------+-------------+\n",
      "|001|asfand|+92329677783|++92329677783|\n",
      "|001|asfand|+92342454672|++92342454672|\n",
      "|002|saeed-|+92329622283|++92329622283|\n",
      "|002|saeed-|+92342442312|++92342442312|\n",
      "|003|ali---|+92329644483|++92329644483|\n",
      "|003|ali---|+92342445552|++92342445552|\n",
      "|004|khan--|            |+++++++++++++|\n",
      "|004|khan--|            |+++++++++++++|\n",
      "+---+------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trim_df=df.select (lpad(col('id'),3,'0').alias('id'),rpad(col('name'),6,'-').alias(\"name\"),explode(split(col('phone'),',')).alias('phone')).withColumn('phones',lpad('phone',13,'+'))\n",
    "trim_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-----------+\n",
      "| id|  name|      phone|     phones|\n",
      "+---+------+-----------+-----------+\n",
      "|001|asfand|92329677783|92329677783|\n",
      "|001|asfand|92342454672|92342454672|\n",
      "|002| saeed|92329622283|92329622283|\n",
      "|002| saeed|92342442312|92342442312|\n",
      "|003|   ali|92329644483|92329644483|\n",
      "|003|   ali|92342445552|92342445552|\n",
      "|004|  khan|           |           |\n",
      "|004|  khan|           |           |\n",
      "+---+------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trim_df.withColumn('name',expr(\"trim(TRAILING '-' FROM trim(name))\")).\\\n",
    "    withColumn('phone',expr(\"trim(LEADING '+' FROM trim(phone))\")).\\\n",
    "        withColumn('phones',expr(\"trim(BOTH '+' FROM phones)\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dealing with String type data and Converting to time and date datatype**\n",
    "* If we get any type of time that is in form of string we can convert it to date and timestamp by using to_date() and to_timestamp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame([('X',),('Y',)]).toDF('sa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n",
      "|sa |dates     |times                  |\n",
      "+---+----------+-----------------------+\n",
      "|X  |2022-01-01|2022-01-01 01:01:01.111|\n",
      "|Y  |2022-01-01|2022-01-01 01:01:01.111|\n",
      "+---+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "df.withColumn('dates',to_date(lit('20220101010101111'),'yyyyMMddHHmmssSSS')).withColumn('times',to_timestamp(lit('20220101010101111'),'yyyyMMddHHmmssSSS')).show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sa: string (nullable = true)\n",
      " |-- dates: date (nullable = true)\n",
      " |-- times: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('dates',to_date(lit('20220101010101111'),'yyyyMMddHHmmssSSS')).withColumn('times',to_timestamp(lit('20220101010101111'),'yyyyMMddHHmmssSSS')).printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Date and Time Arithmetic using Spark Data Frames**\n",
    "* Now as we have gone through how to make a date column and timestamp column out of a string of date we will do arithmatic operation on top of the data.\n",
    "* date_add,date_sub,date_diff,month_between,add_month,sub_month,next_month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x29c3c8a4b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|dates     |times                  |\n",
      "+----------+-----------------------+\n",
      "|2014-01-05|2014-01-05 06:20:01 000|\n",
      "|2013-02-05|2016-01-05 12:32:01 000|\n",
      "|2015-05-08|2018-07-04 11:00:01 000|\n",
      "|2018-03-07|2016-04-07 10:22:01 000|\n",
      "|2019-09-05|2011-05-05 09:32:01 000|\n",
      "+----------+-----------------------+\n",
      "\n",
      "root\n",
      " |-- dates: string (nullable = true)\n",
      " |-- times: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dates=[\n",
    "    ('2014-01-05','2014-01-05 06:20:01 000'),\n",
    "    ('2013-02-05','2016-01-05 12:32:01 000'),\n",
    "    ('2015-05-08','2018-07-04 11:00:01 000'),\n",
    "    ('2018-03-07','2016-04-07 10:22:01 000'),\n",
    "    ('2019-09-05','2011-05-05 09:32:01 000')   \n",
    "]\n",
    "us=\"\"\"dates String,times String\"\"\"\n",
    "df=spark.createDataFrame(dates,us)\n",
    "df.show(truncate=0)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+----------+----------+----------+---------+--------------+\n",
      "|dates     |times                  |add_dates |add_times |sub_date  |sub_time  |add_month |diff_date|months_between|\n",
      "+----------+-----------------------+----------+----------+----------+----------+----------+---------+--------------+\n",
      "|2014-01-05|2014-01-05 06:20:01 000|2014-01-08|2014-01-08|2014-01-02|2014-01-02|2014-04-05|3521     |115.71        |\n",
      "|2013-02-05|2016-01-05 12:32:01 000|2013-02-08|2016-01-08|2013-02-02|2016-01-02|2013-05-05|3855     |126.71        |\n",
      "|2015-05-08|2018-07-04 11:00:01 000|2015-05-11|2018-07-07|2015-05-05|2018-07-01|2015-08-08|3033     |99.61         |\n",
      "|2018-03-07|2016-04-07 10:22:01 000|2018-03-10|2016-04-10|2018-03-04|2016-04-04|2018-06-07|1999     |65.65         |\n",
      "|2019-09-05|2011-05-05 09:32:01 000|2019-09-08|2011-05-08|2019-09-02|2011-05-02|2019-12-05|1452     |47.71         |\n",
      "+----------+-----------------------+----------+----------+----------+----------+----------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('add_dates',date_add(col('dates'),3)).\\\n",
    "    withColumn('add_times',date_add(col('times'),3)).\\\n",
    "        withColumn('sub_date',date_sub(col('dates'),3)).\\\n",
    "            withColumn('sub_date',date_sub(col('dates'),3)).\\\n",
    "                withColumn('sub_time',date_sub(col('times'),3)).\\\n",
    "                    withColumn('add_month',add_months(col('dates'),3)).\\\n",
    "                        withColumn('diff_date',datediff(current_date(),col('dates'))).\\\n",
    "                            withColumn('months_between',round(months_between(current_date(),col('dates')),2)).\\\n",
    "        show(truncate=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date_Functions\n",
    "* *date_format()*: Takes date/timestamp/string and convert to desired pattern.\n",
    "* *to_date*: Takes string or col and then we tell him what value show what and then convert it into yyyy-MM-dd\n",
    "* *to_timestamp*: Takes string or col and then we tell him what value show what and then convert it into yyyy-MM-dd hh:mm:ss SSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_timestamp in module pyspark.sql.functions:\n",
      "\n",
      "to_timestamp(col, format=None)\n",
      "    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\n",
      "    using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "    By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n",
      "    is omitted. Equivalent to ``col.cast(\"timestamp\")``.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 2.2.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n",
      "    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n",
      "    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(to_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x217583d5f30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|     DATES|              TIMES|\n",
      "+----------+-------------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|\n",
      "|2018-02-12|2018-02-12 09:10:23|\n",
      "|2019-03-11|2019-03-11 11:03:22|\n",
      "|2022-04-15|2022-04-15 10:11:21|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_s=[(\"2021-01-14\",\"2021-01-14 10:00:21\"),\n",
    "(\"2018-02-12\",\"2018-02-12 09:10:23\"),\n",
    "(\"2019-03-11\",\"2019-03-11 11:03:22\"),\n",
    "(\"2022-04-15\",\"2022-04-15 10:11:21\")]\n",
    "data=spark.createDataFrame(date_s,schema=\"DATES STRING,TIMES STRING\")\n",
    "data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Time operation::\n",
    "* *date_add*: It will add number of days to the date and then give the result.\n",
    "* *date_sub*: It will subtract number of days to the date and gie result.\n",
    "* *date_diff*: It will subtract the dates and give you int value.\n",
    "* *add_month*: It will add number of months and show the date not timestamp.\n",
    "* *months_between*:It will show the number of months between the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----------+----------+-------------------+----------+-----------+\n",
      "|     DATES|              TIMES|  date_add|      date|             timess|  date_sub|dates_to_go|\n",
      "+----------+-------------------+----------+----------+-------------------+----------+-----------+\n",
      "|2021-01-14|2021-01-14 10:00:21|2021-01-21|2021-01-14|2021-01-14 00:00:00|2021-01-06| 2021-05-14|\n",
      "|2018-02-12|2018-02-12 09:10:23|2018-02-19|2018-02-12|2018-02-12 00:00:00|2018-02-04| 2018-06-12|\n",
      "|2019-03-11|2019-03-11 11:03:22|2019-03-18|2019-03-11|2019-03-11 00:00:00|2019-03-03| 2019-07-11|\n",
      "|2022-04-15|2022-04-15 10:11:21|2022-04-22|2022-04-15|2022-04-15 00:00:00|2022-04-07| 2022-08-15|\n",
      "+----------+-------------------+----------+----------+-------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('date_add',date_add(\"DATES\",7)).\\\n",
    " withColumn('date',to_date('DATES','yyyy-MM-dd')).\\\n",
    "     withColumn('timess',to_timestamp('DATES','yyyy-MM-dd')).\\\n",
    "         withColumn('date_sub',date_sub('TIMES',8)).\\\n",
    "             withColumn('dates_to_go',datediff(\"DATES\",\"date_add\")).\\\n",
    "                 withColumn('dates_to_go',round(months_between(\"DATES\",\"date_add\"),2)).\\\n",
    "                      withColumn('dates_to_go',add_months(\"TIMES\",4)).\\\n",
    "     show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time Trunc Function\n",
    "* *trunc()*: will return date and you can take 'MM' as month first date and 'yyyy' as year first date. It will return date there is no timestamp.\n",
    "* *date_trunc()*: will return timestamp and its very flexible as we can get 'mm' as month first timestamp,'yy' or 'yyyy' as year first timestamp,'HOUR' first timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function trunc in module pyspark.sql.functions:\n",
      "\n",
      "trunc(date, format)\n",
      "    Returns date truncated to the unit specified by the format.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    date : :class:`~pyspark.sql.Column` or str\n",
      "    format : str\n",
      "        'year', 'yyyy', 'yy' to truncate by year,\n",
      "        or 'month', 'mon', 'mm' to truncate by month\n",
      "        Other options are: 'week', 'quarter'\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
      "    >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
      "    [Row(year=datetime.date(1997, 1, 1))]\n",
      "    >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
      "    [Row(month=datetime.date(1997, 2, 1))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|     DATES|              TIMES|\n",
      "+----------+-------------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|\n",
      "|2018-02-12|2018-02-12 09:10:23|\n",
      "|2019-03-11|2019-03-11 11:03:22|\n",
      "|2022-04-15|2022-04-15 10:11:21|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trunc,date_trunc\n",
    "date_s=[(\"2021-01-14\",\"2021-01-14 10:00:21\"),\n",
    "(\"2018-02-12\",\"2018-02-12 09:10:23\"),\n",
    "(\"2019-03-11\",\"2019-03-11 11:03:22\"),\n",
    "(\"2022-04-15\",\"2022-04-15 10:11:21\")]\n",
    "data=spark.createDataFrame(date_s,schema=\"DATES STRING,TIMES STRING\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------------+----------------+----------------+---------------+\n",
      "|     DATES|              TIMES|first_year_date|first_year1_date|first_month_date|first_week_date|\n",
      "+----------+-------------------+---------------+----------------+----------------+---------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|     2021-01-01|      2021-01-01|      2021-01-01|     2021-01-11|\n",
      "|2018-02-12|2018-02-12 09:10:23|     2018-01-01|      2018-01-01|      2018-02-01|     2018-02-12|\n",
      "|2019-03-11|2019-03-11 11:03:22|     2019-01-01|      2019-01-01|      2019-03-01|     2019-03-11|\n",
      "|2022-04-15|2022-04-15 10:11:21|     2022-01-01|      2022-01-01|      2022-04-01|     2022-04-11|\n",
      "+----------+-------------------+---------------+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('first_year_date',trunc(col('DATES'),'yy')).\\\n",
    "    withColumn('first_year1_date',trunc(col('DATES'),'yyyy')).\\\n",
    "    withColumn('first_month_date',trunc(col('DATES'),'month')).\\\n",
    "    withColumn('first_week_date',trunc(col('DATES'),'week')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_trunc in module pyspark.sql.functions:\n",
      "\n",
      "date_trunc(format, timestamp)\n",
      "    Returns timestamp truncated to the unit specified by the format.\n",
      "    \n",
      "    .. versionadded:: 2.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    format : str\n",
      "        'year', 'yyyy', 'yy' to truncate by year,\n",
      "        'month', 'mon', 'mm' to truncate by month,\n",
      "        'day', 'dd' to truncate by day,\n",
      "        Other options are:\n",
      "        'microsecond', 'millisecond', 'second', 'minute', 'hour', 'week', 'quarter'\n",
      "    timestamp : :class:`~pyspark.sql.Column` or str\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
      "    >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n",
      "    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n",
      "    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n",
      "    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|     DATES|              TIMES|               year|              month|            quarter|               hour|             second|               week|\n",
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|2021-01-01 00:00:00|2021-01-01 00:00:00|2021-01-01 00:00:00|2021-01-14 10:00:00|2021-01-14 10:00:21|2021-01-11 00:00:00|\n",
      "|2018-02-12|2018-02-12 09:10:23|2018-01-01 00:00:00|2018-02-01 00:00:00|2018-01-01 00:00:00|2018-02-12 09:00:00|2018-02-12 09:10:23|2018-02-12 00:00:00|\n",
      "|2019-03-11|2019-03-11 11:03:22|2019-01-01 00:00:00|2019-03-01 00:00:00|2019-01-01 00:00:00|2019-03-11 11:00:00|2019-03-11 11:03:22|2019-03-11 00:00:00|\n",
      "|2022-04-15|2022-04-15 10:11:21|2022-01-01 00:00:00|2022-04-01 00:00:00|2022-04-01 00:00:00|2022-04-15 10:00:00|2022-04-15 10:11:21|2022-04-11 00:00:00|\n",
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('year',date_trunc('yy',col('TIMES'))).\\\n",
    "    withColumn('month',date_trunc('month',col('TIMES'))).\\\n",
    "    withColumn('quarter',date_trunc('quarter',col('TIMES'))).\\\n",
    "    withColumn('hour',date_trunc('hour',col('TIMES'))).\\\n",
    "    withColumn('second',date_trunc('second',col('TIMES'))).\\\n",
    "    withColumn('week',date_trunc('week',col('TIMES'))).\\\n",
    "    show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Time extract Function\n",
    "As we have worked on Date and timestamps now lets work on extracting information from date and timestamps.\n",
    "* year\n",
    "* month\n",
    "* weekofyear\n",
    "* dayofyear\n",
    "* dayofmonth\n",
    "* dayofweek\n",
    "* hour\n",
    "* minute\n",
    "* second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1eab8b3d330>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|     DATES|              TIMES|\n",
      "+----------+-------------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|\n",
      "|2018-02-12|2018-02-12 09:10:23|\n",
      "|2019-03-11|2019-03-11 11:03:22|\n",
      "|2022-04-15|2022-04-15 10:11:21|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year,month,weekofyear,dayofmonth,dayofweek,dayofyear,hour,minute,second\n",
    "date_s=[(\"2021-01-14\",\"2021-01-14 10:00:21\"),\n",
    "(\"2018-02-12\",\"2018-02-12 09:10:23\"),\n",
    "(\"2019-03-11\",\"2019-03-11 11:03:22\"),\n",
    "(\"2022-04-15\",\"2022-04-15 10:11:21\")]\n",
    "data=spark.createDataFrame(date_s,schema=\"DATES STRING,TIMES STRING\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function year in module pyspark.sql.functions:\n",
      "\n",
      "year(col)\n",
      "    Extract the year of a given date as integer.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(year('dt').alias('year')).collect()\n",
      "    [Row(year=2015)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----+-----+----------+---------+---------+----+------+\n",
      "|     DATES|              TIMES|year|month|weekofyear|dayofyear|dayofweek|hour|minute|\n",
      "+----------+-------------------+----+-----+----------+---------+---------+----+------+\n",
      "|2021-01-14|2021-01-14 10:00:21|2021|    1|         2|       14|        5|  10|     0|\n",
      "|2018-02-12|2018-02-12 09:10:23|2018|    2|         7|       43|        2|   9|    10|\n",
      "|2019-03-11|2019-03-11 11:03:22|2019|    3|        11|       70|        2|  11|     3|\n",
      "|2022-04-15|2022-04-15 10:11:21|2022|    4|        15|      105|        6|  10|    11|\n",
      "+----------+-------------------+----+-----+----------+---------+---------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('year',year(col('TIMES'))).\\\n",
    "    withColumn('month',month(col('TIMES'))).\\\n",
    "    withColumn('weekofyear',weekofyear(col('TIMES'))).\\\n",
    "    withColumn('dayofyear',dayofyear(col('TIMES'))).\\\n",
    "    withColumn('dayofweek',dayofweek(col('TIMES'))).\\\n",
    "    withColumn('hour',hour(col('TIMES'))).\\\n",
    "    withColumn('minute',minute(col('TIMES'))).\\\n",
    "show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using to_date and to_timestamp on Spark Data Frames**\n",
    "* to_date(): Is used for converting a string or int or long value to date datatype after sharing what every positional letter means. It convert to yyyy-MM-dd.\n",
    "* to_timestamp(): Is used for converting a string or int or long value to timestamp datatype after sharing what every positional letter means. It convert to yyyy-MM-dd HH:mm:ss SSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     slash|  --format|\n",
      "+----------+----------+\n",
      "|2023-01-22|2023-01-11|\n",
      "|2023-01-22|2023-01-11|\n",
      "|2023-01-22|2023-01-11|\n",
      "|2023-01-22|2023-01-11|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(to_date(lit('2023/01/22'),'yyyy/MM/dd').alias('slash'),to_date(lit('2023--01--11 11:11:11'),'yyyy--MM--dd HH:mm:ss').alias('--format')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|       MMM|      MMMM|\n",
      "+----------+----------+\n",
      "|2023-05-01|2023-03-01|\n",
      "|2023-05-01|2023-03-01|\n",
      "|2023-05-01|2023-03-01|\n",
      "|2023-05-01|2023-03-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(to_date(lit('2023-May-01'),'yyyy-MMM-dd').alias('MMM'),to_date(lit('2023-March-01'),'yyyy-MMMM-dd').alias('MMMM')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|       MMM|               MMMM|\n",
      "+----------+-------------------+\n",
      "|2023-05-01|2023-03-01 07:23:11|\n",
      "|2023-05-01|2023-03-01 07:23:11|\n",
      "|2023-05-01|2023-03-01 07:23:11|\n",
      "|2023-05-01|2023-03-01 07:23:11|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(to_date(lit('May ,2023,01'),'MMMM ,yyyy,dd').alias('MMM'),to_timestamp(lit('2023-March-01 07:23:11'),'yyyy-MMMM-dd HH:mm:ss').alias('MMMM')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select(to_date(lit('2023-May-01'),'yyyy-MMM-dd').alias('MMM'),to_date(lit('2023-March-01'),'yyyy-MMMM-dd').alias('MMMM')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using date_format Function on Spark Data Frames**\n",
    "* When we have timestamp or date or string proper formated data and we want to convert into an other formate to make is useful.\n",
    "* It is converted to string datatype.\n",
    "* yyyy for year.\n",
    "* MM for month.\n",
    "* MMM for alphabetical 3 letter month.\n",
    "* MMMM for alphabetical full name of month.\n",
    "* dd for day.\n",
    "* DD for day in the year.\n",
    "* HH for hour 24 hour.\n",
    "* hh for hour 12 hour.\n",
    "* ss for second.\n",
    "* SSS for millisecond.\n",
    "* EE for getting week day short name.\n",
    "* EEEE for getting week day full name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|     DATES|              TIMES|\n",
      "+----------+-------------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|\n",
      "|2018-02-12|2018-02-12 09:10:23|\n",
      "|2019-03-11|2019-03-11 11:03:22|\n",
      "|2022-04-15|2022-04-15 10:11:21|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year,month,weekofyear,dayofmonth,dayofweek,dayofyear,hour,minute,second\n",
    "date_s=[(\"2021-01-14\",\"2021-01-14 10:00:21\"),\n",
    "(\"2018-02-12\",\"2018-02-12 09:10:23\"),\n",
    "(\"2019-03-11\",\"2019-03-11 11:03:22\"),\n",
    "(\"2022-04-15\",\"2022-04-15 10:11:21\")]\n",
    "data=spark.createDataFrame(date_s,schema=\"DATES STRING,TIMES STRING\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----------------+----------------+------------+--------+----+---+--------+\n",
      "|     DATES|              TIMES|           years|           slash|short_months|   month|hour|Day|Day_full|\n",
      "+----------+-------------------+----------------+----------------+------------+--------+----+---+--------+\n",
      "|2021-01-14|2021-01-14 10:00:21| 2021-January-14| 2021/January/14| 2021-Jan-14| January|  10|Thu|Thursday|\n",
      "|2018-02-12|2018-02-12 09:10:23|2018-February-12|2018/February/12| 2018-Feb-12|February|  09|Mon|  Monday|\n",
      "|2019-03-11|2019-03-11 11:03:22|   2019-March-11|   2019/March/11| 2019-Mar-11|   March|  11|Mon|  Monday|\n",
      "|2022-04-15|2022-04-15 10:11:21|   2022-April-15|   2022/April/15| 2022-Apr-15|   April|  10|Fri|  Friday|\n",
      "+----------+-------------------+----------------+----------------+------------+--------+----+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('years',date_format('DATES','yyyy-MMMM-dd')).\\\n",
    "    withColumn('slash',date_format('DATES','yyyy/MMMM/dd')).\\\n",
    "    withColumn('short_months',date_format('DATES','yyyy-MMM-dd')).\\\n",
    "    withColumn('month',date_format('DATES','MMMM')).\\\n",
    "    withColumn('hour',date_format('TIMES','hh')).\\\n",
    "    withColumn('Day',date_format('TIMES','EE')).\\\n",
    "    withColumn('Day_full',date_format('TIMES','EEEE')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATES: string (nullable = true)\n",
      " |-- TIMES: string (nullable = true)\n",
      " |-- years: string (nullable = true)\n",
      " |-- slash: string (nullable = true)\n",
      " |-- short_months: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('years',date_format('DATES','yyyy-MMMM-dd')).\\\n",
    "    withColumn('slash',date_format('DATES','yyyy/MMMM/dd')).\\\n",
    "    withColumn('short_months',date_format('DATES','yyyy-MMM-dd')).\\\n",
    "    withColumn('month',date_format('DATES','MMMM')).\\\n",
    "    withColumn('hour',date_format('TIMES','hh')).\\\n",
    "    printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dealing with Unix Timestamp in Spark Data Frames**\n",
    "* Unix timestamp is time that has started from 1970 and every second add up a value in it.\n",
    "* If you want to convert normal date and time stamp to unix timestamp than use function as **unix_timestamp(col(),'formate')**.\n",
    "* It is important to share the formate as we need to show th unix_timestamp function that want exactly is the date so it could convert it.\n",
    "* Now if you have unix timestamp value you can convert it into date and time stamp **from_unixtime(col(),'yyyyMMdd')**.\n",
    "* If we dont share the formate in the function then it will convert to timestamp standard as yyyy-MM-dd HH:mm:ss SSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24848f04b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|     DATES|              TIMES|\n",
      "+----------+-------------------+\n",
      "|2021-01-14|2021-01-14 10:00:21|\n",
      "|2018-02-12|2018-02-12 09:10:23|\n",
      "|2019-03-11|2019-03-11 11:03:22|\n",
      "|2022-04-15|2022-04-15 10:11:21|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year,month,weekofyear,dayofmonth,dayofweek,dayofyear,hour,minute,second\n",
    "date_s=[(\"2021-01-14\",\"2021-01-14 10:00:21\"),\n",
    "(\"2018-02-12\",\"2018-02-12 09:10:23\"),\n",
    "(\"2019-03-11\",\"2019-03-11 11:03:22\"),\n",
    "(\"2022-04-15\",\"2022-04-15 10:11:21\")]\n",
    "data=spark.createDataFrame(date_s,schema=\"DATES STRING,TIMES STRING\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----------+----------+\n",
      "|     DATES|              TIMES| unix_date| unix_time|\n",
      "+----------+-------------------+----------+----------+\n",
      "|2021-01-14|2021-01-14 10:00:21|1610564400|1610600421|\n",
      "|2018-02-12|2018-02-12 09:10:23|1518375600|1518408623|\n",
      "|2019-03-11|2019-03-11 11:03:22|1552244400|1552284202|\n",
      "|2022-04-15|2022-04-15 10:11:21|1649962800|1649999481|\n",
      "+----------+-------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('unix_date',unix_timestamp('DATES','yyyy-MM-dd')).\\\n",
    "    withColumn('unix_time',unix_timestamp('TIMES','yyyy-MM-dd HH:mm:ss')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     unixs|\n",
      "+----------+\n",
      "|1678934400|\n",
      "|1681564800|\n",
      "|1684204800|\n",
      "|1686835200|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d=[('1678934400',),\n",
    "('1681564800',),\n",
    "('1684204800',),\n",
    "('1686835200',)]\n",
    "df=spark.createDataFrame(d,schema=\"unixs string\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+\n",
      "|     unixs|     dates|              times|\n",
      "+----------+----------+-------------------+\n",
      "|1678934400|2023-03-16|2023-03-16 07:40:00|\n",
      "|1681564800|2023-04-15|2023-04-15 18:20:00|\n",
      "|1684204800|2023-05-16|2023-05-16 07:40:00|\n",
      "|1686835200|2023-06-15|2023-06-15 18:20:00|\n",
      "+----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('dates',from_unixtime('unixs','yyyy-MM-dd')).withColumn('times',from_unixtime('unixs')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dealing with nulls in Spark Data Frames**\n",
    "* To deal with null we have **coalesce,fill,re**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = [\n",
    "    {\"string_column\": \"value1\", \"float_column\": 1.23, \"int_column\": 10, \"timestamp_column\": \"2023-01-15 00:00:00\"},\n",
    "    {\"string_column\": \"\", \"float_column\": None, \"int_column\": None, \"timestamp_column\": \"2023-02-28 00:00:00\"},\n",
    "    {\"string_column\": \"null\", \"float_column\": 4.56, \"int_column\": 20, \"timestamp_column\": None},\n",
    "    {\"string_column\": None, \"float_column\": 7.89, \"int_column\": 30, \"timestamp_column\": \"2023-03-10 00:00:00\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=spark.createDataFrame(da)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- float_column: double (nullable = true)\n",
      " |-- int_column: long (nullable = true)\n",
      " |-- string_column: string (nullable = true)\n",
      " |-- timestamp_column: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+-------------+-----------+--------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|float_columns|int_columns|string_columns|  timestamp_columns|\n",
      "+------------+----------+-------------+-------------------+-------------+-----------+--------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|         1.23|         10|        value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|            0|          0|              |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|         4.56|         20|          null|                  0|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|         7.89|         30|             0|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+-------------+-----------+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('float_columns',coalesce('float_column',lit('0'))).\\\n",
    "    withColumn('int_columns',coalesce('int_column',lit('0'))).\\\n",
    "    withColumn('string_columns',coalesce('string_column',lit('0'))).\\\n",
    "    withColumn('timestamp_columns',coalesce('timestamp_column',lit('0'))).\\\n",
    "    show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we want to replace 'null' by 0 then we use nullif to make it to null and then after that using nvl it will convert null value to 0.\n",
    "* If we have '' then we can do it by using nested nullif function over nullif function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+-------+\n",
      "|float_column|int_column|string_column|   timestamp_column|strings|\n",
      "+------------+----------+-------------+-------------------+-------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00| value1|\n",
      "|        null|      null|             |2023-02-28 00:00:00|       |\n",
      "|        4.56|        20|         null|               null|      0|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|      0|\n",
      "+------------+----------+-------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('strings',expr(\"nvl(nullif(string_column,'null'),0)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+-------+\n",
      "|float_column|int_column|string_column|   timestamp_column|strings|\n",
      "+------------+----------+-------------+-------------------+-------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00| value1|\n",
      "|        null|      null|             |2023-02-28 00:00:00|      0|\n",
      "|        4.56|        20|         null|               null|      0|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|      0|\n",
      "+------------+----------+-------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('strings',expr(\"nvl(nullif(nullif(string_column,'null'),''),0)\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To deal with **NA,EMPTY values ,and null values** first convert it to int and you will find out that it will convert to null (null is not a value its a property) and then you can use \\\n",
    "    coalesce on top of that result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+-----+----+-----+--------------+---------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|EMPTY| NAs|nulls|coalesce_empty|string_columnss|\n",
      "+------------+----------+-------------+-------------------+-----+----+-----+--------------+---------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00| null|null| null|             1|              1|\n",
      "|        null|      null|             |2023-02-28 00:00:00| null|null| null|             1|              1|\n",
      "|        4.56|        20|       value3|               null| null|null| null|             1|              1|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00| null|null| null|             1|              1|\n",
      "+------------+----------+-------------+-------------------+-----+----+-----+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('EMPTY',lit('').cast('int')).\\\n",
    "    withColumn('NAs',lit('NA').cast('int')).\\\n",
    "    withColumn('nulls',lit('null').cast('int')).\\\n",
    "    withColumn('coalesce_empty',coalesce(lit('').cast('int'),lit(1))).\\\n",
    "    withColumn('string_columnss',coalesce(col('string_column').cast('int'),lit(1))).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----------+------------------+\n",
      "|float_column|nvl(float_column, 0)|int_column|nvl(int_column, 1)|\n",
      "+------------+--------------------+----------+------------------+\n",
      "|        1.23|                1.23|        10|                10|\n",
      "|        null|                 0.0|      null|                 1|\n",
      "|        4.56|                4.56|        20|                20|\n",
      "|        7.89|                7.89|        30|                30|\n",
      "+------------+--------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.selectExpr('float_column',\"nvl(float_column,0)\",'int_column',\"nvl(int_column,1)\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here below the nullif will check is float_column value is equal to '' if yes then it will replace it will null property and then nvl will replace it by default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+----------+---+\n",
      "|float_column| abc|int_column|bcs|\n",
      "+------------+----+----------+---+\n",
      "|        1.23|1.23|        10| 10|\n",
      "|        null| 0.0|      null|  1|\n",
      "|        4.56|4.56|        20| 20|\n",
      "|        7.89|7.89|        30| 30|\n",
      "+------------+----+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.selectExpr('float_column',\"nvl(nullif(float_column,''),0) as abc\",'int_column',\"nvl(nullif(int_column,''),1) as bcs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|             total|\n",
      "+------------+----------+-------------+-------------------+------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|0.6150000095367432|\n",
      "|        null|      null|             |2023-02-28 00:00:00|               0.0|\n",
      "|        4.56|        20|         null|               null|2.2799999713897705|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00| 3.944999933242798|\n",
      "+------------+----------+-------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('total',coalesce(col('float_column').cast('float'),lit(0))/lit(2)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* df.na and df.na.fill and fillna are same.\n",
    "* df.na.drop and df.dropna() as same.\n",
    "* df.na.replace and df.replacena() are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b2580a4b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = [\n",
    "    {\"string_column\": \"value1\", \"float_column\": 1.23, \"int_column\": 10, \"timestamp_column\": \"2023-01-15 00:00:00\"},\n",
    "    {\"string_column\": \"\", \"float_column\": None, \"int_column\": None, \"timestamp_column\": \"2023-02-28 00:00:00\"},\n",
    "    {\"string_column\": \"null\", \"float_column\": 4.56, \"int_column\": 20, \"timestamp_column\": None},\n",
    "    {\"string_column\": None, \"float_column\": 7.89, \"int_column\": 30, \"timestamp_column\": \"2023-03-10 00:00:00\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=spark.createDataFrame(da)\n",
    "data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If I want to fill string_column column with '0' we can specify that as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|            0|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.fillna('0','string_column').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you want to work on whole dataframe you can do it by just using df.fillna('0') as 0 is string type so it will replace the columns data that are string columns.\n",
    "* If we use df.fillna(0) it will replace all int and long columns.\n",
    "* you will see null in string even after using fillna function reason is that null in this case is string not a property that is why we spark cant identify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|         0.0|         0|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.fillna(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|                  0|\n",
      "|        7.89|        30|            0|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.fillna('0').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What if we have list of columns here is the solution for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|         0.0|         0|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.fillna(0,['float_column','int_column']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameNaFunctions in module pyspark.sql.dataframe object:\n",
      "\n",
      "class DataFrameNaFunctions(builtins.object)\n",
      " |  DataFrameNaFunctions(df)\n",
      " |  \n",
      " |  Functionality for working with missing data in :class:`DataFrame`.\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, df)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  drop(self, how='any', thresh=None, subset=None)\n",
      " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      how : str, optional\n",
      " |          'any' or 'all'.\n",
      " |          If 'any', drop a row if it contains any nulls.\n",
      " |          If 'all', drop a row only if all its values are null.\n",
      " |      thresh: int, optional\n",
      " |          default None\n",
      " |          If specified, drop rows that have less than `thresh` non-null values.\n",
      " |          This overwrites the `how` parameter.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.drop().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |  \n",
      " |  fill(self, value, subset=None)\n",
      " |      Replace null values, alias for ``na.fill()``.\n",
      " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : int, float, string, bool or dict\n",
      " |          Value to replace null values with.\n",
      " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      " |          from column name (string) to replacement value. The replacement value must be\n",
      " |          an int, float, boolean, or string.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.fill(50).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      |  5|    50|  Bob|\n",
      " |      | 50|    50|  Tom|\n",
      " |      | 50|    50| null|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df5.na.fill(False).show()\n",
      " |      +----+-------+-----+\n",
      " |      | age|   name|  spy|\n",
      " |      +----+-------+-----+\n",
      " |      |  10|  Alice|false|\n",
      " |      |   5|    Bob|false|\n",
      " |      |null|Mallory| true|\n",
      " |      +----+-------+-----+\n",
      " |      \n",
      " |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      " |      +---+------+-------+\n",
      " |      |age|height|   name|\n",
      " |      +---+------+-------+\n",
      " |      | 10|    80|  Alice|\n",
      " |      |  5|  null|    Bob|\n",
      " |      | 50|  null|    Tom|\n",
      " |      | 50|  null|unknown|\n",
      " |      +---+------+-------+\n",
      " |  \n",
      " |  replace(self, to_replace, value=<no value>, subset=None)\n",
      " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      " |      aliases of each other.\n",
      " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      " |      or strings. Value can have None. When replacing, the new value will be cast\n",
      " |      to the type of the existing column.\n",
      " |      For numeric replacements all values to be replaced should have unique\n",
      " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      " |      and arbitrary replacement will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      to_replace : bool, int, float, string, list or dict\n",
      " |          Value to be replaced.\n",
      " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      " |          must be a mapping between a value and a replacement.\n",
      " |      value : bool, int, float, string or None, optional\n",
      " |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      " |          list, `value` should be of the same length and type as `to_replace`.\n",
      " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      " |          used as a replacement for each item in `to_replace`.\n",
      " |      subset : list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.replace(10, 20).show()\n",
      " |      +----+------+-----+\n",
      " |      | age|height| name|\n",
      " |      +----+------+-----+\n",
      " |      |  20|    80|Alice|\n",
      " |      |   5|  null|  Bob|\n",
      " |      |null|  null|  Tom|\n",
      " |      |null|  null| null|\n",
      " |      +----+------+-----+\n",
      " |      \n",
      " |      >>> df4.na.replace('Alice', None).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace({'Alice': None}).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|   A|\n",
      " |      |   5|  null|   B|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(data.na)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using CASE and WHEN on Spark Data Frames**\n",
    "* If there is null values we can use coalesce now we can use case statement as well.\n",
    "* if we use dataframe expression method we use CASE WHEN THEN ELSE END.\n",
    "* if we use dataframe method we will use when (condition,value_to_fill_if_satified) ,otherwise(elsevalue)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe using list called as persons and categorize them based up on following rules.\n",
    "* 0 to 2 Months than New Born\n",
    "* 2+ Months to 12 Months than Infant\n",
    "* 12+ Months to 48 Months than Toddler\n",
    "* 48+ Months to 144 Months than Kids\n",
    "* 144+ Months than Teenager or Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2| 13|\n",
      "|  3| 18|\n",
      "|  4| 60|\n",
      "|  5|120|\n",
      "|  6|  0|\n",
      "|  7| 12|\n",
      "|  8|160|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons = [\n",
    "    (1, 1),\n",
    "    (2, 13),\n",
    "    (3, 18),\n",
    "    (4, 60),\n",
    "    (5, 120),\n",
    "    (6, 0),\n",
    "    (7, 12),\n",
    "    (8, 160)\n",
    "]\n",
    "personsDF = spark.createDataFrame(persons, schema='id INT, age INT')\n",
    "personsDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+\n",
      "| id|age|categorys|\n",
      "+---+---+---------+\n",
      "|  1|  1| New Born|\n",
      "|  2| 13|  Toddler|\n",
      "|  3| 18|  Toddler|\n",
      "|  4| 60|     Kids|\n",
      "|  5|120|     Kids|\n",
      "|  6|  0| New Born|\n",
      "|  7| 12|   Infant|\n",
      "|  8|160|    ADULT|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.withColumn('categorys',expr(\"\"\"\n",
    "                                     CASE when age >= 0 AND age <= 2 then 'New Born' \n",
    "                                     when age between 2 and 12 then 'Infant'\n",
    "                                     when age between 12 and 48 then 'Toddler'\n",
    "                                     when age between 48 and 144 then 'Kids'\n",
    "                                     when age >144 then 'ADULT' ELSE 'None'                           \n",
    "                                     END\n",
    "                                     \"\"\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+\n",
      "| id|age|category|\n",
      "+---+---+--------+\n",
      "|  1|  1|New Born|\n",
      "|  2| 13| Toddler|\n",
      "|  3| 18| Toddler|\n",
      "|  4| 60|    Kids|\n",
      "|  5|120|    Kids|\n",
      "|  6|  0|New Born|\n",
      "|  7| 12|  Infant|\n",
      "|  8|160|   ADULT|\n",
      "+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.withColumn('category',when(col('age').between(0,2),'New Born').\\\n",
    "                                when((col('age')>=2) & (col('age')<=12),'Infant').\\\n",
    "                                when((col('age')>=12) & (col('age')<=48),'Toddler').\\\n",
    "                                when((col('age')>=48) & (col('age')<=144),'Kids').\\\n",
    "                                when((col('age')>144),'ADULT').\\\n",
    "                                    otherwise('None')).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+\n",
      "| id|age|categorys|\n",
      "+---+---+---------+\n",
      "|  1|  1| New Born|\n",
      "|  2| 13|  Toddler|\n",
      "|  3| 18|  Toddler|\n",
      "|  4| 60|     Kids|\n",
      "|  5|120|     Kids|\n",
      "|  6|  0| New Born|\n",
      "|  7| 12|   Infant|\n",
      "|  8|160|    ADULT|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.withColumn('categorys', expr(\"\"\"\n",
    "                                      CASE when age >= 0 AND age <= 2 then 'New Born' \n",
    "                                      when age between 2 and 12 then 'Infant'\n",
    "                                      when age between 12 and 48 then 'Toddler'\n",
    "                                      when age between 48 and 144 then 'Kids'\n",
    "                                      when age > 144 then 'ADULT' ELSE 'None'                           \n",
    "                                      END\n",
    "                                      \"\"\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of Filter or Where Function on Spark Data Frame**\n",
    "* When we have Dataframe we can user where as well as filter to filter the values based upon the conditions.\n",
    "* While using Dataframe we can use filter in both python style syntax as df.filter (df['column']>1).show() or df.filter(col('column)).show().\n",
    "* We can use SQL type conditions as well as df.filter (\"column > 1\").show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, BooleanType\n",
    "\n",
    "sc=StructType([StructField(\"id\",IntegerType()),\n",
    "               StructField(\"age\",IntegerType()),\n",
    "               StructField(\"status\",BooleanType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "| id|age|status|\n",
      "+---+---+------+\n",
      "|  1|  1|  true|\n",
      "|  2| 13| false|\n",
      "|  3| 18| false|\n",
      "|  4| 60|  true|\n",
      "|  5|120|  true|\n",
      "|  6|  0| false|\n",
      "|  7| 12|  null|\n",
      "|  8|160|  null|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons = [\n",
    "    (1, 1,True),\n",
    "    (2, 13,False),\n",
    "    (3, 18,False),\n",
    "    (4, 60,True),\n",
    "    (5, 120,True),\n",
    "    (6, 0,False),\n",
    "    (7, 12,),\n",
    "    (8, 160,)\n",
    "]\n",
    "df=pd.DataFrame(persons,columns=['id','age','status'])\n",
    "pf=spark.createDataFrame(df)\n",
    "pf.show()\n",
    "# personsDF = spark.createDataFrame(persons, schema=sc)\n",
    "# personsDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "| id|age|status|\n",
      "+---+---+------+\n",
      "|  3| 18| false|\n",
      "|  4| 60|  true|\n",
      "|  5|120|  true|\n",
      "|  8|160| false|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.filter(personsDF['age']>13).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  3| 18|\n",
      "|  4| 60|\n",
      "|  5|120|\n",
      "|  8|160|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.where(col('age')>13).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  4| 60|\n",
      "|  5|120|\n",
      "|  8|160|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.filter(\"age>13\").where (\"age>18\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of Conditions and Operators related to Spark Data Frames**\n",
    "* Now here we have conditional logic that we can used in filter and where function to filter our data.\n",
    "* Equal :: == or =\n",
    "* Not Equal :: !=\n",
    "* Great than :: >\n",
    "* Less than :: <\n",
    "* Great than or Equal to :: >=\n",
    "* Less than or Equal to :: <=\n",
    "* In operator :: isin or IN\n",
    "* Between Operator :: between function or BETWEEN WITH AND\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2| 13|\n",
      "|  3| 18|\n",
      "|  4| 60|\n",
      "|  5|120|\n",
      "|  6|  0|\n",
      "|  7| 12|\n",
      "|  8|160|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.filter(personsDF['age']==1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.filter('age = 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.where('age = 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "| id|age|status|\n",
      "+---+---+------+\n",
      "|  1|  1|  true|\n",
      "|  4| 60|  true|\n",
      "|  5|120|  true|\n",
      "|  7| 12|  true|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.where('status = true').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "| id|age|status|\n",
      "+---+---+------+\n",
      "|  1|  1|  true|\n",
      "|  4| 60|  true|\n",
      "|  5|120|  true|\n",
      "|  7| 12|  true|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.where(expr('status = true')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "| id|age|status|\n",
      "+---+---+------+\n",
      "|  1|  1|  true|\n",
      "|  2| 13| false|\n",
      "|  3| 18| false|\n",
      "|  4| 60|  true|\n",
      "|  5|120|  true|\n",
      "|  6|  0| false|\n",
      "|  7| 12|  null|\n",
      "|  8|160|  null|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "| id|age|status|\n",
      "+---+---+------+\n",
      "|  1|  1|  true|\n",
      "|  2| 13| false|\n",
      "|  3| 18| false|\n",
      "|  4| 60|  true|\n",
      "|  5|120|  true|\n",
      "|  6|  0| false|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.filter(\"isnull(status)==False\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get all the ids that have status as true. Here you will see that value that are null are ignored. we can bring that back as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|status|\n",
      "+---+------+\n",
      "|  1|  true|\n",
      "|  4|  true|\n",
      "|  5|  true|\n",
      "|  7|  null|\n",
      "|  8|  null|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select('id','status').filter(\"status != False OR status is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|status|\n",
      "+---+------+\n",
      "|  2| false|\n",
      "|  3| false|\n",
      "|  6| false|\n",
      "|  7|  null|\n",
      "|  8|  null|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select('id', 'status').filter((col('status') != True) | (col('status').isNull())).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Filter using Between Operator on Spark Data Frames**\n",
    "* If we want to use non SQL type syntax then name the dataframe and call the column name and use .between(a,b).\n",
    "* If we want to use SQL type syntax we will use \"between a AND b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "da = [\n",
    "    {\"string_column\": \"value1\", \"float_column\": 1.23, \"int_column\": 10, \"timestamp_column\": \"2023-01-15 00:00:00\"},\n",
    "    {\"string_column\": \"\", \"float_column\": None, \"int_column\": None, \"timestamp_column\": \"2023-02-28 00:00:00\"},\n",
    "    {\"string_column\": \"null\", \"float_column\": 4.56, \"int_column\": 20, \"timestamp_column\": None},\n",
    "    {\"string_column\": None, \"float_column\": 7.89, \"int_column\": 30, \"timestamp_column\": \"2023-03-10 00:00:00\"}\n",
    "]\n",
    "\n",
    "data=spark.createDataFrame(da)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|string_column|   timestamp_column|\n",
      "+-------------+-------------------+\n",
      "|       value1|2023-01-15 00:00:00|\n",
      "|             |2023-02-28 00:00:00|\n",
      "+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('string_column','timestamp_column').filter(data.timestamp_column.between('2023-01-01','2023-02-28 01:00:11')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|string_column|   timestamp_column|\n",
      "+-------------+-------------------+\n",
      "|       value1|2023-01-15 00:00:00|\n",
      "|             |2023-02-28 00:00:00|\n",
      "+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('string_column','timestamp_column').filter(data['timestamp_column'].between('2023-01-01','2023-02-28 01:00:11')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL Style Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|string_column|   timestamp_column|\n",
      "+-------------+-------------------+\n",
      "|       value1|2023-01-15 00:00:00|\n",
      "|             |2023-02-28 00:00:00|\n",
      "+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('string_column','timestamp_column').filter(\"timestamp_column BETWEEN '2023-01-01' AND '2023-02-29'\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dealing with Null Values while Filtering Data in Spark Data Frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IT-130-21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x266e3204b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql  import Row\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+-------------------+\n",
      "|float_column|int_column|string_column|   timestamp_column|\n",
      "+------------+----------+-------------+-------------------+\n",
      "|        1.23|        10|       value1|2023-01-15 00:00:00|\n",
      "|        null|      null|             |2023-02-28 00:00:00|\n",
      "|        4.56|        20|         null|               null|\n",
      "|        7.89|        30|         null|2023-03-10 00:00:00|\n",
      "+------------+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "da = [\n",
    "    {\"string_column\": \"value1\", \"float_column\": 1.23, \"int_column\": 10, \"timestamp_column\": \"2023-01-15 00:00:00\"},\n",
    "    {\"string_column\": \"\", \"float_column\": None, \"int_column\": None, \"timestamp_column\": \"2023-02-28 00:00:00\"},\n",
    "    {\"string_column\": \"null\", \"float_column\": 4.56, \"int_column\": 20, \"timestamp_column\": None},\n",
    "    {\"string_column\": None, \"float_column\": 7.89, \"int_column\": 30, \"timestamp_column\": \"2023-03-10 00:00:00\"}\n",
    "]\n",
    "\n",
    "data=spark.createDataFrame(da)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|float_column|string_column|\n",
      "+------------+-------------+\n",
      "|        1.23|       value1|\n",
      "|        null|             |\n",
      "|        4.56|         null|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('float_column','string_column').filter(col('string_column').isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|float_column|string_column|\n",
      "+------------+-------------+\n",
      "|        7.89|         null|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('float_column','string_column').filter(col('string_column').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|float_column|string_column|\n",
      "+------------+-------------+\n",
      "|        1.23|       value1|\n",
      "|        null|             |\n",
      "|        4.56|         null|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('float_column','string_column').filter(expr('string_column is not null')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show the ids that have vehical null and empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  1|  1|  true|    car|\n",
      "|  2| 13| false|   buss|\n",
      "|  3| 18| false|   bike|\n",
      "|  4| 60|  true|   null|\n",
      "|  5|120|  true|   null|\n",
      "|  6|  0| false|       |\n",
      "|  7| 12|  null|  cycle|\n",
      "|  8|160|  null|       |\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons = [\n",
    "    (1, 1,True,'car'),\n",
    "    (2, 13,False,'buss'),\n",
    "    (3, 18,False,'bike'),\n",
    "    (4, 60,True,'null'),\n",
    "    (5, 120,True,None),\n",
    "    (6, 0,False,''),\n",
    "    (7, 12,None,'cycle'),\n",
    "    (8, 160,None, '')\n",
    "]\n",
    "df=pd.DataFrame(persons,columns=['id','age','status','vehical'])\n",
    "pf=spark.createDataFrame(df)\n",
    "pf.show()\n",
    "# personsDF = spark.createDataFrame(persons, schema=sc)\n",
    "# personsDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  5|120|  true|   null|\n",
      "|  6|  0| false|       |\n",
      "|  8|160|  null|       |\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select ('*').filter((col('vehical').isNull()) | (col('vehical') == '')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL Type syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  5|120|  true|   null|\n",
      "|  6|  0| false|       |\n",
      "|  8|160|  null|       |\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select('*').filter (\"vehical is null or vehical == ''\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  5|120|  true|   null|\n",
      "|  6|  0| false|       |\n",
      "|  8|160|  null|       |\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select('*').filter (\"vehical is null or vehical == ''\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  1|  1|  true|    car|\n",
      "|  2| 13| false|   buss|\n",
      "|  3| 18| false|   bike|\n",
      "|  4| 60|  true|   null|\n",
      "|  5|120|  true|   null|\n",
      "|  6|  0| false|       |\n",
      "|  7| 12|  null|  cycle|\n",
      "|  8|160|  null|       |\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now show me records that have vehical as car bike and null.\n",
    "* Here you need to use isnull function separately as Null is not a value it is a property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  1|  1|  true|    car|\n",
      "|  3| 18| false|   bike|\n",
      "|  5|120|  true|   null|\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select('*').filter (col('vehical').isin('car','bike')|col('vehical').isNull()).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL style syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "|  1|  1|  true|    car|\n",
      "|  3| 18| false|   bike|\n",
      "|  5|120|  true|   null|\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.select('*').filter(\"vehical IN ('car','bike') or vehical is null\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+\n",
      "| id|age|status|vehical|\n",
      "+---+---+------+-------+\n",
      "+---+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.filter((col('vehical') == 'car') & (col('status').isNull())).select('*').show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL Style Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|status|\n",
      "+---+------+\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.filter(\"vehical = 'car' and status is null\").select('id','status').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Spark Data Frame for Dropping Columns**\n",
    "* Lets now come up with some smart work.\n",
    "* Create a file sources.ipynb and then place the script of creating session and dataframes and also print out what is created over there.\n",
    "* We can use col('column name') and df['column name'] and string of column names to drop a columns.\n",
    "* The column you want to drop if that does not exist then it will not throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method drop in module pyspark.sql.dataframe:\n",
      "\n",
      "drop(*cols) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` that drops the specified column.\n",
      "    This is a no-op if schema doesn't contain the given column name(s).\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    cols: str or :class:`Column`\n",
      "        a name of the column, or the :class:`Column` to drop\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.drop('age').collect()\n",
      "    [Row(name='Alice'), Row(name='Bob')]\n",
      "    \n",
      "    >>> df.drop(df.age).collect()\n",
      "    [Row(name='Alice'), Row(name='Bob')]\n",
      "    \n",
      "    >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      "    [Row(age=5, height=85, name='Bob')]\n",
      "    \n",
      "    >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      "    [Row(age=5, name='Bob', height=85)]\n",
      "    \n",
      "    >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      "    [Row(name='Bob')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sm.drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm.drop('nationality','bonus').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----+----------------+-----------+\n",
      "|employee_id|salary|bonus|    phone_number|        ssn|\n",
      "+-----------+------+-----+----------------+-----------+\n",
      "|          1|1000.0|   10| +1 123 456 7890|123 45 6789|\n",
      "|          2|1250.0| null|+91 234 567 8901|456 78 9123|\n",
      "|          3| 750.0|     |+44 111 111 1111|222 33 4444|\n",
      "|          4|1500.0|   10|+61 987 654 3210|789 12 6118|\n",
      "+-----------+------+-----+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm.drop('nationality','first_name','last_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----+----------------+-----------+\n",
      "|employee_id|salary|bonus|    phone_number|        ssn|\n",
      "+-----------+------+-----+----------------+-----------+\n",
      "|          1|1000.0|   10| +1 123 456 7890|123 45 6789|\n",
      "|          2|1250.0| null|+91 234 567 8901|456 78 9123|\n",
      "|          3| 750.0|     |+44 111 111 1111|222 33 4444|\n",
      "|          4|1500.0|   10|+61 987 654 3210|789 12 6118|\n",
      "+-----------+------+-----+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm.drop(*['nationality','first_name','last_name']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dropping Duplicate Records from Spark Data Frames**\n",
    "* Distinct , dropDuplicates are doing same job.\n",
    "* If you pass a column name through dropDuplicates(['id']) it will start droping bases of this column.\n",
    "* We can pass list of columns to see for duplicated on the bases of those columns and then remove the duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method distinct in module pyspark.sql.dataframe:\n",
      "\n",
      "distinct() method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.distinct().count()\n",
      "    2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sm.distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dropDuplicates in module pyspark.sql.dataframe:\n",
      "\n",
      "dropDuplicates(subset=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "    optionally only considering certain columns.\n",
      "    \n",
      "    For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      "    :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      "    duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      "    be and system will accordingly limit the state. In addition, too late data older than\n",
      "    watermark will be dropped to avoid any possibility of duplicates.\n",
      "    \n",
      "    :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> df = sc.parallelize([ \\\n",
      "    ...     Row(name='Alice', age=5, height=80), \\\n",
      "    ...     Row(name='Alice', age=5, height=80), \\\n",
      "    ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      "    >>> df.dropDuplicates().show()\n",
      "    +-----+---+------+\n",
      "    | name|age|height|\n",
      "    +-----+---+------+\n",
      "    |Alice|  5|    80|\n",
      "    |Alice| 10|    80|\n",
      "    +-----+---+------+\n",
      "    \n",
      "    >>> df.dropDuplicates(['name', 'height']).show()\n",
      "    +-----+---+------+\n",
      "    | name|age|height|\n",
      "    +-----+---+------+\n",
      "    |Alice|  5|    80|\n",
      "    +-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sm.dropDuplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %run ./sources.ipynb\n",
    "%run ./sources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 34},\n",
    "    {\"ID\": 2, \"Name\": \"Bob\", \"Age\": 30},\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 25},  # Duplicate row\n",
    "    {\"ID\": 3, \"Name\": \"Charlie\", \"Age\": 35},\n",
    "    {\"ID\": 2, \"Name\": \"Bob\", \"Age\": 30},  # Duplicate row and ID column\n",
    "    {\"ID\": 4, \"Name\": \"David\", \"Age\": 40},\n",
    "    {\"ID\": 5, \"Name\": \"Eve\", \"Age\": 28},\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 25}, \n",
    "    {\"ID\": 6, \"Name\": \"Alice\", \"Age\": 34}# Duplicate row\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| ID|   Name|Age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 34|\n",
      "|  2|    Bob| 30|\n",
      "|  1|  Alice| 25|\n",
      "|  3|Charlie| 35|\n",
      "|  2|    Bob| 30|\n",
      "|  4|  David| 40|\n",
      "|  5|    Eve| 28|\n",
      "|  1|  Alice| 25|\n",
      "|  6|  Alice| 34|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame(data)\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# Show the Spark DataFrame\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| ID|   Name|Age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 34|\n",
      "|  2|    Bob| 30|\n",
      "|  1|  Alice| 25|\n",
      "|  3|Charlie| 35|\n",
      "|  4|  David| 40|\n",
      "|  5|    Eve| 28|\n",
      "|  6|  Alice| 34|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.distinct().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets drop the rows based on the duplication of the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| ID|   Name|Age|\n",
      "+---+-------+---+\n",
      "|  5|    Eve| 28|\n",
      "|  4|  David| 40|\n",
      "|  1|  Alice| 34|\n",
      "|  1|  Alice| 25|\n",
      "|  3|Charlie| 35|\n",
      "|  2|    Bob| 30|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.distinct().dropDuplicates(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'nationality'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm['nationality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[first_name: string, last_name: string]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm['first_name','last_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm['first_name','last_name'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------------+-------------+\n",
      "| ID|   Name|Age|col_trimright|        Names|\n",
      "+---+-------+---+-------------+-------------+\n",
      "|  1|  Alice| 34|      ++92321|Companies....|\n",
      "|  2|    Bob| 30|      ++92321|Companies....|\n",
      "|  1|  Alice| 25|      ++92321|Companies....|\n",
      "|  3|Charlie| 35|      ++92321|Companies....|\n",
      "|  2|    Bob| 30|      ++92321|Companies....|\n",
      "|  4|  David| 40|      ++92321|Companies....|\n",
      "|  5|    Eve| 28|      ++92321|Companies....|\n",
      "|  1|  Alice| 25|      ++92321|Companies....|\n",
      "|  6|  Alice| 34|      ++92321|Companies....|\n",
      "+---+-------+---+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs=spark_df.withColumn('col_trimright',lit(\"++92321\")).withColumn('Names',lit('Companies....'))\n",
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+\n",
      "| ID|phone|companies|\n",
      "+---+-----+---------+\n",
      "|  1|92321|Companies|\n",
      "|  2|92321|Companies|\n",
      "|  1|92321|Companies|\n",
      "|  3|92321|Companies|\n",
      "|  2|92321|Companies|\n",
      "|  4|92321|Companies|\n",
      "|  5|92321|Companies|\n",
      "|  1|92321|Companies|\n",
      "|  6|92321|Companies|\n",
      "+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.select ('ID',expr(\"trim(LEADING '+' From col_trimright) as phone\"),expr(\"trim(TRAILING '.' From Names) as companies\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dropping Null based Records from Spark Data Frames**\n",
    "* using dropna() will by default search for NULL columns and then if it finds a row with a single null value it will drop the complete row.\n",
    "* using dropna(how='all') will drop a rows that have full row as null.\n",
    "* using dropna(how='any') will drop a row that have any of the value of the column as null.\n",
    "* using dropna(subset=['c1',c2]) will find out any value is null it will drop all the row.\n",
    "* using dropna(thres=3) will search for columns that have non null values if there are less then 3 it will drop that if its more than then it will not.\n",
    "* using dropna(how='all',subset=['c1','c2']) it will look for all values of these columns if find null then drop the columns.\n",
    "* using dropna(how='any',subset=['c1','c2']) it will look for any value if its null it will drop the full row.\n",
    "* When we use dropna(how='any',thres=3,subset=['Name','Age']) the effect of how='any' get disappeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 34,\"class\":\"mid\"},\n",
    "    {\"ID\": 2, \"Name\": None, \"Age\": 30,\"class\":\"mid\"},\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 25,\"class\":\"high\"},  # Duplicate row\n",
    "    {\"ID\": 2, \"Name\": None, \"Age\": None,\"class\":\"Low\"},\n",
    "    {\"ID\": 2, \"Name\": \"Bob\", \"Age\": 30,\"class\":\"Low\"},  # Duplicate row and ID column\n",
    "    {\"ID\":3, \"Name\": None, \"Age\": None,\"class\":None},\n",
    "    {\"ID\": 5, \"Name\": \"Eve\", \"Age\": 28,\"class\":\"mid\"},\n",
    "    {\"ID\": 1, \"Name\": \"\", \"Age\": 25,\"class\":\"Low\"}, \n",
    "    {\"ID\": 6, \"Name\": None, \"Age\": 34,\"class\":\"Low\"}# Duplicate row\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:: 9\n",
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  2| null|30.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=spark.createDataFrame(pd.DataFrame(datas))\n",
    "# data.show()\n",
    "print(f'count:: {data.count()}')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.dropna(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(how='all',subset=[ 'Name','class']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(how='any',subset=['Name','class']).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we are just looking at three columns so if we get null in any of these column the whole row will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.dropna(how='any',thresh=3,subset=['Name','class','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "| id|visit_date|people|\n",
      "+---+----------+------+\n",
      "|  1|2017-02-01|    10|\n",
      "|  2|2017-01-02|   109|\n",
      "|  3|2017-01-03|   150|\n",
      "|  4|2017-01-04|    99|\n",
      "|  5|2017-01-05|   145|\n",
      "|  6|2017-01-06|  1455|\n",
      "|  7|2017-01-07|   199|\n",
      "|  8|2017-01-09|   188|\n",
      "+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"id\", IntegerType(), True),\n",
    "#     StructField(\"visit_date\", DateType(), True),\n",
    "#     StructField(\"people\", IntegerType(), True)\n",
    "# ])\n",
    "\n",
    "# Define the data\n",
    "data = [\n",
    "    (1, \"2017-02-01\", 10),\n",
    "    (2, \"2017-01-02\", 109),\n",
    "    (3, \"2017-01-03\", 150),\n",
    "    (4, \"2017-01-04\", 99),\n",
    "    (5, \"2017-01-05\", 145),\n",
    "    (6, \"2017-01-06\", 1455),\n",
    "    (7, \"2017-01-07\", 199),\n",
    "    (8, \"2017-01-09\", 188)\n",
    "]\n",
    "pp=pd.DataFrame(data,columns=['id','visit_date','people'])\n",
    "# Create the DataFrame and convert the date strings to DateType\n",
    "df = spark.createDataFrame(pp)\n",
    "df = df.withColumn(\"visit_date\", to_date(df[\"visit_date\"], \"yyyy-MM-dd\"))\n",
    "# pp\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+----------+\n",
      "| id|visit_date|people|difference|\n",
      "+---+----------+------+----------+\n",
      "|  2|2017-01-02|   109|         8|\n",
      "|  3|2017-01-03|   150|         7|\n",
      "|  5|2017-01-05|   145|         5|\n",
      "|  6|2017-01-06|  1455|         4|\n",
      "|  7|2017-01-07|   199|         3|\n",
      "|  8|2017-01-09|   188|         2|\n",
      "+---+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('people > 100').withColumn('difference',expr(\"10-id\")).filter(expr'difference+id').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of Sorting a Spark Data Frame**\n",
    "* sort() we can pass string ,col type and list of columns.\n",
    "* sort() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sort the data on bases of ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 34,\"class\":\"mid\"},\n",
    "    {\"ID\": 2, \"Name\": None, \"Age\": 30,\"class\":\"mid\"},\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 25,\"class\":\"high\"},  # Duplicate row\n",
    "    {\"ID\": 2, \"Name\": None, \"Age\": None,\"class\":\"Low\"},\n",
    "    {\"ID\": 2, \"Name\": \"Bob\", \"Age\": 30,\"class\":\"Low\"},  # Duplicate row and ID column\n",
    "    {\"ID\":3, \"Name\": None, \"Age\": None,\"class\":None},\n",
    "    {\"ID\": 5, \"Name\": \"Eve\", \"Age\": 28,\"class\":\"mid\"},\n",
    "    {\"ID\": 1, \"Name\": \"\", \"Age\": 25,\"class\":\"Low\"}, \n",
    "    {\"ID\": 6, \"Name\": None, \"Age\": 34,\"class\":\"Low\"}# Duplicate row\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:: 9\n",
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  2| null|30.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=spark.createDataFrame(pd.DataFrame(datas))\n",
    "# data.show()\n",
    "print(f'count:: {data.count()}')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|     |25.0|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2| null|30.0|  mid|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort('ID').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|     |25.0|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null|30.0|  mid|\n",
      "|  2| null| NaN|  Low|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(data['ID']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|     |25.0|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null|30.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(data.ID).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|Alice|25.0| high|\n",
      "|  1|     |25.0|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null|30.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(col('ID')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sorting by DESC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n",
      "data count:: 9\n",
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  6| null|34.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2| null|30.0|  mid|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "|  1|Alice|25.0| high|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort('ID',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  6| null|34.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2| null|30.0|  mid|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  1|     |25.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(data['ID'],ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  6| null|34.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  2| null|30.0|  mid|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null| NaN|  Low|\n",
      "|  1|Alice|25.0| high|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(col('ID'),ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  6| null|34.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  2| null|30.0|  mid|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null| NaN|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "|  1|Alice|25.0| high|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(col('ID').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  6| null|34.0|  Low|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  3| null| NaN| null|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null|30.0|  mid|\n",
      "|  2| null| NaN|  Low|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  1|     |25.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(data['ID'].desc()).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task**\n",
    "Q: Find the top 3 most purchased items from the input data using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = [(1,'Fruit','Apple'),(2,'Fruit','Orange'),(3,'Fruit','Apple'),(4,'Fruit','Grapes'),(5,'Fruit','Apple'),(6,'Fruit','Apple'),(7,'Fruit','Apple'),(8,'Fruit','Banana'),(9,'Fruit','Orange'),(10,'Fruit','Pineapple'),(11,'Fruit','Apple'),(12,'Fruit','Orange'),(13,'Fruit','Banana'),(14,'Fruit','Apple'),(15,'Fruit','Grapes'),(16,'Fruit','watermelon')]\n",
    "schema = ['orderNum','catrgory','itemName']\n",
    "df = spark.createDataFrame(da,schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|itemName|counts|\n",
      "+--------+------+\n",
      "|   Apple|     7|\n",
      "|  Orange|     3|\n",
      "|  Grapes|     2|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(col('itemName')).agg(expr('count(itemName) as counts')).sort('counts',ascending=False).limit(3).show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dealing with Nulls while sorting Spark Data Frame**\n",
    "* When we have null in the column according to which we want to round-off the data then we can use .asc_nulls_first(),.asc_nulls_last(),desc_nulls_first(),desc_nulls_last().\n",
    "* The data is in asc and nulls are at the top if we write nulls_first or at the last when we write nulls_last()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  3| null| NaN| null|  222|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.orderBy (col('class').asc_nulls_first()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  3| null| NaN| null|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  2| null|30.0|  mid|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2| null| NaN|  Low|\n",
      "|  6| null|34.0|  Low|\n",
      "|  1|     |25.0|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.orderBy (col('class').desc_nulls_first()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  2| null|30.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null| NaN|  Low|\n",
      "|  1|     |25.0|  Low|\n",
      "|  6| null|34.0|  Low|\n",
      "|  3| null| NaN| null|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.orderBy (col('class').desc_nulls_last()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  3| null| NaN| null|\n",
      "|  2| null|30.0|  mid|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  2| null| NaN|  Low|\n",
      "|  1|     |25.0|  Low|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.orderBy (data['class'].desc_nulls_first()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.orderBy (data['class'].desc_nulls_first()).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Composite Sorting of a Data Frame**\n",
    "* sort the data according to id in asc and name in descending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+\n",
      "| ID| Name| Age|class|\n",
      "+---+-----+----+-----+\n",
      "|  1|Alice|34.0|  mid|\n",
      "|  2| null|30.0|  mid|\n",
      "|  1|Alice|25.0| high|\n",
      "|  2| null| NaN|  Low|\n",
      "|  2|  Bob|30.0|  Low|\n",
      "|  3| null| NaN| null|\n",
      "|  5|  Eve|28.0|  mid|\n",
      "|  1|     |25.0|  Low|\n",
      "|  6| null|34.0|  Low|\n",
      "+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(col('marks').desc(),col('id').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  6| null|34.0|  Low|  180|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(data['ID'].desc(),data['marks'].asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  6| null|34.0|  Low|  180|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(desc('ID'),asc('marks')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "+---+-----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort(['id','marks'],ascending=[1,0]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prioritized Sorting of a Spark Data Frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n",
      "data count:: 9\n",
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "+---+-----+----+-----+-----+\n",
      "\n",
      "+--------+------------+---------+--------+------+\n",
      "|CourseId|  Complexity|     Name|Enrolled| Price|\n",
      "+--------+------------+---------+--------+------+\n",
      "|       1|    Beginner| Course 1|     150| 99.99|\n",
      "|       2|Intermediate| Course 2|     120|129.99|\n",
      "|       3|    Advanced| Course 3|     180|149.99|\n",
      "|       4|    Beginner| Course 4|      90| 79.99|\n",
      "|       5|Intermediate| Course 5|     160|119.99|\n",
      "|       6|    Advanced| Course 6|     190|159.99|\n",
      "|       7|    Beginner| Course 7|     110| 89.99|\n",
      "|       8|Intermediate| Course 8|     140|109.99|\n",
      "|       9|    Advanced| Course 9|     170|139.99|\n",
      "|      10|    Beginner|Course 10|      70| 69.99|\n",
      "|      11|Intermediate|Course 11|     130| 99.99|\n",
      "|      12|    Advanced|Course 12|     200|169.99|\n",
      "|      13|    Beginner|Course 13|      95| 79.99|\n",
      "|      14|Intermediate|Course 14|     175|129.99|\n",
      "|      15|    Advanced|Course 15|     185|149.99|\n",
      "|      16|    Beginner|Course 16|     105| 89.99|\n",
      "|      17|Intermediate|Course 17|     145|119.99|\n",
      "|      18|    Advanced|Course 18|     195|159.99|\n",
      "|      19|    Beginner|Course 19|      75| 69.99|\n",
      "|      20|Intermediate|Course 20|     125| 99.99|\n",
      "+--------+------------+---------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+---------+--------+------+\n",
      "|CourseId|  Complexity|     Name|Enrolled| Price|\n",
      "+--------+------------+---------+--------+------+\n",
      "|       1|    Beginner| Course 1|     150| 99.99|\n",
      "|      16|    Beginner|Course 16|     105| 89.99|\n",
      "|       7|    Beginner| Course 7|     110| 89.99|\n",
      "|      22|    Beginner|Course 22|     100| 79.99|\n",
      "|      28|    Beginner|Course 28|     105| 79.99|\n",
      "|      13|    Beginner|Course 13|      95| 79.99|\n",
      "|       4|    Beginner| Course 4|      90| 79.99|\n",
      "|      19|    Beginner|Course 19|      75| 69.99|\n",
      "|      10|    Beginner|Course 10|      70| 69.99|\n",
      "|      25|    Beginner|Course 25|      85| 69.99|\n",
      "|      14|Intermediate|Course 14|     175|129.99|\n",
      "|      23|Intermediate|Course 23|     155|129.99|\n",
      "|       2|Intermediate| Course 2|     120|129.99|\n",
      "|       5|Intermediate| Course 5|     160|119.99|\n",
      "|      17|Intermediate|Course 17|     145|119.99|\n",
      "|      29|Intermediate|Course 29|     140|119.99|\n",
      "|       8|Intermediate| Course 8|     140|109.99|\n",
      "|      26|Intermediate|Course 26|     115| 99.99|\n",
      "|      11|Intermediate|Course 11|     130| 99.99|\n",
      "|      20|Intermediate|Course 20|     125| 99.99|\n",
      "+--------+------------+---------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_dfs.sort(when(col('Complexity')=='Beginner',0).when(col('Complexity')=='Intermediate',1).otherwise(2),col('Price').desc()).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL Style Syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cas=expr(\"case when Complexity == 'Beginner' then 0 when Complexity == 'Intermediate' then 1 else 2 end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+---------+--------+------+\n",
      "|CourseId|  Complexity|     Name|Enrolled| Price|\n",
      "+--------+------------+---------+--------+------+\n",
      "|       1|    Beginner| Course 1|     150| 99.99|\n",
      "|       7|    Beginner| Course 7|     110| 89.99|\n",
      "|      16|    Beginner|Course 16|     105| 89.99|\n",
      "|      28|    Beginner|Course 28|     105| 79.99|\n",
      "|      22|    Beginner|Course 22|     100| 79.99|\n",
      "|       4|    Beginner| Course 4|      90| 79.99|\n",
      "|      13|    Beginner|Course 13|      95| 79.99|\n",
      "|      10|    Beginner|Course 10|      70| 69.99|\n",
      "|      19|    Beginner|Course 19|      75| 69.99|\n",
      "|      25|    Beginner|Course 25|      85| 69.99|\n",
      "|       2|Intermediate| Course 2|     120|129.99|\n",
      "|      23|Intermediate|Course 23|     155|129.99|\n",
      "|      14|Intermediate|Course 14|     175|129.99|\n",
      "|      17|Intermediate|Course 17|     145|119.99|\n",
      "|       5|Intermediate| Course 5|     160|119.99|\n",
      "|      29|Intermediate|Course 29|     140|119.99|\n",
      "|       8|Intermediate| Course 8|     140|109.99|\n",
      "|      11|Intermediate|Course 11|     130| 99.99|\n",
      "|      26|Intermediate|Course 26|     115| 99.99|\n",
      "|      20|Intermediate|Course 20|     125| 99.99|\n",
      "+--------+------------+---------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_dfs.sort(cas,col('Price').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n",
      "data count:: 9\n",
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "+---+-----+----+-----+-----+\n",
      "\n",
      "+--------+------------+---------+--------+------+\n",
      "|CourseId|  Complexity|     Name|Enrolled| Price|\n",
      "+--------+------------+---------+--------+------+\n",
      "|       1|    Beginner| Course 1|     150| 99.99|\n",
      "|       2|Intermediate| Course 2|     120|129.99|\n",
      "|       3|    Advanced| Course 3|     180|149.99|\n",
      "|       4|    Beginner| Course 4|      90| 79.99|\n",
      "|       5|Intermediate| Course 5|     160|119.99|\n",
      "|       6|    Advanced| Course 6|     190|159.99|\n",
      "|       7|    Beginner| Course 7|     110| 89.99|\n",
      "|       8|Intermediate| Course 8|     140|109.99|\n",
      "|       9|    Advanced| Course 9|     170|139.99|\n",
      "|      10|    Beginner|Course 10|      70| 69.99|\n",
      "|      11|Intermediate|Course 11|     130| 99.99|\n",
      "|      12|    Advanced|Course 12|     200|169.99|\n",
      "|      13|    Beginner|Course 13|      95| 79.99|\n",
      "|      14|Intermediate|Course 14|     175|129.99|\n",
      "|      15|    Advanced|Course 15|     185|149.99|\n",
      "|      16|    Beginner|Course 16|     105| 89.99|\n",
      "|      17|Intermediate|Course 17|     145|119.99|\n",
      "|      18|    Advanced|Course 18|     195|159.99|\n",
      "|      19|    Beginner|Course 19|      75| 69.99|\n",
      "|      20|Intermediate|Course 20|     125| 99.99|\n",
      "+--------+------------+---------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Subset and calculate**\n",
    "After you've extracted values from a list, you can use them to perform additional calculations. Take this example, where the second and fourth element of a list x are extracted. The strings that result are pasted together using the + operator:\n",
    "\n",
    "x = [\"a\", \"b\", \"c\", \"d\"]\\\n",
    "print(x[1] + x[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Aggregations on a Spark Data Frame**\n",
    "1. Get revenue using order_item_subtotal for a given order_item_order_id eg 2\n",
    "2. Get number of items , total quantity as well as revenue for given items order id eg 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|1            |1                  |957                  |299.98                  |1                  |299.98             |\n",
      "|2            |2                  |1073                 |199.99                  |1                  |199.99             |\n",
      "|3            |2                  |502                  |50.0                    |5                  |250.0              |\n",
      "|4            |2                  |403                  |129.99                  |1                  |129.99             |\n",
      "|5            |4                  |897                  |24.99                   |2                  |49.98              |\n",
      "|6            |4                  |365                  |59.99                   |5                  |299.95             |\n",
      "|7            |4                  |502                  |50.0                    |3                  |150.0              |\n",
      "|8            |4                  |1014                 |49.98                   |4                  |199.92             |\n",
      "|9            |5                  |957                  |299.98                  |1                  |299.98             |\n",
      "|10           |5                  |365                  |59.99                   |5                  |299.95             |\n",
      "|11           |5                  |1014                 |49.98                   |2                  |99.96              |\n",
      "|12           |5                  |957                  |299.98                  |1                  |299.98             |\n",
      "|13           |5                  |403                  |129.99                  |1                  |129.99             |\n",
      "|14           |7                  |1073                 |199.99                  |1                  |199.99             |\n",
      "|15           |7                  |957                  |299.98                  |1                  |299.98             |\n",
      "|16           |7                  |926                  |15.99                   |5                  |79.95              |\n",
      "|17           |8                  |365                  |59.99                   |3                  |179.97             |\n",
      "|18           |8                  |365                  |59.99                   |5                  |299.95             |\n",
      "|19           |8                  |1014                 |49.98                   |4                  |199.92             |\n",
      "|20           |8                  |502                  |50.0                    |1                  |50.0               |\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json('../Source_data/retail_db_json/order_items/')\n",
    "df.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_item_id', 'bigint'),\n",
       " ('order_item_order_id', 'bigint'),\n",
       " ('order_item_product_id', 'bigint'),\n",
       " ('order_item_product_price', 'double'),\n",
       " ('order_item_quantity', 'bigint'),\n",
       " ('order_item_subtotal', 'double')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| total|\n",
      "+------+\n",
      "|579.98|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('order_item_order_id = 2').select (sum('order_item_subtotal').alias('total')).show()\n",
    "# select(sum(col('order_item_subtotal'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-------------+\n",
      "|number_of_items|total_quantites|total_revenue|\n",
      "+---------------+---------------+-------------+\n",
      "|              3|              7|       579.98|\n",
      "+---------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('order_item_order_id = 2').select (count('order_item_quantity').alias(\"number_of_items\"),sum('order_item_quantity').alias('total_quantites'),sum('order_item_subtotal').alias('total_revenue')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting Count of a Spark Data Frame**\n",
    "* If we use df.count() it will count the recording in the dataframe.\n",
    "* When we use df.select (count('*')).show() it will count the recording as by using function count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172198"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  172198|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select (count('*')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of groupBy on Spark Data Frame**\n",
    "* If i write df.groupBy().min().show() it will apply group by and min on all the numeric columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for employees us sm as name of spark dataframe\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n",
      "data count:: 9\n",
      "+---+-----+----+-----+-----+\n",
      "| ID| Name| Age|class|marks|\n",
      "+---+-----+----+-----+-----+\n",
      "|  1|Alice|34.0|  mid|  222|\n",
      "|  2| null|30.0|  mid|  201|\n",
      "|  1|Alice|25.0| high|  100|\n",
      "|  2| null| NaN|  Low|  400|\n",
      "|  2|  Bob|30.0|  Low|  200|\n",
      "|  3| null| NaN| null|  222|\n",
      "|  5|  Eve|28.0|  mid|  150|\n",
      "|  1|     |25.0|  Low|   80|\n",
      "|  6| null|34.0|  Low|  180|\n",
      "+---+-----+----+-----+-----+\n",
      "\n",
      "+--------+------------+---------+--------+------+\n",
      "|CourseId|  Complexity|     Name|Enrolled| Price|\n",
      "+--------+------------+---------+--------+------+\n",
      "|       1|    Beginner| Course 1|     150| 99.99|\n",
      "|       2|Intermediate| Course 2|     120|129.99|\n",
      "|       3|    Advanced| Course 3|     180|149.99|\n",
      "|       4|    Beginner| Course 4|      90| 79.99|\n",
      "|       5|Intermediate| Course 5|     160|119.99|\n",
      "|       6|    Advanced| Course 6|     190|159.99|\n",
      "|       7|    Beginner| Course 7|     110| 89.99|\n",
      "|       8|Intermediate| Course 8|     140|109.99|\n",
      "|       9|    Advanced| Course 9|     170|139.99|\n",
      "|      10|    Beginner|Course 10|      70| 69.99|\n",
      "|      11|Intermediate|Course 11|     130| 99.99|\n",
      "|      12|    Advanced|Course 12|     200|169.99|\n",
      "|      13|    Beginner|Course 13|      95| 79.99|\n",
      "|      14|Intermediate|Course 14|     175|129.99|\n",
      "|      15|    Advanced|Course 15|     185|149.99|\n",
      "|      16|    Beginner|Course 16|     105| 89.99|\n",
      "|      17|Intermediate|Course 17|     145|119.99|\n",
      "|      18|    Advanced|Course 18|     195|159.99|\n",
      "|      19|    Beginner|Course 19|      75| 69.99|\n",
      "|      20|Intermediate|Course 20|     125| 99.99|\n",
      "+--------+------------+---------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            1|                  1|                  957|                  299.98|                  1|             299.98|\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "|            3|                  2|                  502|                    50.0|                  5|              250.0|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json('../Source_data/retail_db_json/order_items/')\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n",
      "|min(order_item_id)|min(order_item_order_id)|min(order_item_product_id)|min(order_item_product_price)|min(order_item_quantity)|min(order_item_subtotal)|\n",
      "+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n",
      "|                 1|                       1|                        19|                         9.99|                       1|                    9.99|\n",
      "+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy().min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| count|\n",
      "+------+\n",
      "|172198|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy().count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+--------+---------------+\n",
      "|order_customer_id|order_date           |order_id|order_status   |\n",
      "+-----------------+---------------------+--------+---------------+\n",
      "|11599            |2013-07-25 00:00:00.0|1       |CLOSED         |\n",
      "|256              |2013-07-25 00:00:00.0|2       |PENDING_PAYMENT|\n",
      "|12111            |2013-07-25 00:00:00.0|3       |COMPLETE       |\n",
      "+-----------------+---------------------+--------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfo=spark.read.json('../Source_data/retail_db_json/orders/')\n",
    "dfo.show(truncate=0,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_customer_id', 'bigint'),\n",
       " ('order_date', 'string'),\n",
       " ('order_id', 'bigint'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfo.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perform Grouped Aggregations using direct functions on a Spark Data Frame**\n",
    "* When we want to used group aggregation using direct function we can use one aggregation function like max,sum,count etc. When we want to use multiple aggregation we can use\\\n",
    "    function call agg()\n",
    "* When we apply direct function without specifying a column name it will perform aggregation on all the columns.\n",
    "* To perform aggregation on a column we need to write the name in string formate.\n",
    "* once we use df.groupBy() it is changed to grouped Dataframe type and we can use agg function on top to this datatype.\n",
    "* When we apply sum function we cant apply round directly on it we have to use withColumn and then use round aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------------+-------------+\n",
      "|order_customer_id|sum(order_customer_id)|sum(order_id)|\n",
      "+-----------------+----------------------+-------------+\n",
      "|            11938|                 71628|       215124|\n",
      "|             1950|                 17550|       261294|\n",
      "|             2529|                 15174|       269506|\n",
      "+-----------------+----------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfo.groupBy('order_customer_id').sum().show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note::\\\n",
    "> Here you cant see any sort of string column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            1|                  1|                  957|                  299.98|                  1|             299.98|\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "|            3|                  2|                  502|                    50.0|                  5|              250.0|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json('../Source_data/retail_db_json/order_items/')\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.group.GroupedData"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group=df.groupBy('order_item_order_id')\n",
    "type(df_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n",
      "|order_item_order_id|sum(order_item_id)|sum(order_item_order_id)|sum(order_item_product_id)|sum(order_item_product_price)|sum(order_item_quantity)|sum(order_item_subtotal)|\n",
      "+-------------------+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n",
      "|                 29|               425|                     145|                      3897|            909.9300000000001|                       9|                 1109.85|\n",
      "|                474|              5815|                    2370|                      4508|           374.94000000000005|                      13|       774.8199999999999|\n",
      "|                964|              9586|                    3856|                      2964|           499.95000000000005|                      11|       739.8800000000001|\n",
      "|               1677|             20860|                    8385|                      2357|                       277.97|                      14|       649.9200000000001|\n",
      "+-------------------+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## We we see no aggregation on top of string column\n",
    "df_group.sum().show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------------+------------------------+------------------------+\n",
      "|order_item_order_id|sum(order_item_product_price)|sum(order_item_quantity)|sum(order_item_subtotal)|\n",
      "+-------------------+-----------------------------+------------------------+------------------------+\n",
      "|                 29|            909.9300000000001|                       9|                 1109.85|\n",
      "|                474|           374.94000000000005|                      13|       774.8199999999999|\n",
      "|                964|           499.95000000000005|                      11|       739.8800000000001|\n",
      "|               1677|                       277.97|                      14|       649.9200000000001|\n",
      "+-------------------+-----------------------------+------------------------+------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### To be very much specific now we can pass the column names in aggregate function\n",
    "## we cant use round on top of it first we need to change the names and the go for rounding the function.\n",
    "df_group.sum('order_item_product_price','order_item_quantity','order_item_subtotal').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_order_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------------+------------------------+-------------------+-------------------+\n",
      "|                 29|                  909.93|                  9|            1109.85|\n",
      "|                474|                  374.94|                 13|             774.82|\n",
      "|                964|                  499.95|                 11|             739.88|\n",
      "|               1677|                  277.97|                 14|             649.92|\n",
      "+-------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.sum('order_item_product_price','order_item_quantity','order_item_subtotal').select('order_item_order_id','sum(order_item_product_price)','sum(order_item_quantity)','sum(order_item_subtotal)').\\\n",
    "    toDF('order_item_order_id','order_item_product_price','order_item_quantity','order_item_subtotal').\\\n",
    "        withColumn('order_item_product_price',round('order_item_product_price',2)).\\\n",
    "        withColumn('order_item_subtotal',round('order_item_subtotal',2)).\\\n",
    "show(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perform Grouped Aggregations using Agg on a Spark Data Frame**\n",
    "* In group aggregation we have two approaches first is using dict and other is using column style approach.\n",
    "* Dict has its limitations so mostly we use list type syntax.\n",
    "* In case of Dict we cant apply round function on top of the sum function.\n",
    "* If we want to work on same column but use different function we cant use that as in Dictionary we are suppose to use unique keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./sources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            1|                  1|                  957|                  299.98|                  1|             299.98|\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "|            3|                  2|                  502|                    50.0|                  5|              250.0|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json('../Source_data/retail_db_json/order_items/')\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------------+------------------------+\n",
      "|order_item_order_id|sum(order_item_product_price)|sum(order_item_subtotal)|\n",
      "+-------------------+-----------------------------+------------------------+\n",
      "|                 29|            909.9300000000001|                 1109.85|\n",
      "|                474|           374.94000000000005|       774.8199999999999|\n",
      "|                964|           499.95000000000005|       739.8800000000001|\n",
      "|               1677|                       277.97|       649.9200000000001|\n",
      "+-------------------+-----------------------------+------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group=df.groupBy('order_item_order_id')\n",
    "\n",
    "df_group.agg({'order_item_product_price':'sum','order_item_subtotal':'sum'}).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+-------------------+\n",
      "|order_item_order_id|order_item_product_price|order_item_subtotal|\n",
      "+-------------------+------------------------+-------------------+\n",
      "|                 29|                  909.93|            1109.85|\n",
      "|                474|                  374.94|             774.82|\n",
      "|                964|                  499.95|             739.88|\n",
      "|               1677|                  277.97|             649.92|\n",
      "+-------------------+------------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.agg({'order_item_product_price':'sum','order_item_subtotal':'sum'}).\\\n",
    "    toDF('order_item_order_id','order_item_product_price','order_item_subtotal').\\\n",
    "    withColumn('order_item_product_price',round('order_item_product_price',2)).\\\n",
    "    withColumn('order_item_subtotal',round('order_item_subtotal',2)).\\\n",
    "    show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------------------------+----------------------------------+\n",
      "|order_item_order_id|round(sum(order_item_product_price), 2)|round(sum(order_item_subtotal), 2)|\n",
      "+-------------------+---------------------------------------+----------------------------------+\n",
      "|                 29|                                 909.93|                           1109.85|\n",
      "|                474|                                 374.94|                            774.82|\n",
      "|                964|                                 499.95|                            739.88|\n",
      "|               1677|                                 277.97|                            649.92|\n",
      "+-------------------+---------------------------------------+----------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.agg(round(sum('order_item_product_price'),2),round(sum('order_item_subtotal'),2)).\\\n",
    "    toDF('order_item_order_id','order_item_product_price','order_item_subtotal').\\\n",
    "    show(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: \\\n",
    "If we want to work on same column but use different function we cant use that as in Dictionary we are suppose to use unique keys.\\\n",
    "It was support show 3 columns but its show the min column.\\\n",
    "Lets go for list type syntax this problem is solved over there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+\n",
      "|order_item_order_id|min(order_item_quantity)|\n",
      "+-------------------+------------------------+\n",
      "|                 29|                       1|\n",
      "|                474|                       1|\n",
      "|                964|                       1|\n",
      "|               1677|                       1|\n",
      "+-------------------+------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.agg({'order_item_quantity':'sum','order_item_quantity':'min'}).\\\n",
    "    show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+------------------------+\n",
      "|order_item_order_id|sum(order_item_quantity)|min(order_item_quantity)|\n",
      "+-------------------+------------------------+------------------------+\n",
      "|                 29|                       9|                       1|\n",
      "|                474|                      13|                       1|\n",
      "|                964|                      11|                       1|\n",
      "|               1677|                      14|                       1|\n",
      "+-------------------+------------------------+------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.agg(sum('order_item_quantity'),min('order_item_quantity')).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd044dfe803f410830d2077cd20a7505658c0f80bea76037d58d809c64c95f16"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
